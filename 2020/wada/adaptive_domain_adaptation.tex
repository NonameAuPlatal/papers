\documentclass[12pt,a4paper,twoside]{report}
\usepackage[left=3cm,right=3cm,top=2cm,bottom=3cm]{geometry}
\usepackage{pagenote}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage[utf8]{inputenc}	% Para caracteres en espa√±ol
\usepackage{amsmath,amsthm,amsfonts,amssymb,amscd}
\usepackage{multirow,booktabs}
\usepackage[table]{xcolor}
\usepackage{fullpage}
\usepackage{lastpage}
\usepackage{enumitem}
\usepackage{fancyhdr}
\usepackage{mathrsfs}
\usepackage{wrapfig}
\usepackage{setspace}
\usepackage{calc}
\usepackage{color,soul}
\usepackage{multicol}
\usepackage{cancel}
\usepackage[retainorgcmds]{IEEEtrantools}
\usepackage{xcolor}
\colorlet{shadecolor}{orange!15}
\parindent 0in
\parskip 12pt
\geometry{margin=1in, headsep=0.25in}
\theoremstyle{definition}
\newtheorem{defn}{Definition}
\newtheorem{reg}{Rule}
\newtheorem{exer}{Exercise}
\newtheorem{note}{Note}
\usepackage{listings}
\usepackage{spverbatim}
\usepackage{fancyvrb}
\usepackage{hyperref}
\usepackage{float} 
\usepackage{natbib}
\usepackage{tikz}
\usepackage{pgfplots}
\usepackage{pgfplotstable}
\usepackage{soulutf8}
\usepackage{color,soul}
\usepackage{xcolor}
\usepackage{tabularx}
\newtheorem{lemma}{Lemma}[section]
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}{Proposition}[section]
\newtheorem{definition}{Definition}[section]
\DeclareMathOperator*{\argmax}{argmax}
\usepackage[draft]{todo}
\newcommand{\fyTodo}[1]{\Todo[FY:]{\textcolor{orange}{#1}}}
\newcommand{\fyTodostar}[1]{\Todo*[FY:]{\textcolor{orange}{#1}}}
\newcommand{\fyDone}[1]{\done[FY]\Todo[FY:]{\textcolor{orange}{#1}}}
\newcommand{\fyDonestar}[1]{\done[FY]\Todo[FY:]{\textcolor{orange}{#1}}}
\pgfkeys{
    /tr/rowfilter/.style 2 args={
        /pgfplots/x filter/.append code={
            \edef\arga{\thisrow{#1}}
            \edef\argb{#2}
            \ifx\arga\argb
            \else
                \def\pgfmathresult{}
            \fi
        }
    }
}
\usepackage{mathtools,xparse}
\DeclarePairedDelimiter{\abs}{\lvert}{\rvert}
\DeclarePairedDelimiter{\norm}{\lVert}{\rVert}
\NewDocumentCommand{\normL}{ s O{} m }{%
  \IfBooleanTF{#1}{\norm*{#3}}{\norm[#2]{#3}}_{L_1(\Omega)}%
}

\begin{document}
\begin{center}
{\LARGE \bf Combination of generic and specific}\\
\end{center}
\setlength{\belowdisplayskip}{8pt} \setlength{\belowdisplayshortskip}{8pt}
\setlength{\abovedisplayskip}{8pt} \setlength{\abovedisplayshortskip}{8pt}
\setlist{nosep}
\setlength{\parskip}{0.1cm}
\setlength{\parindent}{1em}
\section*{Word Level Adaptive Domain Adaptation}
Let $h$ is the output of the encoder which is pretrained, the representation adapted to the domain k using the residual adapter will be $$h' = h + a_k(h)$$ In gating version, $$h' = h * (1-z_k) + a_k(h) * z_k$$ where $z_k$ is also a function of $h$ taking value in $[0,1]$. More precisely, in MT task, $h$ and $h'$ is a sequence of context vectors , and the combination is element wise, i.e, $h'(w) = h(w) * z_k(h(w)) + a_k(h(w)) * z_k(h(w))$ where $w$ is a token in the sequence.
\\

In Word Level Adaptive Domain Adaptation version, $z_k(h(w))$ is designed so that the less probably the word $w$ is in the domain $k$ the smaller $z_k(h(w))$ is. The Word Level Adaptive Domain Adaptation aims to reduce the variation caused by $a_k$ of the adapted context vector $h'$ compared to $h$ of a word that does not surely belong to domain k. \\

For a domain, there are 2 type of "less specific" words: out-of-domain words that rarely or never appear in the domain but is popular in the other domains; or popular words that appear frequently in many domains. By reusing learned representation of the "less specific" words, we can at least guarantee the risk of the bad prediction of out-of-domain words by the risk of the learned generic model.
\section*{Training process}
There are 3 steps:
\begin{itemize}
	\item Pretraining a generic model with mixed corpora.
	\item Training 2 domain classifiers on top of the encoder and on top of the decoder while freezing the parameters of the generic model.
	\item Training parameters of the adapters with in-domain data separately while freezing the parameters of the generic model and of the domain classifiers.
\end{itemize}

\section*{Weakness of the method}
The new method depends essentially to the quality of the classifier. Moreover, the classifier can fail to detect in-domain words in the case of polysemous words. For example, the word "joint" in joint stereo, joint sprain and joint project has 2 different translations as "joint" and "articulaire" in french. In the unbalanced dataset as the mixed corpora (6 in-domain corpora), the classifier somehow fails to predict IT as the most probable domain of "joint stereo".

\section*{Analysis on the risk of new method}

The combination of generic representation and the residual adapter in the adapted model is as following
\begin{equation}
h' = h + a_k(h)
\label{eq:1}
\end{equation}

where h is output of sharing layer, i.e $h=M(x,y)$, $M: \mathbb{X} \times \mathbb{Y} \rightarrow \mathbf{\Omega}\subset \mathbb{R}^{d_{model}}$, $a_k$ is the residual adapter which is a function $\mathbb{R}^{d_{model}} \rightarrow \mathbb{R}^{d_{model}}$. \\

The bad performance of fine-tuned model happens as the value of $a_k(h)$ of the out-of-domain example $(x^{adv}, y^{adv})$ is adversarial to the model. Indeed, by fine-tuning such adapter with only in-domain data, $a_k$ never encounters adversarial examples, thus does not guarantee the risk of prediction on out-of-domain examples. However, if we control the weight of $a_k(h)$ is the combination, we might reduce the wrong belief of $a_k$ on adversarial examples. To execute the idea, we  investigate the version of convex combination.

\begin{equation}
h' = h * (1-f(z_k)) + a_k(h) * f(z_k)
\label{eq:2}
\end{equation}

where 
\begin{itemize}
	\item $z_k = P(d=k | h=M(x,y)) = p(h)$ where $p: \mathbf{\Omega} \rightarrow \mathbf{\Delta}^{K}$
	\item f is an activation function $f: [0,1] \rightarrow [0,1]$ which is monotonically increasing.
\end{itemize}
We define some important constants:
\begin{itemize}
	\item Domain inseparability $\mathbb{\varepsilon}_{insep} = \displaystyle{\mathop{\sum}_{k' \neq k} \mathbb{E}_{x,y}[\mathbf{1}_{d=k'}*p_k(h)]}$. where $h=M(x,y)$
	\item Risk of generic model $\mathbb{R} = \mathbb{E}_{x,y}[l(h,y)]$
	\item $\mathbf{\Omega_{k,p_{0}}} = \lbrace h | z_k = p_k(h)<p_0\rbrace$.
	\item Risk of adapted model in in-domain test set $R_k = \displaystyle{\mathbb{E}_{x,y \sim p(.|d=k)}[l(h * (1-f(z_k)) + a_k(h) * f(z_k), y)]}$
\end{itemize}
We also have 2 assumptions on the regularization of activation function:
\begin{itemize}
	\item $a_k$ is bounded in sense of $\normL{a_k(h)} < M$
	\item loss function is bounded in sense of $l(h * (1-f(z_k)) + a_k(h) * f(z_k),y) < A$ $\forall x,y$
\end{itemize}
We then access the risk of of the model adapted to domain k with the residual $a_k$ in the prediction of arbitrary examples of K domains.
\begin{equation}
\begin{split}
R(M,a_k) &= \mathbb{E}_{x,y}[l(h * (1-f(z_k)) + a_k(h) * f(z_k),y)] \\
		&= \mathbb{E}_{x,y}[l(h * (1-f(z_k)) + a_k(h) * f(z_k),y)\mathbf{1}_{\mathbf{\Omega_{k,p_{0}}}}(h)] + \\
		&+ \mathbb{E}_{x,y}[l(h * (1-f(z_k)) + a_k(h) * f(z_k),y) \mathbf{1}_{\mathbf{\Omega_{k,p_{0}}}^{c}}(h)]
\end{split}
\end{equation}

By designing a suitable activation function, we can map $f: [0,p_{0}] \rightarrow [0,\delta]$. Then with $\delta$ is small enough we can have below approximation of the first term:
\begin{equation}
\mathbb{E}_{x,y}[l(h * (1-f(z_k)) + a_k(h) * f(z_k),y)\mathbf{1}_{\mathbf{\Omega_{k,p_{0}}}}(h)] \sim \mathbb{E}_{x,y}[l(h,y)\mathbf{1}_\mathbf{\Omega_{k,p_{0}}}(h)] \leq \mathbb{E}_{x,y}[l(h,y)] = R
\label{eq:4}
\end{equation}
The second term:
\begin{equation}
\begin{split}
\mathbb{E}_{x,y}[l(h * (1-f(z_k)) + a_k(h) * f(z_k),y) \mathbf{1}_{\mathbf{\Omega_{k,p_{0}}}^{c}}(h)] &= \displaystyle{\mathop{\sum}_{k'}\mathbb{E}_{x,y \sim p(.|d=k')}[l(h * (1-f(z_k))} \\
	& + a_k(h) * f(z_k),y) *\mathbf{1}_{\mathbf{\Omega_{k,p_{0}}}^{c}}(h)]P(d=k') \\
	& \leq \displaystyle{\mathop{\sum}_{k' \neq k}} A * \mathbb{E}_{x,y \sim p(.|d=k')} [\mathbf{1}_{\mathbf{\Omega_{k,p_{0}}}^{c}}(h)]P(d=k') \\
	&+ \mathbb{E}_{x,y \sim p(.|d=k)}[l(h * (1-f(z_k)) + \\
	&+ a_k(h) * f(z_k),y) * \mathbf{1}_{\mathbf{\Omega_{k,p_{0}}}^{c}}(h)]P(d=k) \\
	&\leq A * \frac{\mathbb{\varepsilon}_{insep}}{p_{0}} + R_k * P(d=k)
\end{split}
\label{eq:5}
\end{equation}

The last inequality is verified by the below inequality:
\begin{equation}
\begin{split}
\displaystyle{\mathop{\sum}_{k' \neq k}} A * \mathbb{E}_{x,y \sim p(.|d=k')} [\mathbf{1}_{\mathbf{\Omega_{k,p_{0}}}^{c}}(h)]P(d=k') &\leq A * \displaystyle{\mathop{\sum}_{k' \neq k}} \mathbb{E}_{x,y \sim p(.|d=k') } [\frac{p_k(h)}{p_0}]P(d=k') \\
		& = \frac{A}{p_0} \mathbb{E}[\mathbf{1}_{d\neq k}p_k(h)] \\
		& \leq \frac{A}{p_0} \mathbb{\varepsilon}_{insep}
\end{split}
\end{equation}
because
$$ \mathbf{1}_{\mathbf{\Omega_{k,p_{0}}}^{c}}(h) \leq \frac{p_k(h)}{p_0} $$
Combining the inequality \ref{eq:4} and the inequality \ref{eq:5} we obtain the risk in general case of the adapted model $(M,a_k)$ $$R(M,a_k) \leq R + \frac{A * \mathbb{\varepsilon}_{insep}}{p_0} + P(d=k)R_k$$


\end{document}


