\documentclass[12pt,times,a4paper,twoside]{article}

\usepackage[utf8]{inputenc}	% Para caracteres en español
\usepackage[left=3cm,right=3cm,top=2cm,bottom=3cm]{geometry}
\usepackage{pagenote}

\usepackage{algorithm}
\usepackage{algorithmic}

\usepackage{amsmath,amsthm,amsfonts,amssymb,amscd}
\usepackage{mathtools,xparse}
\usepackage{mathrsfs}

\usepackage{multirow,booktabs}
\usepackage[table]{xcolor}
\usepackage{fullpage}
\usepackage{lastpage}
\usepackage{enumitem}
\usepackage{fancyhdr}

\usepackage{graphicx}
\usepackage{wrapfig}
\usepackage{setspace}
\usepackage{calc}
% \usepackage{color,soul}
\usepackage{multicol}
\usepackage{cancel}
\usepackage[retainorgcmds]{IEEEtrantools}
% \usepackage{xcolor} -- implied by xcolor

\usepackage{hyperref}

\colorlet{shadecolor}{orange!15}
\parindent 0in
\parskip 12pt
\geometry{margin=1in, headsep=0.25in}
\setlength{\belowdisplayskip}{8pt} \setlength{\belowdisplayshortskip}{8pt}
\setlength{\abovedisplayskip}{8pt} \setlength{\abovedisplayshortskip}{8pt}
\setlist{nosep}
\setlength{\parskip}{0.1cm}
\setlength{\parindent}{1em}


\title{Original decision letter}
\author{}
\date{}

\begin{document}
\maketitle
% \section*{General comments}
As TACL action editor for submission 2327, "Revisiting Multi-Domain Machine Translation", I am happy to tell you that I am accepting your paper subject (conditional) to your making specific revisions within two months.

Please address all comments by the reviewers, paying special attention to the following:
\begin{itemize}
	\item Address the concern by reviewer A about the difference between MDL Res and FT-Block, and include FT-block (and ideally FT-Full also) in the later experiments (5.2.2 and 5.3).
	\item Carefully specify the number of parameters for all models, including the proportions that are domain-specific and domain-independent.
	\item Provide all the experimental details requested, and in particular give enough information to allow the corpora to be reproduced, or (ideally) make them publicly available.
\end{itemize}

You are allowed one to two extra pages of content to accommodate these
revisions. To submit your revised version, follow the instructions in the
"Revision and Resubmission Policy for TACL Submissions" section of the
Author Guidelines at
\url{https://transacl.org/ojs/index.php/tacl/about/submissions\#authorGuidelines}.

Thank you for submitting to TACL, and I look forward to your revised
version! \\[2\parskip]

\noindent George Foster\\
\noindent Google Research\\
\noindent geord.foster@gmail.com

\section*{Reviewer A}

Overall, I'm a bit ambivalent about the paper. It is a well-written paper with a carefully-executed set of experiments. An important contribution of the paper is to formulate a set of expected properties of multi-domain MT (MDMT) (against which existing approaches are evaluated), and I generally like the choice of properties and that the paper draws attention to them, for example systematically testing how multi-domain approaches behave when gold domain labels are suboptimal. However, it is ambitious to systematically compare domain adaptation or multi-domain approaches which were developed with various conditions in mind. In several cases, I get the feeling that in an attempt to be broad and systematic, the authors have drawn boundaries and distinctions where none need exist, or drew conclusions that are too general. Details follow below.
\\
The authors have chosen a setting where domain labels are generally
available at training and test time, and in the the empirical comparison, focus on approaches that use domain labels at training and test time (533-537), but this is only partially true: their comparison does include systems which only use domain labels at training time (TTM/DM/ADM/WDCMT), and one set of experiments operates with automatically induced domain labels based on k-means clustering of sentence representations. Systems which were designed to work without domain labels, such as that by Farajian et al.(2017) would be a very relevant comparison at least in the latter setting without gold domain labels.
\\
The authors consider fine-tuning separate from MDMT, and declare failure in that no MDMT system meets the expectation that it should outperform fine-tuning. I'm confused why fine-tuning is specifically excluded, since it meets many of the reasons for building MDMT systems. Authors do mention that it would be cheaper to develop a single system instead of having to maintain multiple systems, but they include "MDL Res", which has domain-specific adaptation modules and is very similar to "FT-Block" (a baseline that also combines a general model with domain-specific adaptation modules). The line
between a single system and multiple systems gets fuzzy here, and I see no clear motivation to draw this clear distinction and only consider "MDL Res" a multi-domain system.
\\
The large number of approaches considered is admirable. However, the depth of the discussion suffers as a consequence, and the paper risks making over-general statements about the weaknesses of different approaches, despite related work that has already addressed them. The authors conclude that fine-tuning "incurs large losses [when applied on] the small data condition", and fails to deteriorate gracefully in the presence of tag errors, issues that have been addressed by Miceli Barone et al. (2017) and Thompson et al. (2019), respectively. Similarly, the negative impact of adding data from a "new" domain seems slightly overblown. Firstly, this is probably highly dependent on data mixtures, length of training and regularization strategies, so I'd be wary of drawing general conclusions from a single set of experiments. Secondly, when looking at the models where such a performance deterioration is said to happen (DC-Tag, TTM, ADM; lines
770-771), this deterioration is actually small or non-existent when
comparing table 3 and table 5 averages (-0.2 BLEU for DC-Tag; -0.3 BLEU for TTM; +0.9 BLEU for ADM). \\

\noindent Question to authors:
\begin{itemize}
\item can you describe in more detail the differences between FT-Block and MDL Res, apart from the fact that the former is fine-tuned from a generic model, the latter trained from scratch?  
\end{itemize}

\noindent Minor issues:
\begin{itemize}
\item I disagree that generalization to unseen domains is similar to zero-shot multilingual MT. In zero-shot multilingual MT, both the source and target distribution have been observed at training time, just paired with other languages. In the generalization to unseen domains, there may be characteristics in the data that were never seen in the training data, including new vocabulary (in the source text, target text, or both).
\item 567-568: "having the domain tag on the source is better than on the target". I assume this refers to the comparison of "DC-Tag" and "TTM". However, TTM actually uses the predicted domain label on the target side, and could perform better with forced decoding of the gold domain label. More precisely, the statement should be that "having the gold tag is better than a predicted tag", but this is hardly surprising.
\end{itemize}

\section*{Reviewer B}
The paper presents a thorough review of Multi-Domain Machine Translation (MDMT) approaches, and the authors provide comprehensive experiments to study different aspects of MDMT. The innovation of this work can be seen in two main categories:
\begin{itemize}
\item suggesting a methodology to evaluate MDMT models based on seven properties that they defined in the paper.
\item empirical studies on eight recent MTMD approaches.
\end{itemize}
In general, the paper has written well, and it is a good read for those who are researching in the domain adaptation field. 
\\
I have a few comments:
\begin{itemize}
\item[*] You provide some useful experimental results on various MDMT approaches. To benefit from your results, we need access to the same segmentation of train/dev/test sets that I hope you will provide to the community.
\item[*] On your discussion on the number of parameters (L. 591), could you share the number of parameters per each MDMT approach? It will be helpful to see the impact of the number of parameters on the MT results.  * Thinking about your seven operational requirements, I wonder if you could also define a domain or the minimum number of samples to specify a domain. I like your selections of domains that you include multiple domains with varying sizes, but what we expect in extreme cases. Having your Split experiments on REL (Table 4), or providing further details on your last analysis in Section 5.3, if you have tiny clusters could help.
\item[*] Please define Lambda coefficients in lines 142 and 146.
\item[*] On dynamically changing the number of training and test domains, there is a new related work published recently that might worth citing it: Sharaf A, Hassan H, Daumé III H. Meta-Learning for Few-Shot NMT Adaptation.
\item[*] I’d prefer to see the number of running words in Table 1, especially when the batch size is given as the number of tokens (L. 485).
\item[*] L. 503: It seems you refer to this method as LDR in the rest of the paper. Please make them consistent.
\item[*] L. 576: Although you describe in footnote 11, it is better to mention that you compare WDCMT results with Mixed-NAT-RNN explicitly.
\item[*] L. 681-683: I don’t see it as a failure, and I think it is a property (requirement) as you describe in the definition of [P4-ERR], we expect deterioration in the presence of tag error. Am I right?
\item[*]  L. 687: Please describe how you predict the domain tag, and please share the tag prediction error rate on each domain test set. It is helpful to see the amount and the impact of the error on MDMT results.
\end{itemize}

I spotted a few typos in the text:
\begin{itemize}
\item[*] L 100: section 2 $\rightarrow$ Section 2
\item[*] L 104: section 3 $\rightarrow$ Section 3
\item[*] L 256: could ben $\rightarrow$ could be (?)
\item[*] L 702, Table 4, Col. 5: MED1 $\rightarrow$ MED2
\item[*] L 790: that $\rightarrow$ than
\item[*] L 802-805: It’s better to bring DC-Feat results after DC-Tag to keep the consistency with other tables
\item[*] L 928: ge:en, do you mean de:en (German-English)?
\item[*] L 973: domains:this $\rightarrow$ domains this
\item[*] L 979: drawed $\rightarrow$ drawn
\end{itemize}

\section*{Reviewer D}
The authors deliver on their promise given in the title and abstract of the paper by doing exactly what I was hoping for: They provide a dense but comprehensive review of approaches to multi-domain MT. In a commendably straight and structured manner, they identify requirements and challenges of multi-domain MT. And they then proceed with empirically evaluating lots of multi-domain MT approaches.
\\
The empirical evaluation replicates more than ten multi-domain translation approaches from the literature and directly compares them against each other on the English-French language pair. The experiments are conducted using a coherent scenario with respect to both the amount of training data and the amount and type of distinct domains that the authors test on.
\\
The authors come up with a series of experiments that carefully considers the initially identified requirements and challenges. The (seven) identified challenges fall under the three categories "effectiveness", "robustness", and "scalability". There can be competing objectives when building multi-domain translation engines, and the results and discussion in the paper reveal the strengths and weaknesses of the various approaches in the considered scenario.
\\
Even though I cannot easily name the one striking contribution of this work, it's quite obvious to me that many fellow researchers will benefit from the clear presentation of multi-domain MT as well as from the extensive empirical results provided in this paper. I'd like to see it published.
\\
Let me nevertheless formulate a few questions or points of criticism:
\begin{itemize}
\item I'd like to see some analysis of vocabulary coverages (such as cross-domain OOV rates), and of "seen errors" and "new sense errors" as defined by Irvine et al. in "Measuring machine translation errors in new domains" (TACL 2013).
\item Maybe the (possible) effect of early stopping (and the choice of the validation set) should be discussed? E.g., the validation set for finetuning can originate from one single domain for each finetuned model. The way you're doing early stopping may even have had an impact on the results in Table 5. Is your validation set always a concatenation of 1000 lines from the six domains, without NEWS? Are you actually always training until the early stopping criterion is reached, and are you then using the model from the best checkpoint? (There are contradictory statements in the paper about "50,000 additional iterations" (?) for the experiments in Table 5.)
\item Report the learning rate. Is it decreased for finetuning?
\item To some extent, I dislike Table 5. When not reading carefully, the difference values could be interpreted as differences wrt. scores in Table 3. (Except that the numbers would be wrong...) Differences wrt. to Table 3 are important and it took me a while to obtain an overview for myself. Furthermore, it should be immediately obvious which number is "resumed training" and which is training from scratch. (It isn't from the table caption alone.)
\item How is "Mixed-Bal" achieved? By means of upsampling of the corpora from the less highly represented domains?
\item For "DC-Feat", is the feature added for each word or for each subword token? Is it attached to the word (increasing the vocabulary size a lot) or does it remain a separate token (increasing the input length a lot)?
\item How is the domain-weighted average (WAVG) calculated? Is the weight determined by proportion of the respective domain in the training data?
\item Wasn't it possible to use standard test sets? The "REL" Bleu scores are suspiciously high and there might be considerable redundancy in the data, or rather: training data contaminated with test data. (Since duplicates are possible, have you verified that the test sentences don't still appear in the remaining training data?)
\end{itemize}

Other (minor) remarks:
\begin{itemize}
\item From the UFAL medical corpus, please say that you're using the indomain sections only (which is mostly PatTR Medical).
\item The "DC-Feat" and "DC-Tag" lines in Table 5 should be swapped in order to be in line with Tables 3 and 4.
\item l. 503: "LDC-Feat" $\rightarrow$ "LDR"
\item l. 108: "automating" $\rightarrow$ "automatic"
\item l. 183: "accross" $\rightarrow$ "across"
\item l. 208: "under-resource" $\rightarrow$ "under-resourced"
\item l. 256: "ben" $\rightarrow$ "be"
\item l. 310: "used MT" $\rightarrow$ "used in MT"
\item l. 379: "Corpus statistics in Table 1." $\rightarrow$ "Corpus statistics are given in Table 1."
\item l. 380: "variable" $\rightarrow$ "available"
\item l. 426: "the all others" $\rightarrow$ "all others"
\item l. 696: "suplementary" $\rightarrow$ "supplementary"
\item l. 979: "drawed" $\rightarrow$ "drawn"
\item l. 991: "may be" $\rightarrow$ "maybe"
\end{itemize}
\end{document}


