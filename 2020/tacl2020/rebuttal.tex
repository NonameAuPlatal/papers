\documentclass[12pt,times,a4paper,twoside]{article}

\usepackage[utf8]{inputenc}	% Para caracteres en espa√±ol
\usepackage[left=3cm,right=3cm,top=2cm,bottom=3cm]{geometry}
\usepackage{pagenote}

\usepackage{algorithm}
\usepackage{algorithmic}

\usepackage{amsmath,amsthm,amsfonts,amssymb,amscd}
\usepackage{mathtools,xparse}
\usepackage{mathrsfs}

\usepackage{multirow,booktabs}
\usepackage[table]{xcolor}
\usepackage{fullpage}
\usepackage{lastpage}
\usepackage{enumitem}
\usepackage{fancyhdr}

\usepackage{graphicx}
\usepackage{wrapfig}
\usepackage{setspace}
\usepackage{calc}
% \usepackage{color,soul}
\usepackage{multicol}
\usepackage{cancel}
\usepackage[retainorgcmds]{IEEEtrantools}
% \usepackage{xcolor} -- implied by xcolor

\usepackage{listings}
\usepackage{spverbatim}
\usepackage{fancyvrb}
\usepackage{hyperref}
\usepackage{float} 
\usepackage{natbib}

\usepackage{tikz}
\usepackage{pgfplots}
\usepackage{pgfplotstable}

\usepackage{soulutf8}
\usepackage{tabularx}

\usepackage[draft]{todo}
\newcommand{\fyTodo}[1]{\Todo[FY:]{\textcolor{orange}{#1}}}
\newcommand{\fyTodostar}[1]{\Todo*[FY:]{\textcolor{orange}{#1}}}
\newcommand{\fyDone}[1]{\done[FY]\Todo[FY:]{\textcolor{orange}{#1}}}
\newcommand{\fyDonestar}[1]{\done[FY]\Todo[FY:]{\textcolor{orange}{#1}}}

\colorlet{shadecolor}{orange!15}
\parindent 0in
\parskip 12pt
\geometry{margin=1in, headsep=0.25in}
\setlength{\belowdisplayskip}{8pt} \setlength{\belowdisplayshortskip}{8pt}
\setlength{\abovedisplayskip}{8pt} \setlength{\abovedisplayshortskip}{8pt}
\setlist{nosep}
\setlength{\parskip}{0.1cm}
\setlength{\parindent}{1em}

\theoremstyle{definition}
\newtheorem{defn}{Definition}
\newtheorem{lemma}{Lemma}[section]
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}{Proposition}[section]
\newtheorem{definition}{Definition}[section]
\DeclareMathOperator*{\argmax}{argmax}
\newcommand{\R}{\ensuremath{\mathbb{R}}}

\pgfkeys{
    /tr/rowfilter/.style 2 args={
        /pgfplots/x filter/.append code={
            \edef\arga{\thisrow{#1}}
            \edef\argb{#2}
            \ifx\arga\argb
            \else
                \def\pgfmathresult{}
            \fi
        }
    }
}

\DeclarePairedDelimiter{\abs}{\lvert}{\rvert}
\DeclarePairedDelimiter{\norm}{\lVert}{\rVert}
\NewDocumentCommand{\normL}{ s O{} m }{%
  \IfBooleanTF{#1}{\norm*{#3}}{\norm[#2]{#3}}_{L_1}%
}

\newcommand{\domain}[1]{\texttt{\textsc{#1}}}
\newcommand{\system}[1]{\texttt{\textbf{#1}}}

\title{Responses to the reviews of TACL paper \#2327:
  ``Revisiting Multi-Domain Machine Translation''
}
\author{Action Editor: G. Foster}
\date{}

\begin{document}
\maketitle

The authors wish thank the reviewers and editor for their comments and suggestions, and have tried to address their questions and concerned in the revised version. Details are given below, with pointers to the main changes in the text. Typos and presentations errors have also been corrected in the new submission.

\section*{Cover mail}
As TACL action editor for submission 2327, "Revisiting Multi-Domain Machine Translation", I am happy to tell you that I am accepting your paper subject (conditional) to your making specific revisions within two months.

Please address all comments by the reviewers, paying special attention to the following:
\begin{itemize}
\item Address the concern by reviewer~A about the difference between \system{MDL Res} and \system{FT-Block}, and include \system{FT-block} (and ideally \system{FT-Full} also) in the later experiments (5.2.2 and 5.3). \\
  \textcolor{blue}{discusion XXX.}
\item Carefully specify the number of parameters for all models, including the proportions that are domain-specific and domain-independent. \\
  \textcolor{blue}{New table ...}
\item Provide all the experimental details requested, and in particular give enough information to allow the corpora to be reproduced, or (ideally) make them publicly available.\\
  \textcolor{blue}{Appendix ....}
\end{itemize}

You are allowed one to two extra pages of content to accommodate these
revisions. To submit your revised version, follow the instructions in the
"Revision and Resubmission Policy for TACL Submissions" section of the
Author Guidelines at
\url{https://transacl.org/ojs/index.php/tacl/about/submissions\#authorGuidelines}.

Thank you for submitting to TACL, and I look forward to your revised
version! \\[2\parskip]

\noindent George Foster\\
\noindent Google Research\\
\noindent geord.foster@gmail.com

\section*{Reviewer A}

Overall, I'm a bit ambivalent about the paper. It is a well-written paper with a carefully-executed set of experiments. An important contribution of the paper is to formulate a set of expected properties of multi-domain MT (MDMT) (against which existing approaches are evaluated), and I generally like the choice of properties and that the paper draws attention to them, for example systematically testing how multi-domain approaches behave when gold domain labels are suboptimal. However, it is ambitious to systematically compare domain adaptation or multi-domain approaches which were developed with various conditions in mind. In several cases, I get the feeling that in an attempt to be broad and systematic, the authors have drawn boundaries and distinctions where none need exist, or drew conclusions that are too general. Details follow below.
\\
The authors have chosen a setting where domain labels are generally
available at training and test time, and in the the empirical comparison, focus on approaches that use domain labels at training and test time (533-537), but this is only partially true: their comparison does include systems which only use domain labels at training time (TTM/DM/ADM/WDCMT), and one set of experiments operates with automatically induced domain labels based on k-means clustering of sentence representations. Systems which were designed to work without domain labels, such as that by Farajian et al.(2017) would be a very relevant comparison at least in the latter setting without gold domain labels.
\\
The authors consider fine-tuning separate from MDMT, and declare failure in that no MDMT system meets the expectation that it should outperform fine-tuning. I'm confused why fine-tuning is specifically excluded, since it meets many of the reasons for building MDMT systems. Authors do mention that it would be cheaper to develop a single system instead of having to maintain multiple systems, but they include "\system{MDL Res}", which has domain-specific adaptation modules and is very similar to "FT-Block" (a baseline that also combines a general model with domain-specific adaptation modules). The line
between a single system and multiple systems gets fuzzy here, and I see no clear motivation to draw this clear distinction and only consider "\system{MDL Res}" a multi-domain system.
\\
The large number of approaches considered is admirable. However, the depth of the discussion suffers as a consequence, and the paper risks making over-general statements about the weaknesses of different approaches, despite related work that has already addressed them. The authors conclude that fine-tuning "incurs large losses [when applied on] the small data condition", and fails to deteriorate gracefully in the presence of tag errors, issues that have been addressed by Miceli Barone et al. (2017) and Thompson et al. (2019), respectively. Similarly, the negative impact of adding data from a "new" domain seems slightly overblown. Firstly, this is probably highly dependent on data mixtures, length of training and regularization strategies, so I'd be wary of drawing general conclusions from a single set of experiments. Secondly, when looking at the models where such a performance deterioration is said to happen (\system{DC-Tag}, \system{TTM}, \system{ADM}; lines 770-771), this deterioration is actually small or non-existent when comparing table 3 and table 5 averages (-0.2 BLEU for \system{DC-Tag}; -0.3 BLEU for \system{TTM}; +0.9 BLEU for \system{ADM}).
\\
\textcolor{red}{%
  We have tried to address these commentaries in the revised version of the paper. Notably, we have clarified the distinction between \system{FT-block} and \system{MDL Res} (see details below). We have also tried to clarify the distinction between multi-domain MT and fine-tuning; while the former is designed with an a priori knowledge that the test data is sampled from a single domain, the latter assumes that the test data will be sampled from multiple domains, using domain probabilities that are not known a priori. This raises the issues of robustness wrt to adversarial test distribution and even new domains, two issues that are somewhat irrelevant for DA.

In our experiments, fine-tuned systems share no parameter and show large drops in performance when translating data from the wrong domain (even though this could be somewhat mitigated through regularization, as the reviewer points out). By contrast, the majority of the parameters of our MDMT systems are shared for all domains. Arguably, one could see here a continuum between no parameter sharing and full parameter sharing, with \system{MDL Res} and \system{FT Block} corresponding to some intermediay situation. 
}
\noindent Question to authors:
\begin{itemize}
\item can you describe in more detail the differences between \system{FT-Block} and \system{MDL Res}, apart from the fact that the former is fine-tuned from a generic model, the latter trained from scratch~?
  \\
  \textcolor{red}{%
    The two methods use the same architecture composed of one standard Transformer model extended with domain adapters following the design of \cite{Bapna19simple}. As the reviewer correctly points out, the only difference between \system{FT-block} and \system{MDL Res} is their training process and training data distribution.
    \\
    To train \system{FT-block}, we plug residual adapters to the output of each self-attention layer of the generic model Mixed-NAT, then resume the training with the learning rate attained at the end of the training the generic model while freezing all of the other parameters of the model \system{Mixed-NAT}. For this step, we only use only the in-domain data, as we do during the full-tuning procedure.
    \\
    To train MDL res, we optimize the parameters of the Transformer model and the parameters of the residual adapters from scratch. More precisely, to create a training batch, we randomly pick a domain with a probability equal to the proportion of the domain in training data, then sample a batch from the corresponding corpus. Given a batch from domain $i$, we simultaneously optimize the parameters of Transformer model and the parameters of the residual adapters for that domain.
    \\
    We understand that our naming schemes was insisting on training procedures, rather than on architectures, thus possibly introducing some confusions. We have accordingly changed the name of \system{FT-Block} to \system{FT Res}, clarified the description of the two systems (see lines YYY\fyTodo{fix this}), and reported \system{FT Res}\fyTodo{or new name} as another multi-domain system in all tables.\fyTodo{Make sure it is the case}.}
  %}
\end{itemize}

\noindent\textbf{Minor issues:}
\begin{itemize}
\item I disagree that generalization to unseen domains is similar to zero-shot multilingual MT. In zero-shot multilingual MT, both the source and target distribution have been observed at training time, just paired with other languages. In the generalization to unseen domains, there may be characteristics in the data that were never seen in the training data, including new vocabulary (in the source text, target text, or both).
  \\
  \textcolor{blue}{We agree with the reviewer that the analogy is misleading. As this is not central to our discussion, we have repharsed and dropped the comparison. See changes in line XXX}\fyTodo{Changes line in.}
\item 567-568: "having the domain tag on the source is better than on the target". I assume this refers to the comparison of "DC-Tag" and "TTM". However, TTM actually uses the predicted domain label on the target side and could perform better with forced decoding of the gold domain label. More precisely, the statement should be that "having the gold tag is better than a predicted tag", but this is hardly surprising.
  \\
  \textcolor{blue}{We agree that the conclusion is not really fair for "TTM" because "TTM" intend to predict domain label rather than to use gold domain label and TTM could definitively translate with gold domain label as prefix of each hypothesis. In our experiments, we found that TTM is pretty accurate in its prediction, making about 2\% errors in the predicted tags, which may explain the small difference with observe in Table~3. We have rephrased acccordingly in lines WWW.}\fyTodo{Add lines}
\end{itemize}

\section*{Reviewer B}
The paper presents a thorough review of Multi-Domain Machine Translation (MDMT) approaches, and the authors provide comprehensive experiments to study different aspects of MDMT. The innovation of this work can be seen in two main categories:
\begin{itemize}
\item suggesting a methodology to evaluate MDMT models based on seven properties that they defined in the paper.
\item empirical studies on eight recent MTMD approaches.
\end{itemize}
In general, the paper has written well, and it is a good read for those who are researching in the domain adaptation field. 
\\
I have a few comments:
\\
\begin{itemize}
\item[*] You provide some useful experimental results on various MDMT approaches. To benefit from your results, we need access to the same segmentation of train/dev/test sets that I hope you will provide to the community.
\textcolor{blue}{Supplementary details regarding the settings used in our experiments have been added in a new Appendix YYY.\fyTodo{Fix the appendix.}. We will also distribute the detailed composition of our train / dev / test split upon acceptance of the paper.}
\\
\item[*] On your discussion on the number of parameters (L. 591), could you share the number of parameters per each MDMT approach~? It will be helpful to see  the impact of the number of parameters on the MT results.
  \textcolor{blue}{This information has been added in Table~3\fyTodo{Check number, page and line number and content}.}
\\ 
\item[*] Thinking about your seven operational requirements, I wonder if you could also define a domain or the minimum number of samples to specify a domain. I like your selections of domains that you include multiple domains with varying sizes, but what we expect in extreme cases. Having your Split experiments on REL (Table 4), or providing further details on your last analysis in Section 5.3, if you have tiny clusters could help.\fyTodo{Answer this with more experiments ?}
  \\
  \textcolor{blue}{We agree that carefully analyzing the effects of using corpora of varying size is important, as this corresponds to practical training scenario. From our results in Table 6 (and Table XXX in appendix), we see that some techniques can work well with small in-domain corpora. However, this analysis must also take in account the similarity between domains, as these improvements will be stronger if similar or related data are also present in the training mixture -- a situation that we have artificially created in our split conditions. Precisely sorting out these two effects is beyond the scope of this study and left for future work.}
  
\item[*] Please define Lambda coefficients in lines 142 and 146.
\\
\textcolor{blue}{We formalize the data distribution as a mixture of several in-domain distribution. $\lambda$ correspond to the mixing weights, that also define the prior distribution on domains. We hypothesize that training distribution is different from testing distribution, so we have need notation for two different mixtures. This has been clarified in the text (see line~XXX) \fyTodo{add lines}}
\\
\item[*] On dynamically changing the number of training and test domains, there is a new related work published recently that might worth citing it: Sharaf A, Hassan H, Daum√© III H. Meta-Learning for Few-Shot NMT Adaptation.
\\
\textcolor{blue}{This reference has been added to the related work section.}\fyTodo{Add citation}
\\
\item[*] I‚Äôd prefer to see the number of running words in Table~1, especially when the batch size is given as the number of tokens (L. 485).
  \\
  \textcolor{blue}{The table has been updated accordingly.}\fyTodo{to report number of tokens of the datasets}
\\
\item[*] L. 503: It seems you refer to this method as LDR in the rest of the paper. Please make them consistent.
\\
\textcolor{blue}{Thanks for pointing this out. We have made this notation more consistent.}
\\
\item[*] L. 576: Although you describe in footnote 11, it is better to mention that you compare WDCMT results with Mixed-NAT-RNN explicitly.
\\
\textcolor{blue}{This has been added in the text (see line YYY).}\fyTodo{to explicitly compare WDCNMT with Mixed-NAT-RNN. This is in the legend - change presentations ?}
\\
\item[*] L. 681-683: I don‚Äôt see it as a failure, and I think it is a property (requirement) as you describe in the definition of [P4-ERR], we expect deterioration in the presence of tag error. Am I right?
\\
\textcolor{blue}{This is correct - a drop is performance is expected when using wrong domain tags; [P4-ERR] is however about robustness against domain errors and a good MDMT system should keep this loss small. This is not what we see for the systems that use domain tags in testing.}

\item[* L]. 687: Please describe how you predict the domain tag, and please share the tag prediction error rate on each domain test set. It is helpful to see the amount and the impact of the error on MDMT results.
\\
\textcolor{blue}{We train a language model in source language for each domain using respective source training corpus. To predict domain tag of a sentence, we compute probability of the sentence according to each pretrained language model and choose the domain corresponding to the highest probability as domain tag of sentence. Note that this classifier is only used for the \domain{news} test data. This is now explained in footnote ZZZ.}\fyTodo{to compute error rate of domain prediction over each test set, check comment, add footnote number.}
\end{itemize}

\section*{Reviewer D}
The authors deliver on their promise given in the title and abstract of the paper by doing exactly what I was hoping for: They provide a dense but comprehensive review of approaches to multi-domain MT. In a commendably straight and structured manner, they identify requirements and challenges of multi-domain MT. And they then proceed with empirically evaluating lots of multi-domain MT approaches.
\\
The empirical evaluation replicates more than ten multi-domain translation approaches from the literature and directly compares them against each other on the English-French language pair. The experiments are conducted using a coherent scenario with respect to both the amount of training data and the amount and type of distinct domains that the authors test on.
\\
The authors come up with a series of experiments that carefully considers the initially identified requirements and challenges. The (seven) identified challenges fall under the three categories "effectiveness", "robustness", and "scalability". There can be competing objectives when building multi-domain translation engines, and the results and discussion in the paper reveal the strengths and weaknesses of the various approaches in the considered scenario.
\\
Even though I cannot easily name the one striking contribution of this work, it's quite obvious to me that many fellow researchers will benefit from the clear presentation of multi-domain MT as well as from the extensive empirical results provided in this paper. I'd like to see it published.
\\
Let me nevertheless formulate a few questions or points of criticism:
\\
\begin{itemize}
\item I'd like to see some analysis of vocabulary coverages (such as
cross-domain OOV rates), and of "seen errors" and "new sense errors" as defined by Irvine et al. in "Measuring machine translation errors in new domains" (TACL 2013).
\\
\textcolor{blue}{Table~1 now contains the number of types as well as the number of types that are unique in a domain. We fill that performing an error analysis for each of the system / domain compared in this study would require us to focus more on a detailled analysis of each system, while our main goal is to propose new ways to assess the actual abilities of existing MDMT systems.}\fyTodo{Will this be enough ?}
\\
\item Maybe the (possible) effect of early stopping (and the choice of the validation set) should be discussed~? E.g., the validation set for finetuning can originate from one single domain for each finetuned model. The way you're doing early stopping may even have had an impact on the results in Table 5. Is your validation set always a concatenation of 1000 lines from the six domains, without NEWS~? Are you actually always training until the early stopping criterion is reached, and are you then using the model from the best checkpoint? (There are contradictory statements in the paper about "50,000  additional iterations" (?) for the experiments in Table 5.)
\\
\textcolor{red}{For each domain, we create train/dev/test sets by randomly each corpus. We maintain the size of validation sets and of test sets equal to 1,000 lines for every domain. In Tables 3 and 4, to train MDMT systems (except FT-full and FT Res), we use a validation set which is a concatenation of the 6 validation sets of 6 domains. In Table 5, to train MDMT systems (except \system{FT-Full} and \system{FT Res}) we append the validation set of \domain{news} to the concatenation of 6 validation sets. To train \system{FT-full} and \system{FT Res} model we only use the in-domain validation sets. We stop training if either the training iteration reach the maximal number of iterations or the score on the validation set does not increase in 3 consecutive evaluations. We average 5 checkpoints up to the best checkpoint to get the final model.}\fyTodo{up to the best ?}
\\
\item Report the learning rate. Is it decreased for finetuning?
\\
\textcolor{blue}{The learning rate is determined according to $lr = d_{model}^{-0.5}*\min(step\_num^{-0.5},step\_num * warmup\_steps^{-1.5})$ where $d_{model}=512$,$warmup\_steps=4000$, as in the original Transformer paper. For the finetuning procedure, we continue training using the same learning rate schedule and the same $step\_num$. This is now clarified in the appendix.}\fyDone{Fix this, add comments in appendix}
\\
\item To some extent, I dislike Table~5. When not reading carefully, the difference values could be interpreted as differences wrt. scores in Table 3. (Except that the numbers would be wrong...) Differences wrt. to Table 3 are important and it took me a while to obtain an overview for myself. Furthermore, it should be immediately obvious which number is "resumed training" and which is training from scratch. (It isn't from the table caption alone.)
\\
\textcolor{blue}{In Table 5, we report the difference in performance between 2 situations: resuming training and training from beginning with 7 domains. We agree that the comparison to Table 3 is also important, because helps assess the impact of a new domain to the previously learned domains, and have now reported this comparison in table 5. We have also modified the caption to clarify our reference situation (coldstart, with 7 domains). See changes in Table 5.}\fyDone{What should we do about it ?}
\\
\item How is \system{Mixed-Bal} achieved? By means of upsampling of the corpora from the less highly represented domains?
\\
\textcolor{blue}{In all of our experiments, to get a training batch, we first sample a domain from a set of available domains in training procedure with some fixed probability, then we sample a batch from the corpora corresponding to the previously picked domain. \system{Mixed-NAT} samples a domain based one the observed proportions of the training data (in Table~1), whereas \system{Mixed-Bal} samples domains with uniform probability.}
\\
\item For \system{DC-Feat}, is the feature added for each word or for each subword token? Is it attached to the word (increasing the vocabulary size a lot) or does it remain a separate token (increasing the input length a lot)?
\\
\textcolor{blue}{\system{DC-Feat} uses a small fixed embedding representing the domain. It is concatenated to every word embedding. The use of \system{DC-Feat} increases the size of the input embedding, but does not increase the vocabulary size nor the input length. This is clarified in the Appendix containing supplementary implementation details.}
\\
\item How is the domain-weighted average (WAVG) calculated? Is the weight determined by proportion of the respective domain in the training data?
\\
\textcolor{blue}{This is correct: the domain-weighted average is computed by weighting the performance of the model in each domain test with the proportion of the respective domain in the training data.}
\\
\item Wasn't it possible to use standard test sets? The "REL" Bleu scores are suspiciously high and there might be considerable redundancy in the data, or rather: training data contaminated with test data. (Since duplicates are possible, have you verified that the test sentences don't still appear in the remaining training data?)
\\
\textcolor{blue}{There are standard test sets for some domains such as medical, IT, news and talk, but we could not find any standard test sets for the other domains including religion, law and bank. We carefully removed every duplicate sentence in all the corpora. In our opinion, the reason why the models achieve extremely high scores in the \domain{rel} domain is because texts in this domain is very repetitive and often predictable.}\fyTodo{sentence length ?, diversity index of vocab ?}
\end{itemize}

Other (minor) remarks:

\begin{itemize}
\item From the UFAL medical corpus, please say that you're using the indomain sections only (which is mostly PatTR Medical).
\\
\textcolor{blue}{From the UFAL medical corpus of the language pair En-Fr, we select only sentences in categories including EMEA, ParTR, CESTA, ECDC. This is now mentioned in the text (lines XXX)}\fyTodo{Add lines num}
\\
\item The \system{DC-Feat} and \system{DC-Tag} lines in Table 5 should be swapped in order to be in line with Tables 3 and 4.
  \\
  \textcolor{blue}{We have changed the order of "DC-Feat" and "DC-Tag" to be consistent with tables 3 and 4.}
\end{itemize}

The remaining style issues have also been corrected.

\bibliographystyle{acl_natbib}
\bibliography{multidomain}
\end{document}


