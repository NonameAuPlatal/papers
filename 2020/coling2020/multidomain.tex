%
% File coling2020.tex
%
% Contact: feiliu@cs.ucf.edu & liang.huang.sh@gmail.com
%% Based on the style files for COLING-2018, which were, in turn,
%% Based on the style files for COLING-2016, which were, in turn,
%% Based on the style files for COLING-2014, which were, in turn,
%% Based on the style files for ACL-2014, which were, in turn,
%% Based on the style files for ACL-2013, which were, in turn,
%% Based on the style files for ACL-2012, which were, in turn,
%% based on the style files for ACL-2011, which were, in turn, 
%% based on the style files for ACL-2010, which were, in turn, 
%% based on the style files for ACL-IJCNLP-2009, which were, in turn,
%% based on the style files for EACL-2009 and IJCNLP-2008...

%% Based on the style files for EACL 2006 by 
%%e.agirre@ehu.es or Sergi.Balari@uab.es
%% and that of ACL 08 by Joakim Nivre and Noah Smith

\documentclass[11pt]{article}
\usepackage{coling2020}
\usepackage{times}
\usepackage{url}
\usepackage{latexsym}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage{longtable}
\usepackage{tikz}
\usetikzlibrary{calc}
\usepackage[draft]{todo}
\usepackage{xspace}
\usepackage{amsmath,amsfonts,amssymb}
%\setlength\titlebox{5cm}
%\colingfinalcopy % Uncomment this line for the final submission

% You can expand the titlebox if you need extra space
% to show all the authors. Please do not make the titlebox
% smaller than 5cm (the original size); we will check this
% in the camera-ready version and ask you to change it back.
\newcommand{\fyTodo}[1]{\Todo[FY:]{\textcolor{orange}{#1}}}
\newcommand{\fyTodostar}[1]{\Todo*[FY:]{\textcolor{orange}{#1}}}
\newcommand{\fyDone}[1]{\done[FY]\Todo[FY:]{\textcolor{orange}{#1}}}
\newcommand{\fyDonestar}[1]{\done[FY]\Todo[FY:]{\textcolor{orange}{#1}}}
\newcommand{\jcTodo}[1]{\Todo[JC:]{\textcolor{blue}{#1}}}
\newcommand{\jcDone}[1]{\done[JC]\Todo[JC:]{\textcolor{blue}{#1}}}

\newcommand{\src}{\ensuremath{\mathbf{f}}} % source sentence
\newcommand{\trg}{\ensuremath{\mathbf{e}}} % target sentence
\newcommand{\domain}[1]{\texttt{\textsc{#1}}}
\newcommand{\system}[1]{\texttt{\textbf{#1}}}
\newcommand{\vlambda}{\ensuremath{\boldsymbol\lambda}\xspace} % parameters vector for a distribution
\newcommand{\indic}[1]{\ensuremath{\mathbb{I}(#1)}}

\title{Revisiting Multi-Domain Machine Translation}

\author{First Author \\
  Affiliation / Address line 1 \\
  Affiliation / Address line 2 \\
  Affiliation / Address line 3 \\
  {\tt email@domain} \\\And
  Second Author \\
  Affiliation / Address line 1 \\
  Affiliation / Address line 2 \\
  Affiliation / Address line 3 \\
  {\tt email@domain} \\}

\date{}

\begin{document}
\maketitle
\begin{abstract}
Abstract goes here \fyTodo{Write abstract}
\end{abstract}

\section{Introduction} \label{sec:intro}

%
% The following footnote without marker is needed for the camera-ready
% version of the paper.
% Comment out the instructions (first text) and uncomment the 8 lines
% under "final paper" for your variant of English.
% 
\blfootnote{
    %
    % for review submission
    %
    \hspace{-0.65cm}  % space normally used by the marker
    Place licence statement here for the camera-ready version. See
    Section~\ref{licence} of the instructions for preparing a
    manuscript.
    %
    % % final paper: en-uk version 
    %
    % \hspace{-0.65cm}  % space normally used by the marker
    % This work is licensed under a Creative Commons 
    % Attribution 4.0 International Licence.
    % Licence details:
    % \url{http://creativecommons.org/licenses/by/4.0/}.
    % 
    % % final paper: en-us version 
    %
    % \hspace{-0.65cm}  % space normally used by the marker
    % This work is licensed under a Creative Commons 
    % Attribution 4.0 International License.
    % License details:
    % \url{http://creativecommons.org/licenses/by/4.0/}.
}

Data-based Machine Translation (MT), whether statistical or neural, rests on well understood machine learning principles. Given a training sample of matched source-target sentence pairs $(\src,\trg)$\jcDone{shoulnt trg be e as below ?} drawn from an underlying distribution $\mathcal{D}_s$, a model parameterized by $\theta$ (here, a translation function $h_{\theta}$) is trained by minimizing the empirical expectation of a loss function $\ell(h_\theta(\src), \trg)$. This approach ensures that the translation loss remains low when translating more sentences drawn from the same distribution.

Owing to the great variability of language data, this ideal situation is rarely met in practice, warranting the study of an alternative scenario, where the test distribution $\mathcal{D}_t$ differs from $\mathcal{D}_s$. In this setting, \emph{domain adaptation} (DA) methods are in order. DA has a long history in Machine Learning in general (eg.\ \cite{Shimodaira00improving,Ben-David09atheory,Pan10asurvey})\fyDone{cite non covariate shift} and in NLP in particular (eg.\ \cite{Daume06domain,Blitzer07domain,Jiang07instance}) and various techniques exist to handle both the situations where a (small) training sample drawn from $\mathcal{D}_t$ is available in training, or where only samples of source-side (or target-side) sentences are available (see \cite{Foster07mixture,Bertoldi09domain,Axelrod11domain} for proposals from the statistical MT era, or \cite{Chu18asurvey} for a recent survey of DA for Neural MT).
% In a nutshell, DA attempts to compensate for the distribution mismatch either by biasing the training sample to match ${\mathcal{D'}$ (eg.\ using resampling or instance-weighting strategies), or by adapting the 

A seemingly related problem is multi-domain (MD) machine translation \cite{Hassan17neural,Farajian17multidomain,Kobus17domaincontrol,Zeng18multidomain,Pham19generic} where one single system has to be trained as well as tested with data from multiple domains. MD machine translation corresponds to a very common situation, where all available data, no matter its origin, is used to train a system that will be robust to domain variability in testing.  
% As pointed out both by \newcite{Dredze08online} and \cite{Finkel09hierarchical}, this setting borrows much from DA, but also from multi-task learning \cite{Caruana97multitask,Yang15unified}.
If the intuitions behind MDMT are quite simple, the exact specifications of MDMT systems are rarely spelled out: for instance, should MDMT perform well when the test data is distributed like the training data, when it is equally distributed across domains or when the test distribution is unknown? Should MDMT also be robust to new domains? How should it handle domain labeling errors? % , which makes their evaluation fragile

A related question concerns the relationship between domain adaptation and multi-domain translation. The latter setting seems more challenging and one would expect that adapting a system to a known target domain $A$ will be easier than training a MDMT system, where only part of the data belongs to domain $A$. Are there situations where MD systems can surpass DA, as is sometimes expected?
 
In this paper, we try to study these questions and to spell out in a more precise fashion the requirements that a MDMT system should meet. Our first contribution is of methodological nature and consists of lists of expected properties of MDMT system and associated measurements to evaluate them. In doing so, we also shed lights on new problems that arise in this context, regarding for instance the accomodation of new domains in the course of training, or the computation of automating domain tags.\fyDone{New problems - continuous learning, automatic domains} Our second main contribution is experimental, and consists in a thorough reanalysis of eight recent multi-domain approaches from the literature, including a variant of a model initially introduced for DA. We show that\fyTodo{Spell out conclusions}.
 
\section{Requirements of multi-domain MT \label{sec:requirements}}
In this section, we try to recap the main reasons for considering a multi-domain scenario and discuss their implications in terms of performance evaluation.
\fyTodo{Relation to theory, discussion of worst case scenario}
\fyTodo{Insist on the work of Dredze}

\subsection{Complements - to be removed \label{ssec:oldformalization}}

We define a domain $d$ as a joint distribution $\mathcal{D}_d(x,y)$ over input-output pairs $(x,y) \in \mathcal{X}\times{}\mathcal{Y}$. Assuming that the training data is sampled from a known mixture of $D$ source domains $\mathcal{D}_s(x,y) = \sum_d \lambda_{s,d} \mathcal{D}_d(x,y)$, several cases are studied in the ML literature:

\begin{itemize}
\item the test distribution $\mathcal{D}_t$ is known 
  \begin{itemize}
  \item $\mathcal{D}_t = \mathcal{D}_{s}$: this is standard ML, with mixture densities
  \item $\mathcal{D}_t(x,y) = \sum_d \lambda_{t,d} \mathcal{D}_d(x,y) $ with a different mixture of domains in training and testing ($\vlambda_s \neq \vlambda_t$), corresponding to multiple source adaptation (MSA) \cite{Mansour09domainadaptation},
  \item $\mathcal{D}_t(x,y) \neq \sum_d \lambda_{t,d} \mathcal{D}_d(x,y) \forall \lambda$, where the target domain for adapation is not a mixture of source domains, studied in \cite{Mansour09multiple}
  \item variants of MSA which only assume knowledge of the marginal distribution $\mathcal{D}_t(x) = \sum_{y} \lambda_{t,d} \mathcal{D}_d(x) $
  \item more realistic situations where  the train and test distributions are estimated from finite samples, also studied in \cite{Mansour09multiple}.
  \end{itemize}
   Note that we recover standard supervised DA is when there is only one component in the training and test mixtures, and multiple-source single domain adaptation when the source is a mixture but the test domain is homogeneous. Unsupervised DA \cite{Daume06domain} corresponds to the case where only the marginal distribution of the test domain inputs is available, while semi-supervised DA is the reverse situation.

\item the test distribution is not known and training needs to be distributionally robust, ie.\ minimize the error against all possible distributions, with variants depending on whether it is or not assumed to be a mixture of existing domains, and whether it is perfectly or imperfectly known. This setting is also studied in eg.\ \cite{Mansour09multiple,Hoffman18algorithms}, and  in \cite{Oren19distributionally} for adversarial language model learning.
\item with respect to the conditional label distribution $P(y|x)$:
  \begin{itemize}
  \item $P(y|x)$ is deterministic, meaning that the joint distribution is entirely specified by the marginal and a labelling function $y=h(x)$; a more complex case is when $h()$ also varies across domains;  
  \item $P(y|x)$ is stochastic, but shared across all domains, which is the typical covariate-shift scenario \cite{Shimodaira00improving}; it can also vary across domains \cite{???}\fyTodo{Missing citation vary accross domains}
  \end{itemize}
\end{itemize}


Multi-domain learning, as defined in \cite{Dredze08online} additionally makes the requirement that categorial domain tags are available in training and testing; a multi-domain learner should use them effectively \cite{Joshi12multidomain} to improve simultaneously on all domains \cite{Finkel09hierarchical}. This is also the view that we adopt here, where we assume training inputs in $(\mathcal{X}\times{}\{1\dots{}D\})$, sampled from a mixture $\sum \lambda_{s,d}\mathcal{D}_{d} \times \{d\}$, and also consider various test scenarios, depending whether $\mathcal{D}_t$ is a fixed mixture of known domains, include new domain(s), in the supervised and unsupervised, and zero-shot settings.\fyTodo{explain zero shot}.


% ----
\subsection{Formalizing multi-domain translation \label{ssec:formalization}}

We conventionally define a domain $d$ as a distribution $\mathcal{D}_d(x)$ over some feature space $\mathcal{X}$ that is shared across domains \cite{Pan10asurvey}: in machine translation, $\mathcal{X}$ is the representation space for source sentences; each domain corresponds to a specific source of data, and differs from the other data sources in terms of textual genre, thematic content \cite{Zhang16topicinformed}, register, style \cite{Niu18multitask}, etc. Translation in domain $d$ is formalized by a translation function $h_d(y|x)$ pairing sentences in a source language with sentences in a target language $y \in \mathcal{Y}$. $h_d$ is usually assumed to be deterministic (hence $y = h_d(x)$, but can differ from one domain to the other.

A typical learning scenario in MT is to have access to a samples from $n_d$ domains, which means that the training distribution $\mathcal{D}^s$ is a mixture $\mathcal{D}^s(x) = \sum_d \lambda^{s}_{d} \mathcal{D}_d(x)$. Multi-domain learning, as defined in \cite{Dredze08online} further assumes that domain tags are also available in testing; the implication being that the test distribution is also as a mixture $\mathcal{D}^t(x) = \sum_d \lambda^{t}_{d} \mathcal{D}_d(x)$ of several domains, making the problem distinct from mere domain adaption.  A multi-domain learner is then expected to use these tags effectively \cite{Joshi12multidomain} when computing the combined translation function $h(x,d)$, and to perform well in all domains \cite{Finkel09hierarchical}. Note that this setting is closely related to the multi-source adaptation problem studied in \cite{Mansour09domainadaptation,Mansour09multiple}.

This definition seems to be the most accepted view of a multi-domain MT\footnote{One important exception is in \cite{Zeng18multidomain}, where test translations rely on a similarity score between sentences, rather than domain labels.}, and one that we also adopt here. Note that in the absence of further specification, the naive answer to the MD setting should be to train one distinct translation function $\hat{h}_d(x)$ for each domain, then translate using $\hat{h}(x,d) = \sum_{d'} h_{d'}(x) \indic{d' = d}$. We now discuss the arguments that are usually put forward to proceed differently.
% --

% This is also the view that we adopt here, where we assume training inputs in $(\mathcal{X}\times{}\{1\dots{}D\})$, sampled from a mixture $\sum \lambda_{s,d}\mathcal{D}_{d} \times \{d\}$, and also consider various test scenarios, depending whether $\mathcal{D}_t$ is a fixed mixture of known domains, include new domain(s), in the supervised and unsupervised, and zero-shot settings.\fyTodo{explain zero shot}.

\subsection{Reasons for building MDMT systems \label{ssec:whymdmt}}

A first motivation for moving away from the one-domain / one-system solution are practical  \cite{Sennrich13multidomain,Farajian17neural}: when faced with inputs that are potentially from multiple domains, it is easier and computationally cheaper to develop one single system instead of having to optimize and maintain multiple engines. The underlying assumption here is that the number of domains of interests can be large, a limiting scenario being fully personalized machine translation \cite{Michel2018extreme}.
% which is not always the case: for instance \cite{Britz17mixing,Zeng18multidomain,Jiang19multidomain} only consider 2-4 domains in their experiments.\fyTodo{Keep this last part ?}

A second line of reasoning rests on linguistic properties of the translation function and contends that domain specificities are mostly expressed lexically and will primarily affect content words or multi-word expressions; function words, on the other hand, are domain agnostic and tend remain semantically stable accross domains, motivating some cross-domain parameter sharing. A MDMT system should simultaneously learn lexical domain peculiarities, and leverage cross-domain similarities to improve the translation of generic contexts and words \cite{Zeng18multidomain,Pham19generic}. It is here expected that the MDMT scenario should be more profitable when the domain mix includes domains that are closely related and which can share more information.

A third series of motivations are of statistical nature. The training data available for each domain is usually unevenly distributed, and domain-specific systems trained or adapted on small datasets are likely to have a high variance and perform poorly. Training mix-domain systems is likely to reduce this variance, at the expense of a larger statistical bias \cite{Clark12onesystem}. Under this view, MDMT would be especially beneficial for domains with little training data. This is observed for multilingual MT from English: an increase for under-resource languages due to positive transfer, at the cost of a decrease in performance for well-resourced languages \cite{Arivazhagan19massively}.

Combining multiple domain-specific MTs can also been justified in the sake of distributional robustness \cite{Mansour09domainadaptation,Mansour09multiple}, for instance when the test mixture differs from the train mixture, or when it includes new domains unseen in training. Such scenario is already well documented for zero-shot multi-lingual MT \cite{Firat16multiway,Ha16towards,Johnson17google,Platanios18contextual}, where mixing languages has more than demonstrated its usefulness.  An even more challenging case is when the MT would need to perform well for any test distribution (as considered eg.\ in \cite{Hoffman18algorithms,Oren19distributionally}  for other tasks). In all these cases, mixing domains in training and/or testing is likely to improve robustness against unexpected inputs.

A distinct line of reasoning is that mixing domains can a positive regularization effect for all domains. By introducing variability in training, it prevents domain adaptation from overfitting the available adaptation data and could help improve generalization even for well-resourced domains. A related case is made in \cite{Joshi12multidomain}, where some of the benefits of MD training are attributed to an ensembling effect, where several systems from multiple domains are simultaneously used in the prediction phase\cite{Saunders19ucam}; this effect may subsist even in the absence of clear domain separations.

\fyDone{recap}
To recap, there are multiple arguments for adopting MDMT, some already used in DA settings, and some are new. These arguments are not mutually exclusive; however each yields specific expectations with respect to the performance of this approach, and should also yield appropriate evaluation procedure. If the motivation is primarily computational, then a drop in MT quality with respect to multiple individual domain might be acceptable if compensated by the computational savings. If it is to improve statistical estimation, then the hope will be that MDMT will improve, at least for some under-resourced domains, over individually trained systems. If finally, it is to make the system more robust to unexpected or adversarial test distributions, then this is the setting that should be used to evaluate MDMT. The next section discusses ways in which these various requireents of MDMT systems can ben challenged. 

% \cite[eq. (3)]{Wang20general} defines the loss of MDMT as the (macro) average of domain loss - all domains are equally important.

% \subsection{MDMT: a tentative definition \label{ssec:definition}}

% Informally, a multi-domain system should be able to accurately translate heterogeneous sources of data. This loose definition correspond to several practical situations:
% \begin{itemize}
% \item the more common situation is when both train and test documents (or sentences) are domain-tagged, where each domain corresponds to a specific source of data, and differ from the other sources in terms of textual genre, thematic content \cite{Zhang16topicinformed}, register, style \cite{Niu18multitask}, etc. It is usually assumed that the set of test domains is the same as the set of training domains -- when this is not the case, we would have zero-shot domain adaptation, similar to what is studied in multilingual MT. This is the more general setting, already proposed in \cite{Dredze09multidomain}.\fyTodo{do we need a name here: such as supervised MDMT ? and mostly supervised MDMT for the second case ?}

% \item It can also be the case that domain labels are only known in training, while test inputs may or may not be associated with explicit domains. This is, for instance, the setting adopted \cite{Zeng18multidomain}, where automatic domain identification is performed for test sentences. Note that automatic domain identification can also be effectively performed in the first setting, especially in the presence of close or related domains in training.
  
% \item A last setting, that has not received much attention for MT (see however \cite{Sennrich13multidomain}), is where both train and test domains are automatically infered from data, using clustering techniques. Even though domain labels are typically available in most training scenario, these approaches assumes that might be preferable, owing to the multiple sources of intra-corpus and inter-corpus variabilities, to resort to automatic domain assignments. The online approach of \newcite{Farajian17multidomain}, where distributional cues, rather that categorical tags, are used for fine-tuning, is a variant of this strategy.
% \end{itemize}

% In the rest of the paper, we mostly adhere to the first definition which correspond to a situation of practical interest for the industry, and assume that multi-domain MT is mostly about effectively using domain tags in training and testing. We also experiment in Section~\ref{sec:results} with the other two alternative scenarios, as they also define useful contexts to evaluate the actual robustness of MDMT systems.

% \jcDone{...a very common scenario in MT industry where clients dispose of mulitiple data corresponding to different domains (differing in genre, thematic, register, style, etc.). }

% \subsection{Evaluation issues \label{ssec:evaluation}}

% In theory, evaluation procedures should report realistic estimates of the test loss, computed on a sample deemed representative of the anticipated test conditions. Three scenarios need be distinguished:
% \begin{itemize}
% \item test data are mixed in the same proportion as training data; this warrants the design of an unbalanced mix-domain test sets in the same proportions as in training; alternatively, if per domain scores are ever reported, their average should take the mixing proportions into account;
% \item all domains are equally likely and important in testing (the prior over test domains is uniform), which would warrant the computation of per domain scores and of unweighted averages, as reported in \cite{Farajian17multidomain,Su19exploring,Wang20general}. Note that in this situation, the optimal training regime would be to balance the training set in the same way -- which is hardly ever done in practice, as the baseline scores always consider mixing unevenly all available data. \cite{Wang20general} uses an intermediate mixing regime, which tends to reduce part of the unbalance, and improve the performance for underrepresented domains.
% \item the test distribution is neither equal to the training set, nor uniform. Here again, if this distribution is known in advance, an optimal domain mix should be used in training to compute baseline scores, rather than using the empirical distribution of all the training data. A more challenging case is when the test set includes data from domains unseen in training, or when it is chosen adversarially \cite{XXX}.
% % : this situation, where test instances have a null probability in training, goes agains the basic statistical assumptions of DA \cite{Shimodaira00improving}. It is thus important that the likeliness of such events should be clearly formulated when describing the evaluation protocol.
% \end{itemize}

% The papers in the literature are seldom explicit regarding their assumptions regarding the test distribution. In most cases, per-domain performances are reported, and even sometimes averaged, suggesting that the scenario~2 most faithfully reflects the expected testing conditions. Another important issue is the choice of the evaluation metrics: if BLEU is still the standard metric used in most studies, it is not necessary the best choice to measure subtle improvements of lexical choices \cite{Irvine13measuring} that are often associated with adapted systems.

% A last issue regarding evaluation is related to the measurement of computational costs: when this is the main a motivation for using MDMT, we expect that computational costs would be reported and compared, which however is hardly ever done in practice. \fyTodo{Keep this ?}

\section{Challenging multi-domain translation systems \label{sec:challenging}}
In this section, we discuss seven\fyTodo{check number} operational requirements that can be expected from an effective multi-domain system, and discuss ways to evaluate whether these requirements are actually met.\fyTodo{Insert discussion about scores}

\subsection{Multi-domain systems should be effective \label{ssec:effective}}
% \subsubsection{Performance}
A first expectation is that MDMT systems should perform well in the face of mixed-domain test data. This yields the following three requirements.

\paragraph{[P1-LAB]}\fyTodo{Decide naming scheme} A MDMT should perform better than the baseline which disregards domain labels, or reassigns them in a random fashion \cite{Joshi12multidomain}. Evaluating this requirement is a matter of a mere comparison, assuming the test distribution of domains is known: if all domains are equally important, performance averages can be reported; if they are not, weighted averages should be used instead.

\fyTodo{Try random label assignments ?} % Testing that MDMT actually benefit from domain tags is simple, notwithstanding the previous remarks above regarding evaluation procedures.

\paragraph{[P2-TUN]} Additionally, one can expect that MDMT will improve over fine-tuning \cite{Luong15stanford,Freitag16fast}, at least in domains where data is scarce, or in situations where several domains are close. To evaluate this, we perform two measurements, using a real as well as an artificial scenario. In the real scenario, we simply compare the performance of MDMT and fine-tuning for domains of varying sizes, expecting a larger gain for smaller domains. In the artificial scenario, we split a single domain in two parts which are considered as distinct in training. The expectation here is that a MDMT should yield a clear gain for both pseudo sub-domains, which should benefit from the supplementary amount of relevant training. In this situation, MDMT should even outperform fine-tuning on either of the pseudo sub-domain.
\jcDone{I dont understand the prediction... why running MDMT on 2 artificial subdomains should show  gains over fine-tunning?}\fyTodo{small loss with respect to non split ?}

\subsection{Robustness with respect to domain definition \label{ssec:robusness}}
A second set of requirements is related to the definition of a domain. As repeatedly pointed out in the literature, parallel corpora used MT are often collected opportunistically and the view that each corpus constitutes a single domain is often a gross approximation.\footnote{Two of our own ``domains'' actually comprise several subcorpora (IT and MED), see details in section~\ref{ssec:corpora}.} MDMT should aim to make the best of the available data and be robust to domain assignments. To challenge these requirements we propose the evaluate the following requirements.

\paragraph{[P3-HET]}
The notion of a domain being a fragile one, an effective MDMT system should be able to discover not only when cross-domain sharing is useful (cf.\ requirement [P2-TUN])\jcDone{what is P2.2?}, but also when intra-domain heterogeneity is hurting. This requirement is tested by artificially conjoining two separate domains into one during training. The loss in performance with respect to the baseline (using correct domain tags) should then be as small as possible.

\paragraph{[P4-ERR]}
A MDMT should perform best when the true domain tag is known, but deteriorate gracefully in the presence of domain tag errors. This is in sharp contrast with what is commonly observed with fine-tuning, where giving the wrong domain tag can yield ``catastrophic'' drop in performance. This requirement is measured by decoding the test set for domain $A$ with all other domain tags, and report average drop in performance.

\paragraph{[P5-UNK]}
A related situation occurs when the domain of a test document is unknown. Several situations need be considered: when the domain is seen in training, then using automatically predicted domain labels should not be much worse than using the correct one. For test documents from other domains, the same procedure can be used, and should outperform the use of a single mixed-domain system.\fyTodo{Systems react to unknown domains}
% These requirements are respectively labelled [P5.1] and [P5.2]\fyTodo{Not so sure of these ones}.

\paragraph{[P6-DYN]}
Another requirement, more of an operational nature, is that a MDMT system should smoothly evolve to accomodate a growing number of domains, without having to retrain the full system each time new data is available. This is a requirement [P6-DYN] that we challenge by dynamically changing the number of training and test domains.

\subsection{Effectiveness in handling a large number of domains \label{ssec:scaling}}

\paragraph{[P7-NUM]} As mentioned above, MDMT systems have often been motivated by computational arguments. This argument is all the more sensible as the number of domains increases, making the optimization of many individual systems both ineffective and undesirable. For lack of having access to corpora containing very large sets (eg.\ in the order of 100-1000) domains, we experiment with automatically learned domains.\fyTodo{considering a varying number of clusters.}

\section{Experimental settings \label{sec:experiments}}

\subsection{Data and metrics \label{ssec:corpora}}

We experiment with translation from English into French and use data initially originating from 6~domains, corresponding to texts from the following data sources: the UFAL Medical corpus V1.0 (\domain{med})\footnote{\url{https://ufal.mff.cuni.cz/ufal_medical_corpus}}, the European Central Bank corpus (\domain{bank}) \cite{Tiedemann12parallel}; The JRC-Acquis Communautaire corpus (\domain{law}) \cite{Steinberger06acquis}, documentations for KDE, Ubuntu, GNOME and PHP from Opus collection \cite{Tiedemann09news}, collectively merged in a \domain{it}-domain, Ted Talks (\domain{talk}) \cite{Cettolo12wit}, and the Koran (\domain{rel}). Complementary experiments also use v12 of the News Commentary corpus (\domain{news}). Corpus statistics in Table~\ref{tab:Corpora}.  Most corpora are variable from the Opus web site.\footnote{\url{http://opus.nlpl.eu}} These corpora were deduplicated and tokenized with in-house tools. To reduce the number of types and build open-vocabulary systems, we use Byte-Pair Encodings \cite{Sennrich16BPE} with 30,000 merge operations on a corpus containing all sentences in both languages.%

We randomly select in each corpus a development and a test set of 1,000 lines and keep the rest for training. Validation sets are used to chose the best model according to the average BLEU score \cite{Papineni02bleu}.\fyDone{A word about meta-parameter settings}, computed with\fyTodo{}. Significance tests are performed using the bootstrap resampling method of \newcite{Koehn04statistical} and implementation of compare-mt\footnote{\url{https://github.com/neulab/compare-mt}} \cite{Neubig19compare-mt}. We report significant differences at the level of $p=0.05$\fyDone{Fix correct p value}.

%for contrast experiments, we also use supplementary test sets from three other domains: the official Khresmoi testset \cite{Khresmoi17test}, which is close to EMEA, News test 2014 \cite{Bojar14findings}, and IWSLT 2010 (Talk track) \cite{Paul10overview}. This enables us to evaluate the loss in performance when the test set is from a domain not seen in training.
% The model is also required to achieve comparable performance to generic model. To do so, we use newstest 2009 and IWSLT 2010 whose contain does not particularly belong to any domain.

\begin{table}[h]
  \centering
  \begin{tabular}{ |lllllll|} %*{4}{|r|}}
    \hline
    %\multicolumn{4}{|l|}{Vocab size - En: 30,165, Fr: 30,398}\\
    \domain{med} & \domain{law} & \domain{bank} & \domain{it} & \domain{talk} & \domain{rel} & \domain{news} \\
    \hline
    2609 (0.68) & 190 (0.05)  & 501 (0.13) & 270 (0.07) & 160 (0.04) & 130 (0.03) & 260 (0) \\
    \hline
  \end{tabular}
\caption{Corpora statistics: number of parallel lines ($\times 10^3$) and proportion in the basic domain mix (which excludes the \domain{news} domain).}
\label{tab:Corpora}
\end{table}

We measure the distance between domains using the $\mathcal{H}$-Divergence introduced in \cite{Ben-David09atheory}, which relates domain similarity to the test error of a domain discriminator: the larger the error, the closer the domains. Formally, given test sets of size $m$ for domains $A$ and $B$, and $h(x)$ a trained domain predictor, $\mathcal{H}(A,B)$ is computed as:
$$
\mathcal{H}(A,B) = 2(1 - [\frac{1}{m} \sum_{x:h(x) = B} \mathbb{I}( x \in A) + \frac{1}{m} \sum_{x: h(x) = A} \mathbb{I}(x \in B)]),
$$
where $\mathbb{I}$ is the indicator function. Our domain discriminator is SVM independently trained for each pair of domains with sentence representations derived via mean pooling from the source side representation of our generic Transformer model. We used the sci-kit learn implementation with default values.\fyDone{Inform the classifier details}\fyDone{Insert tableau} Results in Table~\ref{tab:domaindist} show that all domains are well defined and separated from the all others, with \domain{rel} being the furthest apart, while \domain{talk} is slightly more central.

\begin{table}\centering
  \begin{tabular}{|l*{5}{|r}|} \hline
    & law & bank & talk & IT & rel \\ \hline
    med &1.93 &1.97 &1.9 &1.93 &1.97 \\
    law   && 1.94 & 1.97 &1.93 & 1.99 \\
    bank &&&1.98 &1.94 &1.99 \\
    talk   &&&&1.92 &1.93 \\
     IT     &&&&& 1.99 \\ \hline
  \end{tabular}
  \caption{The $\mathcal{H}$-divergence between domains}
  \label{tab:domaindist}
\end{table}

\subsection{Baselines \label{ssec:baselines}}

Baselines correspond to standard practices for multi-domain systems.\footnote{We however omit domain-specific systems trained with the corresponding subset of the data, which are always inferior to the mix-domain strategy eg.\ \cite{Britz17mixing}.} Using the Transformer architecture of \shortcite{Vaswani17attention} implemented in OpenNMT-tf\footnote{\url{https://github.com/OpenNMT/OpenNMT-tf}} \cite{Klein17opennmt}, we build the following systems:

\begin{itemize}
\item a generic model trained on a concatenation of all corpora (\texttt{Mixed}). We develop two versions of this system, one where domain unbalance reflects the distribution of our parallel data (\system{Mixed-Nat}) and one where all domains are equally represented in training (\system{Mixed-Bal}). As explained above, depending whether MDMT are expected to perform specially well on large domains, or to treat all domains equally, one or the other training distribution should be prefered.\jcTodo{All systems cand be learned following both Bal and Nat. does Nat corresponds to wAVG in test? is wAVG defined somewhere?}
\item models fine-tuned \cite{Luong15stanford,Freitag16fast}\jcDone{i would use Luong and Manning 2015} separately on each domain for at most 20~000 iterations, with early stopping when the dev BLEU stops increasing. In our setting, we again contrast two versions: full fine-tuning (\system{FT-Full}), which may update all the parameters of the initial generic model; and the variant of \cite{Bapna19simple}, where fine-tuning only updates a simple adaptation module that is added (with residual connexions) on top of every Transformer layer (\system{FT-Block}).
\end{itemize}

For all models, we set the embeddings size and the size of hidden layers to~512. Transformers use multi-head attention with 8 heads in each of the 6 layers; the inner feedforward layer contains 2048 cells. The multi-domain residual system (see description below) additionally use an adaptation block in each transformer layer, composed of 2-layer perceptron, with an inner RELU activation function operating on normalized entries of dimension 1024. The gated variant is made of a dense layer, followed by a layer normalization and a sigmoid activation.
% The domain control systems are exactly as their baseline counterparts (RNN and Transformer), with an additional 2 cells encoding the domain on the input layer.
 %
Training uses Adam with parameters $\beta_1=0.9$, $\beta_2= 0.98$ and Noam decay ($warmup\_steps=4000$). In all cases, we use a batch size of~128 and a dropout rate of 0.1 for all layers. % All our systems are implemented in OpenNMT-tf\footnote{\url{https://github.com/OpenNMT/OpenNMT-tf}} \cite{Klein17opennmt}.
\fyDone{Describe the block adaptation layer - voir slides} 

\subsection{Multi-domain systems \label{ssec:systems}}
\fyDone{Remove Gated residual}
Our comparison of multi-domain systems includes the following reimplementations of recent proposals from the literature:
\begin{itemize}
\item a system using domain control as in \cite{Kobus17domaincontrol}: domain information is introduced either as an additional token for each source sentence (\system{DC-Tag}), or introduced in the form of a supplementary feature for each word (\system{DC-Feat}).
\item the three proposals of \cite{Britz17mixing}: \system{TTM}, another tag-based approach where the domain tag is introduced as an extra word \textsl{on the target side}. Training is performed with reference tags and inference is performed with predicted tags, just like for any other target words; \system{DM} is a multi-task learner where a domain classifier is trained on top the MT encoder, so as to make it aware of domain differences; \system{ADM} is the adversarial version of \system{DM}, pushing the encoder towards learning domain-independent source representations;
\item a system using lexicalized domain representations \cite{Pham19generic}: word embeddings are composed of a generic and a domain specific part (\system{LDC-Feat});
\item the multi-domain model of \cite{Zeng18multidomain} (\system{WDCMT}): in this approach two representations, a domain-agnostic and a domain-specialized representation of the input are simultaneously processed; supervised classification models and adversarial training are used to compute these representations.\footnote{For this system, we use the available RNN-based system from the authors (\url{https://github.com/DeepLearnXMU/WDCNMT}\fyDone{URLs}) which does not directly compare to the other, Transformer-based, systems; the improved version of \cite{Su19exploring} seems to produce comparable, albeit slighly improved version.}
\item an original, multi-domain version of the approach of \newcite{Bapna19simple}, denoted \system{MDL Res}, where we add a dedicated adaptation module for each domain, which is only activated for the appropriate set of training data; within each layer, residual connexions make it possible to by-pass this module.
  % In a variant (\system{MDL Gated}), we use a gating mechanism to merge computations using the the adapter modules and with those that don not.
Note that contrarily to \cite{Bapna19simple}, we do not start with a trained generic system, but learn the multi-domain from scratch. \fyDone{Check this.}
\end{itemize}

Note that we did not include here the system of \cite{Farajian17multidomain} which performs on-the-fly tuning for each test sentence, and slightly diverges from our notion of MDMT, which should be able to handle available domain information in training and testing. This system has shown to meet many of our requirements and would also have no problem scaling to a large number of domains.\fyDone{TBC}

\section{Results and discussion \label{sec:results}}

\subsection{Performance of MDMT systems \label{ssec:rawperformance}}
In this section, we discuss the raw performance of MDMT systems trained and tested on $6$~domains. Results are in Table~\ref{tab:performance}. As expected, balancing data in the generic setting makes a great difference (the unweighted average is 2~BLEU points\jcDone{BP?} better, notably owing to the much better results for \domain{rel}). As explained above, this setting should be the baseline when the test distribution is assumed to be balanced across domains. As all other systems are trained with an unbalanced data distribution, we use the weighted average to perform global comparisons.

Fine-tuning each domain separately provides us with better baselines, outperforming \system{generic} for all domains, with significant gains for domains that are distant from \domain{med}: \domain{rel}, \domain{it}, \domain{bank}, \domain{law}. A complete fine-tuning, updating all parameters, proves more effective than only adapting a subpart of the network - a result that could be due to keeping fixed the capacity of the adapter module.\fyDone{Add 2 averages ?}\fyDone{Significance testing wrt Mix Generic, Full Fine-tuned, for each domain}

A  first group of MDMTs use an extra domain feature, at the sentence or at the word level: among these, having the domain tag on the source is better than on the target, and very comparable to repeating the tag for every source word. If all MDMTs (except \system{DM} and \system{ADM}) slightly improve over the generic baseline (for most domains), these improvements are rarely significant, the best contender overall being \system{MDL Res}, which significantly improves over \system{Mixed Nat} for three domains, and approximately halves the gap between the baseline and \system{FT-Full}, and can be considered to match [P1-LAB].

All participating systems fail to outperform fine-tuning, failing to match [P2-TUN]. This might be expected, even with all significantly outperform the baseline for \domain{rel}, the gap with fine-tuning remains large for an ``isolated'' domain such as \domain{rel}, 
\fyTodo{More comments when we have all the results}
The use of an decidated adaptation component in each of the transformer layer proves an even better strategy and closes the gap with fine-tuning for half of the domains: the use of a gating mechanism yielding a clear improvement over the board. The effect of the adaptation layer is especially significant for the smaller domains (\domain{bank}, \domain{it} and \domain{rel}) for which unplugging this layer causes dramatic performance drops. 

\fyTodo{Importance of sharing and unsharing}

\begin{table*}
  \centering
  \fyDone{Fix column size}
  \begin{tabular}{|p{3cm}|*{8}{r|}|r|} \hline
%     &&&&&& \\
    Model / Domain & \multicolumn{1}{c|}{\domain{ med}} & \multicolumn{1}{c|}{\domain{ law}} & \multicolumn{1}{c|}{\domain{bank}} & \multicolumn{1}{c|}{\domain{talk}} & \multicolumn{1}{c|}{\domain{ it }} & \multicolumn{1}{c|}{\domain{ rel}} & \multicolumn{1}{c|}{w\domain{avg}} & \multicolumn{1}{c||}{\domain{avg}} & \multicolumn{1}{c|}{\domain{news}} \\ \hline
    \system{Mixed-Nat}  & 37.3 & 54.6 & 50.1 & 33.5 & 43.2 & 77.5  & 41.1  & 49.4 & 23.5\\
    \system{Mixed-Bal}   &  35.3 & 54.1 & 52.5 & 31.9 & 44.9 & 89.5 & 40.3  & 51.4 &\\
    \system{FT-Full}       & 37.7 & \bf 59.2 & \bf 54.5 & 34.0 & \bf 46.8 & \bf 90.8   & 42.8 & 53.8 &\\
   \system{FT-Block}     & 37.3 & \bf 57.9 & 53.9 & 33.8 & \bf 46.7 & \bf 90.2    & 42.3 & 53.3 &\\ \hline
 %   Full-finetuned on extended in-domain corpora (news) & && 33.96&&& & &\\nn
    \system{DC-Tag}       & 38.1 & 55.3 & 49.9   & 33.2 & 43.5 & \bf 80.5 &41.7 & 50.1     & 21.8 \\
    \system{DC-Feat}      & 37.7  & 54.9 & 49.5   & 32.9 & 43.6 & \bf 79.9 &41.4 & 49.9   & \sl 21.7 \\
    \system{TTM}            & 37.3 & 54.9 & 49.5 & 32.9 & 43.6 & \bf 79.9 &41.1 & 49.7       & \\
    \system{LDC-Tag}     & 37.0   & 54.7 & 49.9 & 33.9 & 43.6 & \bf 79.9 & & 49.8 & 22.1\\ 
    \system{DM}             & \sl 35.6 & \sl 49.5  & \sl 45.6& \sl 29.9 & \sl 37.1 & \sl 62.4 & 39.7 & 43.8 & \\
    \system{ADM}           & 36.4 & \sl 53.5  & \sl 48.3 & \sl 32 & \sl 41.5 & \sl 73.4 & 40.7 & 47.5 & \\
    \system{WDCMT}       & 34.6 & 51.7 & 47.5 & 29.4 & 36.5 & 49.3 & & 41.5 & \\
    \system{MDL Res}     & 37.9 & \bf 56.0  & \bf 51.2   & 33.5   &  44.4  & \bf 88.3 & 42.1 & 51.9  & \sl 21.2 \\
    \hfill MDL Res (gen)           & 37.7 & 51.0 & 34.0 & 30.4 & 34.2 & 15.2 & 36.4 & 33.7&\\
%    \system{MDL Gated} & 37.7 & 56.5 & 53.2 & 34.1 & 44.6 & 90.7 & 42.3 & 53.3&\\
     \hline
  \end{tabular}
  \caption{Translation performance of various MDMT systems. We report the BLEU score for each domain, as well as domain-weighted (w\domain{avg}) and unweighted (\domain{avg}) averages. Boldface denotes significant differences with respect to the \system{Mix-Nat} system. Scores for the \domain{news} domain correspond to the out-of-domain scenario (see Section~\ref{ssec:redomains}).}
  \label{tab:performance}
\end{table*}
\fyTodo{Fill the table with all results, including a comparison with *RNNs* for wcmd}
\fyDone{A new column for the News domain with predicted domains (except for the generic case).}

\subsection{Redefining domains \label{ssec:redomains}}
Table~\ref{tab:redomains} summaries the results of four experiments where we artificially redefine the limits of domains. In the three \textsl{split} experiments, we randomly split a domain corpus in two part: a good MDMT system should detect that these two ``domains'' should be mutually beneficial, and should be hardly affected by this change wrt.\ the baseline scenario. This is the situation where we should see clear improvements even over finetuning, which only exploits part of the available relevant data. In the \textsl{merge} experiment, we do the reverse and merge two corpora in training, in order to assess the robustness with respect to heterogenous domains. \fyTodo{Prepare table for the wrong domain analysis (see also last col. of the first table)}In the last column, we finally report the averaged (across domains) drop in performance when the domain information is erroneous.

\begin{table}
  \centering
  \begin{tabular}{|p{2.5cm}|*{9}{r|}} \hline
    % &&&&&& \\
    \hfill Set-up & \multicolumn{2}{c|}{Split} &  \multicolumn{2}{c|}{Split} & \multicolumn{2}{c|}{Split} & \multicolumn{2}{c|}{Merge} & Wrong \\ % \hline
     Model \hfill & \multicolumn{2}{c|}{\domain{med} \footnotesize{(0.5 / 0.5)}} &  \multicolumn{2}{c|}{\domain{med} {\footnotesize (0.25 / 0.75)}} & \multicolumn{2}{c|}{\domain{bank} {\footnotesize (0.5 / 0.5)}} & \multicolumn{2}{c|}{\domain{bank}+\domain{law}} & domain \\ \hline
    & \domain{med-1} & \domain{med-2} & \domain{med-1} & \domain{med-2} &  \domain{bank-1} & \domain{bank-2} & \domain{bank} & \domain{law} & \\
    \system{FT-Full}      & -0.13 & -0.6 & -1.51& -0.25& & & & & \\
    \system{DC-Tag}     & -0.16 & -0.27& +0.13& +0.2& & & & & \\
    \system{DC-Feat}    & -0.47& 0.0 & +0.27 $^+$ & +0.27 $^+$& & & & & \\
    \system{TTM}          & & & & & & & & &  \\
    \system{LDC-Tag}   & & & & & & & & & \\ 
                     
    \system{MDL Res}   & -0.2$^+$& -0.07$^+$&+0.18$^+$  &+0.0$^+$ & & & & & \\
%    \system{MDL Gated} & & & & & & & & & \\

    \system{ADM} & & & & & & & & & \\
    \system{DM}    & & & & & & & & & \\
    \system{WDC}  & & & & & & & & & \\
    \hline
  \end{tabular}
  \caption{Translation performance with variable domain definitions. For the Split experiments, we only report the loss in performance for the \domain{med} test set, (*) denote significant losses when domains are split or merged; (+) denotes a significant improvement over \system{Full FT}; \fyTodo{How about wrong ?}}
  \label{tab:redomains}
\end{table}

Our main findings can be summarized as follows: for the split experiments, we see a general loss with respect to the situation were domains are kept a whole, except in rare cases\fyTodo{To be completed}. As expected, \system{FT} incurs a large loss, especially for the small split. 
Findings so far:
\begin{itemize}
\item Split UFAL half / half: domain-tag and Gated-res are OK, all systems loose wrt baseline; 
\item Split UFAL 0,75 / 0.25: same thing, and we even see improvements for domain tags - they are small though
\item Split Law : only domain feature works ? all the rest have losses, from large to small
\item Merge domain: bad for many, OK for TTM (as the domain is learnt this is not so bad ?); bad for domain tag, domain feature is again OK ?
\end{itemize}

\fyTodo{We do need significance testing (a) each system against itself (non-split); (b) each system agains FT in split  condition}

Another series of experiments evaluate the ability to handle additional domains (requirement [P6-DYN]) as follows. Starting with the existing MDMT systems of Section~\ref{ssec:rawperformance}, we introduce an additional domain (\domain{News}) and resume training with a mixture of data\footnote{The design of a proper mixture is again of utmost importance for achieving optimal performance: since our goal is to evaluate all systems in the same conditions, we consider a basic mixing policy where. This is of course detrimental to the small domains, for which the ``negative transfer'' effect is stronger than for larger domains.} for 2 additional epochs.\fyTodo{Training regime of continuation}. We contrast this approach with retraining all the systems from scratch, and report the difference in performance in Table~\ref{tab:XXX}. We expect that a good MDMT system should not be significantly affected by the addition of new data, and reach the same performance as if the training included this domain from scratch. It should be noted that dynamically integrating new domains is straightforward for \system{DC-Tag}, \system{TTC} or \system{DC-Feat} where adding a new domain simply adds a new tag. It is less so for \system{LDC} \fyTodo{Complete effects of new domain}, for which the number of possible domains is built in the architecture and has to be anticipated from the begining. This makes a first difference between domain-bounded MT-systems, for which the number of possible domains is upper bounded; and the open-domain systems, which seamlessly integrate additional domains.

\begin{table*}
  \begin{tabular}{|p{3cm}|*{9}{r|}} \hline
    Model / Domain & \multicolumn{1}{c|}{\domain{ med}} & \multicolumn{1}{c|}{\domain{ law}} & \multicolumn{1}{c|}{\domain{bank}} & \multicolumn{1}{c|}{\domain{talk}} & \multicolumn{1}{c|}{\domain{ it }} & \multicolumn{1}{c|}{\domain{ rel}} & \multicolumn{1}{c|}{\domain{news}} & \multicolumn{1}{c|}{w\domain{avg}} & \multicolumn{1}{c||}{\domain{avg}} \\ \hline
    
    \system{Mixed-Nat}  & 37.3  & 54.6   & 50.1   & 33.5  & 43.2  & 77.5  & & 41.1 & 49.4 \\
    \system{FT-Full}       & 37.7  & 59.2   & 54.5   & 34.0  & 46.8  & 90.8  & & 42.8 & 53.8 \\ \hline\hline
     \system{DC-Feat}     & 37.4 & 54.9   & 50.0    & 34.7  &  43.9  & 79.6 & 28.9 \\
                                    & 36.9 & 53.7   & 49.1    & 33.7  &  42.8  & 74.1 & 28.9 &\\
    \system{DC-Tag}      &  37.7 & 54.5   & 49.9    &  34.8 &  43.9  & 78.8 & 29.5 \\
                                   & 38.0  & 54.0   & 48.8    & 33.5  &  42.7  &73.2  & 28.3& \\
    \system{TTM}           &  37.3 & 54.4   & 49.6    & 33.8  &  42.9  &78.1  & 29.1 \\
                                   & 36.8  & 53.4   & 47.9    & 32.4  &  42.5  &72.8  & 28.4& \\
    \system{LDC-Tag}    & 37.0   & 54.6  & 49.6    & 34.3  &  43.0  &77.0  & 28.7 \\
    continual                 &&&&&& \\
    \system{DM}            &36.0 &51.3&46.8&31.8&39.8&65.7&27.0 \\
                                  &36.5&51.6&47.3&31.7&40.3&65.7&25.8& \\
    \system{ADM}          &36.6&54.2&49.1&32.9&42.1&75.7&28.7 \\
                                  &  36.9 &53.5&48.3&32.7&41.7&70.7&26.8& \\
    \system{WDCMT}     &34.8  &51.8&47.8&30.7&37.3&49.3&25.9 \\
    continual                 &&&&&&&& \\
    \system{MDL-Res}    &37.7   & 55.6   & 51.1   & 34.4  & 44.5  & 87.5  & 29.1 & & \\
     continual                &&&&&&&& \\
     \hline
  \end{tabular}
  \caption{Ability to handle new a domain: BLEU performance in ``resume training'' mode compared to a complete training session.}
  \label{tab:warmrestart}
\end{table*}
\fyTodo{Insert table, significancy testing against full learning condition, for each domain}

Considerations, \fyTodo{To be completed}: 

\begin{itemize}
\item even when trained from entirely scratch, very few systems are able to take advantage of the additional data for the 6 initial domains - this is observed on the averaged scores in Table~\ref{tab:warmrestart} with hardly surpass those in Table~\ref{tab:performance}
\item restart is computationally cheaper (\fyTodo{how much cheaper here ?}) but underperforms cold start for all systems, sometimes by a wide margin.
\item the loss is largest for the small domains - how about distance to \domain{news} ?
\end{itemize}
\fyDone{Very close to fine tuning towards News, no ?}

\subsection{Automatic domains \label{ssec:autodomains}}
In this section, we experiment with automatic domains, obtained by clustering sentences into $k=30$ classes using the k-means algorithm based on generic sentence representations obtained via mean pooling as in section~\ref{ssec:corpora}. This allows us to evaluate requirement [P7-scale], training and testing our systems as if these domains were genuine. We also note that many of these classes are mere split of the large \domain{med}, while a few number of classes being mix of two existing domains. We are thus in a position to reiterate in more realistic conditions the measurements of section~\label{ssec:}, and to test whether multi-domain systems can effectively take advantage from the cross-domain similarities, and to eventually perform better that fine-tuning.

Results are in table~\ref{tab:subdomains}.\fyTodo{Fill the tables with automatic and existing domains - we will have significancy results for the existing domains, for the other may be for larger classes we will have enough test instances?}

\section{Related work \label{sec:related}}

The multi-domain training regime is more the norm than the exception for NLP \cite{Dredze08online}. We focus here mostly on multi-domain learning in machine translation, keeping in mind that similar problems and solutions (parameter sharing, instance selection and weighting, adversarial training, etc) have also been proposed for other language processing tasks.
%; scores are reported separately per domain, suggesting that all domains are equally important at test time.
% Adversarial adpatation is used in many domains
% for QA @misc{lee2019domainagnostic,
%    title={Domain-agnostic Question-Answering with Adversarial Training},
%    author={Seanie Lee and Donggyu Kim and Jangwon Park},
%    year={2019},
%    eprint={1910.09342},
%    archivePrefix={arXiv},
%    primaryClass={cs.CL}
%}, NLU and many others

The authors of \cite{Kobus17domaincontrol} propose to inject additional domain feature in the model, either in the form of an extra (initial) domain-token, or in the form of additional domain-feature associated to each word. This technique has also been used to control the style of MT outputs in \cite{Sennrich16politeness}, and to the source or target languages in multilingual MT \cite{Firat16multiway,Johnson17google}. This approach is extended in \cite{Pham19generic} where source word embeddings are likely to vary across domains: this is achieved by spliting the embedding vector into a generic and a domain-specific part (one for each possible domain). Such feature-based approaches strongly rely on earlier proposals from the era of statistical MT \cite{Daume07frustratingly,Clark12onesystem}. Domain control can also be included in the target side, as in \cite{Chen16guided}, where a vector of topic describing the current document is included as an extra context in the softmax layer of the decoder.  

In \cite{Britz17mixing}, several methods are considered which aim to ensure that domain information is actually used in a mix-domain system. Three alternatives are considered, using either domain classification (or domain normalization, via adversarial training) on source or target representations. There is no clear winner in either of their three language pairs considered, which only distinguish a limited number of domains. The same basic techniques are simultaneously at play in \cite{Zeng18multidomain,Su19exploring}: in this approach, the upper layers of the MT aimed to sort out domain specific information on the one hand, and domain-agnostic information on the other hand, using auxiliary classification tasks, hoping that the domain-specific part will capture specific peculiarities of a domain, while the domain-agnostic part will be useful across the board. These techniques are implemented in an attentional sequence-to -sequence architecture in \cite{Zeng18multidomain}, and in Transformers in \cite{Su19exploring}.

A slightly more complex parameter-sharing scheme is presented in \cite{Jiang19multidomain}, which augments a transformer model with domain specific heads, whose individual contributions are regulated at the word (position-level): some words have ``generic'' use and rely on mixed-domain heads, while for some other words it is preferable to use domain specific heads. The results for three language pairs outperform several standard baselines for a 2-domain systems (in fr:en and ge:en) and a 4-domain system (zh:en).% \footnote{The reported results for WDC+WL are very bad, worst than anything \fyTodo{This is a mystery to me}.}   

The work of \cite{Tars18multidomain} makes a good case against fine-tuning, which is often found to be too brittle, a defect that MDMT can fix, delivering superior generalization accuracy. They perform experiment with reference domain tags, using mostly the domain-control approach. They also report results obtained with automatically induced domain tags, which show that working with automatically induced tags (at the sentence level) is a viable alternative to using corpus labels as domain tags, a result that we have confirmed in section~\ref{}.

\newcite{Farajian17multidomain} also used automatic domains, performing on-line adaptation at the sentence-level. In their approach, the processing of a test sentence triggers the selection of a dedicated (small) set of fine-tuning instances assumed to be similar to the input text; using these, the generic NMT system is then tuned for some epochs, before delivering its translation. Different of most approaches studied here, this approach entirely dispenses with the notion of a domain, relying primarily on data selection/weighting techniques to handle data heterogeneity. 

\subsection{Formalizing multi-domain adaptation (MDA)}

In MDA, the typical training situation (for a classifier) considers $k$ domains with $\mathcal{D}_k(x,y)$ the joint distribution of features $x \in \mathcal{X}$ and labels $y \in \mathcal{Y}$ for domain $k$. In \cite{Mansour09domainadaptation}, the labeling function $f$ is deterministic and the joint distribution is entirely defined by the marginal $\mathcal{D}_k(x)$ and $f()$.This work further assumes $k$ hypotheses $h_k(x): \mathcal{X} \rightarrow{} \mathcal{Y}$ with an error smaller than $\epsilon$ when $x\sim \mathcal{D}_k$: $\sum_{x} L(h_k(x), f(y)) \mathcal{D}_k(x) < \epsilon_k$ and that the target distribution is a convex combination of $\{\mathcal{D}_k\}$: $\mathcal{D}_T(x) = \sum_k \lambda_k \mathcal{D}_k(x)$. \cite{Mansour09domainadaptation} focuses on \textsl{combined hypotheses}, ie.\ hypotheses for which each prediction $h(x)$ is a linear combination of domain specific hypotheses $h(x_k)$, and derives generalization bounds for the case where $\mathcal{D}_T(x)$ is fixed in advance, or worst case bounds valid for any mixture target distribution. The optimal solution has the form $h_{z,\eta}(x) = \sum_k \frac{z_kD_k(x) + U(x)/\gamma}{\sum_{k'} z_kD_k(x) + \eta U(x)/l}$ and is termed the \emph{distribution weighted combined rule}. Further work in \cite{Mansour09multiple} generalizes these bounds to the case where $\mathcal{D}_T$ is not a mixture of existing domains (in that case we will find a mixture that is close to $\mathcal{}$in the Renyì-divergence sense ); where $\mathcal{D}_k$ are unknown and estimated from the data; or when the labelling function is not the same across domains. More recent work study the case where the labelling function is stochastic, meaning that domains are defined by joint ($\mathcal{D}_k(x,y)$) rather than marginal distributions \cite{Hoffman18algorithms}, and provides an algorithm to compute the optimal weights for the combined rule based on estimates of the joint distributions. A guiding principle is that this rule should tend towards uniform performance across domains, to ensure robustness in the worst case scenario \cite{Oren19distributionally}.

A related problem is \emph{Multiple source single domain adaptation} which is a variation of conventional DA from a mixture of source domains. In this setting it is advantageous to take advantage of the similarities between each source and the fixed target domain (see eg.\ \cite{Duan12domain,Cortes19AdaptationBO,Wen19domain} for studies of the supervised case, \cite{Zhao18adversarial} for the unsupervised case).

\section{Conclusion and outlook \label{sec:conclusion}}
\fyTodo{Write conclusions}

% --------------
% \section*{Acknowledgements}
% This work was granted access to the HPC resources of [TGCC/CINES/IDRIS] under the allocation 20XX- [numéro de dossier] made by GENCI (Grand Equipement National de Calcul Intensif)
% The acknowledgements should go immediately before the references.  Do
% not number the acknowledgements section. Do not include this section
% when submitting your paper for review.

% include your own bib file like this:
\bibliographystyle{coling}
\bibliography{multidomain}

\appendix{}

\begin{table}
  \centering
  \begin{tabular}{|p{3.2cm}|*{30}{r|}} \hline
    Model / Set-up &d1&d2&d3&d4&d5&d6&d7&d8&d9&d10&d11&d12&d13&d14&d15
    &d16&d17&d18&d19&d20&d21&d22&d23&d24&d25&d26&d27&d28&d29&d30 \\\hline
    %
    & \multicolumn{30}{|p{20cm}|}{%
      }% \hline
    \\
    %
    domain size ($\times 10^3$) &&&&&&&&&&&&&&&&&&&&&&&&&&&&&& \\ \hline\hline
    %
   \system{FT-Full}      &&&&&&&&&&&&&&&&&&&&&&&&&&&&&& \\  
    \system{DC-Tag}    &&&&&&&&&&&&&&&&&&&&&&&&&&&&&& \\     
    \system{DC-Feat}   &&&&&&&&&&&&&&&&&&&&&&&&&&&&&& \\ 
    \system{TTM}         &&&&&&&&&&&&&&&&&&&&&&&&&&&&&& \\ 
    \system{LDC-Tag}  &&&&&&&&&&&&&&&&&&&&&&&&&&&&&& \\ 

    \system{MDL Res} 
%    &&&&&&&&&&&&&&&&&&&&&&&&&&&&&& \hline
  \end{tabular}
  \caption{Detailed results of the experiments with automatic domains}
  \label{tab:automatic_domains}
\end{table}

\todos{}

\end{document}