%
% File coling2020.tex
%
% Contact: feiliu@cs.ucf.edu & liang.huang.sh@gmail.com
%% Based on the style files for COLING-2018, which were, in turn,
%% Based on the style files for COLING-2016, which were, in turn,
%% Based on the style files for COLING-2014, which were, in turn,
%% Based on the style files for ACL-2014, which were, in turn,
%% Based on the style files for ACL-2013, which were, in turn,
%% Based on the style files for ACL-2012, which were, in turn,
%% based on the style files for ACL-2011, which were, in turn, 
%% based on the style files for ACL-2010, which were, in turn, 
%% based on the style files for ACL-IJCNLP-2009, which were, in turn,
%% based on the style files for EACL-2009 and IJCNLP-2008...

%% Based on the style files for EACL 2006 by 
%%e.agirre@ehu.es or Sergi.Balari@uab.es
%% and that of ACL 08 by Joakim Nivre and Noah Smith

\documentclass[11pt]{article}
\usepackage{coling2020}
\usepackage{times}
\usepackage{url}
\usepackage{latexsym}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage{longtable}
\usepackage{tikz}
\usetikzlibrary{calc}
\usepackage[draft]{todo}
%\setlength\titlebox{5cm}
%\colingfinalcopy % Uncomment this line for the final submission

% You can expand the titlebox if you need extra space
% to show all the authors. Please do not make the titlebox
% smaller than 5cm (the original size); we will check this
% in the camera-ready version and ask you to change it back.
\newcommand{\fyTodo}[1]{\Todo[FY:]{\textcolor{orange}{#1}}}
\newcommand{\fyTodostar}[1]{\Todo*[FY:]{\textcolor{orange}{#1}}}
\newcommand{\fyDone}[1]{\done[FY]\Todo[FY:]{\textcolor{orange}{#1}}}
\newcommand{\fyDonestar}[1]{\done[FY]\Todo[FY:]{\textcolor{orange}{#1}}}
\newcommand{\jcTodo}[1]{\Todo[JC:]{\textcolor{blue}{#1}}}
\newcommand{\jcDone}[1]{\done[JC]\Todo[JC:]{\textcolor{blue}{#1}}}

\newcommand{\src}{\ensuremath{\mathbf{f}}} % source sentence
\newcommand{\trg}{\ensuremath{\mathbf{e}}} % target sentence
\newcommand{\domain}[1]{\texttt{\textsc{#1}}}
\newcommand{\system}[1]{\texttt{\textbf{#1}}}

\title{Revisiting Multi-Domain Machine Translation}

\author{First Author \\
  Affiliation / Address line 1 \\
  Affiliation / Address line 2 \\
  Affiliation / Address line 3 \\
  {\tt email@domain} \\\And
  Second Author \\
  Affiliation / Address line 1 \\
  Affiliation / Address line 2 \\
  Affiliation / Address line 3 \\
  {\tt email@domain} \\}

\date{}

\begin{document}
\maketitle
\begin{abstract}
Abstract goes here \fyTodo{Write abstract}
\end{abstract}

\section{Introduction} \label{sec:intro}

%
% The following footnote without marker is needed for the camera-ready
% version of the paper.
% Comment out the instructions (first text) and uncomment the 8 lines
% under "final paper" for your variant of English.
% 
\blfootnote{
    %
    % for review submission
    %
    \hspace{-0.65cm}  % space normally used by the marker
    Place licence statement here for the camera-ready version. See
    Section~\ref{licence} of the instructions for preparing a
    manuscript.
    %
    % % final paper: en-uk version 
    %
    % \hspace{-0.65cm}  % space normally used by the marker
    % This work is licensed under a Creative Commons 
    % Attribution 4.0 International Licence.
    % Licence details:
    % \url{http://creativecommons.org/licenses/by/4.0/}.
    % 
    % % final paper: en-us version 
    %
    % \hspace{-0.65cm}  % space normally used by the marker
    % This work is licensed under a Creative Commons 
    % Attribution 4.0 International License.
    % License details:
    % \url{http://creativecommons.org/licenses/by/4.0/}.
}

Data-based Machine Translation (MT), whether statistical or neural, rests on well understood machine learning principles. Given a training sample of matched source-target sentence pairs $(\src,\trg)$\jcDone{shoulnt trg be e as below ?} drawn from an underlying distribution $\mathcal{D}$, a model parameterized by $\theta$ (here a translation function $f_{\theta}$) is trained by minimizing the empirical expectation of a loss function $\ell(f_\theta(\src), \trg)$. This approach ensures that the expectation of the translation loss will remain low when translating new sentences \emph{drawn from the same distribution}.

Owing to the great variability of language data, this ideal situation is rarely met in practice, warranting the study of an alternative scenario, where the test distribution $\mathcal{D'}$ differs from $\mathcal{D}$. In this setting, \emph{domain adaptation} (DA) methods are required. DA has a long history in Machine Learning in general (eg.\ \cite{Shimodaira00improving,Ben-David09atheory}) and in NLP in particular (eg.\ \cite{Daume06domain,Blitzer07domain,Jiang07instance}), and various techniques exist to handle both the situations where a (small) training sample drawn from $\mathcal{D'}$ is available in training, or where only samples of source-side (or target-side) sentences are available (see \cite{Foster07mixture,Bertoldi09domain,Axelrod11domain} for early proposals for statistical MT, or \cite{Chu18asurvey} for a recent survey of DA in the context of Neural MT).
% In a nutshell, DA attempts to compensate for the distribution mismatch either by biasing the training sample to match ${\mathcal{D'}$ (eg.\ using resampling or instance-weighting strategies), or by adapting the 

A seemingly related problem is \emph{multi-domain (MD) machine translation} \cite{Hassan17neural,Farajian17multidomain,Kobus17domaincontrol,Zeng18multidomain,Pham19generic} where a single system has to be trained  and tested with data from multiple domains. As pointed out eg.\ by \newcite{Dredze08online}, this setting borrows much from DA, but also from multi-task learning \cite{Caruana97multitask}. If the intuitions behind MD may seem simple, the exact specifications of MDMT systems are rarely spelled out, which makes their evaluation difficult: for instance, should they perform well when the test data is distributed like the train data, when it is equally distributed across training domains or when the test distribution is unknown? should MDMT also be robust to new domains? How should they handle domain labeling errors? 

A related question concerns the relationship between domain adaptation and multi-domain translation. The latter setting seems more challenging and one would expect that adapting a system to a known target domain $A$ will be easier than training a MDMT system, where only part of the data belongs to domain $A$. Are there situations where MD systems can surpass DA, as is sometimes expected?
 
In this paper, we try to answer these questions and to spell out in a more precise fashion the requirements that a MDMT system should meet. Our main contribution is of methodological nature and consists of lists of expected properties of MDMT system and associated measurements to evaluate them. \fyTodo{New problems - continuous learning, automatic domains}Our second contribution is experimental, and we perform a thorough reanalysis of most recent multi-domain approaches from the literature, including a variant of a model initially introduced for DA. We show that\fyTodo{Spell out conclusions}.
 
\section{Requirements of multi-domain MT \label{sec:requirements}}
In this section, we try to recap the main reasons for considering a multi-domain scenario and discuss their implications in terms of performance evaluation.
\fyTodo{Relation to theory, discussion of worst case scenario}

\subsection{MDMT: a tentative definition}
Informally, a multi-domain system should be able to accurately translate heterogeneous sources of data. This loose definition correspond to several practical situations:
\begin{itemize}
\item the more common situation is when both train and test documents (or sentences) are domain-tagged, where each domain corresponds to a specific source of data, and differ from the other sources in terms of textual genre, thematic content \cite{Zhang16topicinformed}, register, style \cite{Niu18multitask}, etc. It is usually assumed that the set of test domains is the same as the set of training domains -- when this is not the case, we would have zero-shot domain adaptation, similar to what is studied in multilingual MT \cite{Firat16multiway,Ha16towards,Johnson17google,Platanios18contextual}. This is the more general setting, already proposed in \cite{Dredze09multidomain}.\fyTodo{do we need a name here: such as supervised MDMT ? and mostly supervised MDMT for the second case ?}

\item It can also be the case that domain labels are only known in training, while test inputs may or may not be associated with explicit domains. This is, for instance, the setting adopted \cite{Zeng18multidomain}, where automatic domain identification is performed for test sentences. Note that automatic domain identification can also be effectively performed in the first setting, especially in the presence of close or related domains in training.
  
\item A last setting, that has not received much attention for MT (see however \cite{Sennrich13multidomain}), is where both train and test domains are automatically infered from data, using clustering techniques. Even though domain labels are typically available in most training scenario, these approaches assumes that might be preferable, owing to the multiple sources of intra-corpus and inter-corpus variabilities, to resort to automatic domain assignments. The online approach of \newcite{Farajian17multidomain}, where distributional cues, rather that categorical tags, are used for fine-tuning, is a variant of this strategy.
\end{itemize}

In the rest of the paper, we mostly adhere to the first definition which correspond to a situation of practical interest for the industry, and assume that multi-domain MT is mostly about effectively using domain tags in training and testing. We also experiment in Section~\ref{sec:results} with the other two alternative scenarios, as they also define useful contexts to evaluate the actual robustness of MDMT systems.

\jcDone{...a very common scenario in MT industry where clients dispose of mulitiple data corresponding to different domains (differing in genre, thematic, register, style, etc.). }

\subsection{Reasons for using MDMT? \label{ssec:whymdmt}}
The motivations for considering MDMT are many. Looking at the literature, the main arguments in support of MDMT seem to be the following:
\begin{enumerate}
\item[[M1\hspace{-0.5em}]] computational / practical arguments. A first motivation for MDMT is practical: when faced with inputs that are potentially from multiple domains, it is easier and computationally cheaper to develop only one system instead of having to optimize (and maintain) multiple MT engines \cite{Sennrich13multidomain,Farajian17neural}. Such arguments are all the more convincing that the number of domains considered is large, which is not always the case: for instance \cite{Britz17mixing,Zeng18multidomain,Jiang19multidomain} only consider 2-4 domains in their experiments.
  
\item[[M2\hspace{-0.5em}]] statistical arguments. The training data available for each domain is very unevenly distributed, and domain trained or adapted on small datasets will yield large estimator variance. Building mix-domain systems is likely to reduce this variance, at the expense of a larger statistical bias \cite{Clark12onesystem}. Under this view, MDMT would be especially useful for domains with little training data. This is what is observed for multilingual MT from English: an increase for under-resource languages due to positive transfer, at the cost of a decrease for well-resourced languages \cite{Arivazhagan19massively}. A slightly distinct line of reasoning is that mixing domains has a positive regularization effect for all domains. By introducing variability in training, it prevents domain adaptation from overfitting the available adaptation data and could help improve generalization even for well-resourced domains.

\item[[M3\hspace{-0.5em}]] linguistic arguments. A related line of reasoning contends that domain specificities are mostly expressed lexically and will primarily affect content words or multi-word expressions; function words, on the other hand, are domain agnostic and remain semantically stable accross domain. A MDMT system should simultaneously learn lexical domain peculiarities, and leverage cross-domain similarities to improve the representation of generic contexts and words \cite{Zeng18multidomain,Pham19generic}. It is expected that the MDMT scenario should be more profitable when the domain mix includes domains that are closely related and which can share more information. \fyTodo{polysemy}
\end{enumerate}

These various reasons for adopting MDMT are not mutually exclusive. Each however yields specific expectations with respect to the performance of this approach: if the motivation is primarily computational, then a drop in MT quality with respect to fine-tuning each domain might be acceptable; if it is to improve statistical estimation, then the hope will be that MDMT will improve, at least for some domains, if not all, over individually trained systems.

\cite[eq. (3)]{Wang20general} defines the loss of MDMT as the (macro) average of domain loss - all domains are equally important.

\subsection{Evaluation issues \label{ssec:evaluation}}

In theory, evaluation procedures should report realistic estimates of the test loss, computed on a sample deemed representative of the anticipated test conditions. Three scenarios need be distinguished:
\begin{itemize}
\item test data are mixed in the same proportion as training data; this warrants the design of an unbalanced mix-domain test sets in the same proportions as in training; alternatively, if per domain scores are ever reported, their average should take the mixing proportions into account;
\item all domains are equally likely and important in testing (the prior over test domains is uniform), which would warrant the computation of per domain scores and of unweighted averages, as reported in \cite{Farajian17multidomain,Su19exploring}. Note that in this situation, the optimal training regime would be to balance the training set in the same way -- which is hardly ever done in practice, as the baseline scores always consider mixing unevenly all available data. \cite{Wang20general} uses an intermediate mixing regime, which tends to reduce part of the unbalance, and improve the performance for underrepresented domains.
\item the test distribution is neither equal to the training set, nor uniform. Here again, if this distribution is known in advance, an optimal domain mix should be used in training to compute baseline scores, rather than using the empirical distribution of all the training data. A more challenging case is when the test set includes data from domains unseen in training, or when it is chosen adversarially \cite{XXX}.
% : this situation, where test instances have a null probability in training, goes agains the basic statistical assumptions of DA \cite{Shimodaira00improving}. It is thus important that the likeliness of such events should be clearly formulated when describing the evaluation protocol.
\end{itemize}

The papers in the literature are seldom explicit regarding their assumptions regarding the test distribution. In most cases, per-domain performances are reported, and even sometimes averaged, suggesting that the scenario~2 most faithfully reflects the expected testing conditions. Another important issue is the choice of the evaluation metrics: if BLEU is still the standard metric used in most studies, it is not necessary the best choice to measure subtle improvements of lexical choices \cite{Irvine13measuring} that are often associated with adapted systems.

A last issue regarding evaluation is related to the measurement of computational costs: when this is the main a motivation for using MDMT, we expect that computational costs would be reported and compared, which however is hardly ever done in practice. \fyTodo{Keep this ?}

\section{Challenging multi-domain translation systems}
In this section, we discuss the operational requirements that can be expected from a true multi-domain system, as well as ways to evaluate how much these requirements are met in practice.

\subsection{Multi-domain systems should be effective \label{ssec:effective}}
% \subsubsection{Performance}
The first set of requirements for MDMT systems is that they should perform well in the face of mix-domain data. These yields the following desired properties.

\paragraph{[P1-LAB]}\fyTodo{Decide naming scheme} A first expectation for MDMT is that they perform better that the obvious baseline which disregards domain labels. % Testing that MDMT actually benefit from domain tags is simple, notwithstanding the previous remarks above regarding evaluation procedures.

\paragraph{[P2-TUN]} Additionally, following motivation~(M2) above, one would expect that MDMT improves over fine-tuning, at least in domains where data is scarce, or in situations where several domains are close. To evaluate this, we perform two measurements, using a real as well as an artificial scenario. In the real scenario, we simply compare the performance of MDMT and fine-tuning for domains of varying sizes, expecting a larger gain for smaller domains (property [P2.1-TUN]). In the artificial scenario, we split a single training domain in two parts which are considered as distinct in training. The expectation here is that a MDMT should yield a clear gain in the both pseudo sub-domains, which should benefit from the supplementary amount of relevant training (this is property [P2.2-TUN]). In this situation, MDMT should even outperform fine-tuning on either of the pseudo sub-domain.
\jcDone{I dont understand the prediction... why running MDMT on 2 artificial subdomains should show  gains over fine-tunning?}\fyTodo{Clearer now ?}

\subsection{Robustness with respect to domain definition \label{ssec:robusness}}
A second set of requirements is related to the definition of a domain. As repeatedly pointed out in the literature, training corpora for MT are often collected opportunistically and the view that each corpora constitutes a single domain is often a gross approximation.\footnote{Two of our own domains are actually composed of several subcorpora (IT and MED), see details in section~\ref{ssec:corpora}.} MDMT should aim to make the best of the available data and be robust to domain assignments. To challenge these requirements we propose the evaluate the following properties.

\paragraph{[P3-HET]}
The notion of a domain is often a fragile one and an effective MDMT system should be able to discover not only when cross-domain sharing is most useful (cf.\ property [P2.2-TUN]) \jcDone{what is P2.2?}, but also when intra-domain heterogeneity is hurting. Requirement [P3-HET] is tested by artificially conjoining two separate domains in one during training. The loss in performance with respect to the baseline (correct domain tags) should then be as small as possible.

\paragraph{[P4-ERR]}
A MDMT should perform best when the true domain tag is known, but deteriorate gracefully in the presence of domain tag errors. This is in sharp contrast with what is commonly observed with fine-tuning, where giving the wrong domain tag can yield ``catastrophic'' drop in performance. This requirement is measured by decoding the test set for domain A with all other domain tags, and report average drop in performance.

\paragraph{[P5-UNK]}
A related situation occurs when the domain of a test document is unknown. Several alternatives are in order: (a) automatically assigning one of the domain tag for decoding should not be much worse than using the true domain tag, assuming this domain is seen in training. For test documents from other domains, the same procedure can apply, and should outperform the use of a single mix-domain system. \fyTodo{Systems react to unknown domains}These requirements are respectively labelled [P5.1] and [P5.2]\fyTodo{Not so sure of these ones}.

\paragraph{[P6-DYN]}
Another requirement, more of an operational nature, is that a MDMT system should smoothly evolve to accomodate a larger number of domains, without having to retrain the full system each time new data is available. This is a requirement [P6] that we challenge by dynamically changing the number of training and test domains.

\subsection{Effectiveness in handling a large number of domains \label{ssec:scaling}}

\paragraph{[P7-NUM]} As mentioned above, the computational argument (M1) is all the more sensible as the number of domains increases, making the optimization of many individual systems both ineffective and undesirable. For lack of having access to corpora containing a large (eg.\ in the order of 100-1000 domains), we experiment with automatically learned domains.\fyTodo{considering a varying number of clusters.}

\section{Experimental settings \label{sec:experiments}}

\subsection{Data and metrics \label{ssec:corpora}}

We experiment with translation from English into French and use data initially originating from 6~domains, corresponding to texts from the following data sources: the UFAL Medical corpus V1.0 (\domain{med})\footnote{\url{https://ufal.mff.cuni.cz/ufal_medical_corpus}}, the European Central Bank corpus (\domain{bank}) \cite{Tiedemann12parallel}; The JRC-Acquis Communautaire corpus (\domain{law}) \cite{Steinberger06acquis}, documentations for KDE, Ubuntu, GNOME and PHP from Opus collection \cite{Tiedemann09news}, collectively merged in a \domain{it}-domain, Ted Talks (\domain{talk}) \cite{Cettolo12wit}, and the Koran (\domain{rel}). Complementary experiments also use v12 of the News Commentary corpus (\domain{news}). Corpus statistics in Table~\ref{tab:Corpora}.  Most corpora are variable from the Opus web site.\footnote{\url{http://opus.nlpl.eu}} These corpora were deduplicated and tokenized with in-house tools. To reduce the number of types and build open-vocabulary systems, we use Byte-Pair Encodings \cite{Sennrich16BPE} with 30,000 merge operations on a corpus containing all sentences in both languages.%

We randomly select in each corpus a development and a test set of 1000 lines, and keep the rest for training. Validation sets are used to chose the best model according to the average BLEU score computed with\fyTodo{A word about meta-parameter settings}. Significance tests are performed using the bootstrap resampling method of \newcite{Koehn04statistical}. We report significant differences at the level of $p=$ \fyTodo{Fix correct p value}.

%for contrast experiments, we also use supplementary test sets from three other domains: the official Khresmoi testset \cite{Khresmoi17test}, which is close to EMEA, News test 2014 \cite{Bojar14findings}, and IWSLT 2010 (Talk track) \cite{Paul10overview}. This enables us to evaluate the loss in performance when the test set is from a domain not seen in training.
% The model is also required to achieve comparable performance to generic model. To do so, we use newstest 2009 and IWSLT 2010 whose contain does not particularly belong to any domain.

\begin{table}[h]
  \centering
  \begin{tabular}{ |lllllll|} %*{4}{|r|}}
    \hline
    %\multicolumn{4}{|l|}{Vocab size - En: 30,165, Fr: 30,398}\\
    \domain{med} & \domain{law} & \domain{bank} & \domain{it} & \domain{talk} & \domain{rel} & \domain{news} \\
    \hline
    2609 (0.68) & 190 (0.05)  & 501 (0.13) & 270 (0.07) & 160 (0.04) & 130 (0.03) & 260 (0) \\
    \hline
  \end{tabular}
\caption{Corpora statistics: number of parallel lines ($\times 10^3$) and proportion in the basic domain mix (which excludes the \domain{news} domain).}
\label{tab:Corpora}
\end{table}

We measure the distance between domains using the $\mathcal{H}$-Divergence introduced in \cite{Ben-David09atheory}, which relates domain similarity to the test error of a domain discriminator: the larger the error, the closer the domains. Formally, given test sets of size $m$ for domains $A$ and $B$, and $h(x)$ a trained domain predictor, $\mathcal{H}(A,B)$ is computed as:
$$
\mathcal{H}(A,B) = 2(1 - [\frac{1}{m} \sum_{x:h(x) = B} \mathbb{I}( x \in A) + \frac{1}{m} \sum_{x: h(x) = A} \mathbb{I}(x \in B)]),
$$
where $\mathbb{I}$ is the indicator function. Our domain discriminator is a multilayer perceptron independently trained for each pair of domains with sentence representations derived via mean pooling from the source side representation of our generic Transformer model. The hidden layer has dimension XXX, and the network is trained on balanced sets of YYY instances for ZZZ epochs.\fyTodo{Inform the classifier details}.

\subsection{Baselines \label{ssec:baselines}}

Baselines correspond to standard practices for multi-domain systems.\footnote{We however omit domain-specific systems trained with the corresponding subset of the data, which are always inferior to the mix-domain strategy eg.\ \cite{Britz17mixing}.} Using the Transformer architecture of \shortcite{Vaswani17attention} implemented in OpenNMT-tf\footnote{\url{https://github.com/OpenNMT/OpenNMT-tf}} \cite{Klein17opennmt}, we build the following systems:

\begin{itemize}
\item a generic model trained on a concatenation of all corpora (\texttt{Mixed}). We develop two versions of this system, one where domains unbalance reflects the distribution of our parallel data (\system{Mixed-Nat}) and one where all domains are equally represented in training (\system{Mixed-Bal}) (cf.\ section~\ref{sec:evaluation}); \jcTodo{All systems cand be learned following both Bal and Nat. does Nat corresponds to wAVG in test? is wAVG defined somewhere?}
\item models fine-tuned \cite{Luong15stanford,Freitag16fast} \jcDone{i would use Luong and Manning 2015} separately on each domain for at most 20~000 iterations, with early stopping when the dev BLEU stops increasing. In our setting, we again contrast two versions: full fine-tuning (\system{FT-Full}), which may update all the parameters of the initial generic model; and the variant of \cite{Bapna19simple}, where fine-tuning only updates a simple adaptation module that is added (with residual connexions) on top of every Transformer layer (\system{FT-Block}).
\end{itemize}

For all models, we set the embeddings size and the size of hidden layers to~512. Transformers use multi-head attention with 8 heads in each of the 6 layers; the inner feedforward layer contains 2048 cells. The multi-domain residual system (see description below) additionally use an adaptation block in each transformer layer, composed of 2-layer perceptron, with an inner RELU activation function operating on normalized entries of dimension 1024. The gated variant is made of a dense layer, followed by a layer normalization and a sigmoid activation.
% The domain control systems are exactly as their baseline counterparts (RNN and Transformer), with an additional 2 cells encoding the domain on the input layer.
 %
Training uses Adam with parameters $\beta_1=0.9$, $\beta_2= 0.98$ and Noam decay ($warmup\_steps=4000$). In all cases, we use a batch size of~128 and a dropout rate of 0.1 for all layers. % All our systems are implemented in OpenNMT-tf\footnote{\url{https://github.com/OpenNMT/OpenNMT-tf}} \cite{Klein17opennmt}.
\fyDone{Describe the block adaptation layer - voir slides} 

\subsection{Multi-domain systems \label{ssec:systems}}
\fyDone{Remove Gated residual}
Our comparison of multi-domain systems includes the following reimplementations of recent proposals from the literature:
\begin{itemize}
\item a system using domain control as in \cite{Kobus17domaincontrol}: domain information is introduced either as an additional token for each source sentence (\system{DC-Tag}), or introduced in the form of a supplementary feature for each word (\system{DC-Feat}).
\item the three proposals of \cite{Britz17mixing}: \system{TTM}, another tag-based approach where the domain tag is introduced as an extra word \textsl{on the target side}. Training is performed with reference tags and inference is performed with predicted tags, just like for any other target words; \system{DM} is a multi-task learner where a domain classifier is trained on top the MT encoder, so as to make it aware of domain differences; \system{ADM} is the adversarial version of \system{DM}, pushing the encoder towards learning domain-independent source representations;
\item a system using lexicalized domain representations \cite{Pham19generic}: word embeddings are composed of a generic and a domain specific part (\system{LDC-Feat});
\item the multi-domain model of \cite{Zeng18multidomain} (\system{WDCMT}): in this approach two representations, a domain-agnostic and a domain-specialized representation of the input are simultaneously processed; supervised classification models and adversarial training are used to compute these representations.\footnote{For this system, we use the available RNN-based system from the authors (\fyTodo{URLs}) which does not directly compare to the other, Transformer-based, systems; the improved version of \cite{Su19exploring} seems to produce comparable, albeit slighly improved version.}
\item an original, multi-domain version of the approach of \newcite{Bapna19simple}, denoted \system{MDL Res}, where we add a dedicated adaptation module for each domain, which is only activated for the appropriate set of training data; within each layer, residual connexions make it possible to by-pass this module.
  % In a variant (\system{MDL Gated}), we use a gating mechanism to merge computations using the the adapter modules and with those that don not.
Note that contrarily to \cite{Bapna19simple}, we do not start with a trained generic system, but learn the multi-domain from scratch. \fyDone{Check this.}
\end{itemize}

Note that we did not include here the system of \cite{Farajian17multidomain} which performs on-the-fly tuning for each test sentence, and slightly diverges from our notion of MDMT, which should be able to handle available domain information in training and testing. This system has shown to meet many of our requirements and would also have no problem scaling to a large number of domains.\fyTodo{TBC}

\section{Results and discussion \label{sec:results}}
\subsection{Performance of MDMT systems \label{ssec:rawperformance}}
In this section, we discuss the raw performance of MDMT systems trained and tested on $6$~domains. Results are in Table~\ref{tab:performance}. As expected, balancing data in the generic setting makes a great difference (the unweighted average is 2~BLEU points \jcDone{BP?} better, notably owing to the much better results for \domain{rel}). As explained above, this setting should be the baseline when the test distribution is assumed to be balanced across domains. As all other systems are trained with an unbalanced data distribution, use the weighted average to perform global comparisons.

Fine-tuning each domain separately provides us with better baselines, significantly outperforming \system{generic} for all domains. A complete fine-tuning, which updates all parameters, proves more effective than only adapting a subpart of the network - a result that could be due to lack of tuning the capacity of the adapter module. \fyDone{Add 2 averages ?}\fyTodo{Significance testing wrt Mix Generic, Full Fine-tuned, for each domain}
A  first group of MDMTs use an extra domain feature, at the sentence or at the word level: among these, having the domain tag on the source is better than on the target, and very comparable to repeating the tag for every source word. All (except \system{DM} and \system{ADM}) improve over the generic baseline (for most domains), matching requirement [P1-LAB], but yield weaker results than fine tuning. As expected, the gap with fine-tuning remains large for an ``isolated'' domain such as \domain{rel}, the best contender being overall \system{MDL Res}, which approximately halves this gap as compared to \system{Mixed-Nat}.
\fyTodo{More comments when we have all the results}
The use of an decidated adaptation component in each of the transformer layer proves an even better strategy and closes the gap with fine-tuning for half of the domains: the use of a gating mechanism yielding a clear improvement over the board. The effect of the adaptation layer is especially significant for the smaller domains (\domain{bank}, \domain{it} and \domain{rel}) for which unplugging this layer causes dramatic performance drops. 

\fyTodo{Importance of sharing and unsharing}

\begin{table*}
  \centering
  \fyDone{Fix column size}
  \begin{tabular}{|p{3cm}|*{8}{r|}|r|} \hline
%     &&&&&& \\
    Model / Domain & \multicolumn{1}{c|}{\domain{ med}} & \multicolumn{1}{c|}{\domain{ law}} & \multicolumn{1}{c|}{\domain{bank}} & \multicolumn{1}{c|}{\domain{talk}} & \multicolumn{1}{c|}{\domain{ it }} & \multicolumn{1}{c|}{\domain{ rel}} & \multicolumn{1}{c|}{w\domain{avg}} & \multicolumn{1}{c||}{\domain{avg}} & \multicolumn{1}{c|}{\domain{news}} \\ \hline
    \system{Mixed-Nat}  & 37.3 & 54.6 & 50.1 & 33.5 & 43.2 & 77.5  & 41.1  & 49.4 &\\
    \system{Mixed-Bal}   &  35.3 & 54.1 & 52.5 & 31.9 & 44.9 & 89.5 & 40.3  & 51.4 &\\
    \system{FT-Full}       & 37.7 & 59.2 & 54.5 & 34.0 & 46.8 & 90.8   & 42.8 & 53.8 &\\
   \system{FT-Block}     & 37.3 & 57.9 & 53.9 & 33.8 & 46.7 & 90.2    & 42.3 & 53.3 &\\ \hline
 %   Full-finetuned on extended in-domain corpora (news) & && 33.96&&& & &\\nn
    \system{DC-Tag}       & 38.1 & 55.3 & 49.9   & 33.2 & 43.5 & 80.5 &41.7 & 50.1     &\\
    \system{DC-Feat}      & 37.3  & 54.9 & 49.5   & 32.9 & 43.6 & 79.9 &41.4 & 49.9   & \\
    \system{TTM}            & 37.7 & 54.8 & 49.7 & 33.4 & 43.8 & 80.0 &41.1 & 49.7       & \\
    \system{LDC-Tag}     & 37   & 54.7 & 49.9 & 33.9 & 43.6 & 79.9 & & & \\ 
    \system{DM}             & 36.5 & 52.4  & 47.7& 32.1 & 41.0 & 70.4 & 39.7 & 46.7 & \\
    \system{ADM}           & 37.1 & 53.8  & 48.9 &32.8 & 42.3 & 78.5 & 40.7 & 48.9 & \\
    \system{WDCMT}       & & & & & & & & &\\
    \system{MDL Res}     & 37.9 & 56.0  & 51.2   & 33.5   &  44.4  & 88.3 & 42.1 & 51.9       &\\
    \system{MDL Res} (gen)           & 37.7 & 51.0 & 34.0 & 30.4 & 34.2 & 15.2 & 36.4 & 33.7&\\
%    \system{MDL Gated} & 37.7 & 56.5 & 53.2 & 34.1 & 44.6 & 90.7 & 42.3 & 53.3&\\
     \hline
  \end{tabular}
  \caption{Translation performance of various MDMT systems. We report the BLEU score for each domain, as well as domain-weighted (w\domain{avg}) and unweighted (\domain{avg}) averages. (*) denotes significant improvements with respect to the \system{Mix-Nat} system. Scores for the \domain{news} domain correspond to the out-of-domain scenario (see section~\ref{ssec:performance}).}
  \label{tab:performance}
\end{table*}
\fyTodo{Fill the table with all results, including a comparison with *RNNs* for wcmd}
\fyTodo{A new column for the News domain with predicted domains (except for the generic case).}

\subsection{Redefining domains \label{ssec:redomains}}
Table~\ref{fig:redomains} summaries the results of \fyTodo{XX} experiments where we artificially redefine the limits of domains. In the three \textsl{split} experiments, we randomly split a domain corpus in two part: a good MDMT system should detect that these two ``domains'' should be mutually beneficial, and should be hardly affected by this change wrt.\ the baseline scenario. This is the situation where we should see clear improvements even over finetuning, which only exploits part of the available relevant data. In the \textsl{merge} experiment, we do the reverse and merge two corpora in training, in order to assess the robustness with respect to heterogenous domains. \fyTodo{Prepare table for the wrong domain analysis (see also last col. of the first table)}In the last column, we finally report the averaged (across domains) drop in performance when the domain information is erroneous.

\begin{table}
  \centering
  \begin{tabular}{|p{2.5cm}|*{9}{r|}} \hline
    % &&&&&& \\
    Model / Set-up & \multicolumn{2}{c|}{Split} &  \multicolumn{2}{c|}{Split} & \multicolumn{2}{c|}{Split} & \multicolumn{2}{c|}{Merge} & Wrong \\ % \hline
     Model / Set-up & \multicolumn{2}{c|}{\domain{med} \footnotesize{(0.5 / 0.5)}} &  \multicolumn{2}{c|}{\domain{med} {\footnotesize (0.25 / 0.75)}} & \multicolumn{2}{c|}{\domain{bank} {\footnotesize (0.5 / 0.5)}} & \multicolumn{2}{c|}{\domain{bank}+\domain{law}} & domain \\ \hline
    & \domain{med-1} & \domain{med-2} & \domain{med-1} & \domain{med-2} &  \domain{bank-1} & \domain{bank-2} & \domain{bank} & \domain{law} & \\
    \system{FT-Full}      & -0.13 & -0.6 & -1.51& -0.25& & & & & \\
    \system{DC-Tag}     & -0.16 & -0.27& +0.13& +0.2& & & & & \\
    \system{DC-Feat}    & -0.47& 0.0 & +0.27 $^+$ & +0.27 $^+$& & & & & \\
    \system{TTM}          & & & & & & & & &  \\
    \system{LDC-Tag}   & & & & & & & & & \\ 
                     
    \system{MDL Res}   & -0.2$^+$& -0.07$^+$&+0.18$^+$  &+0.0$^+$ & & & & & \\
%    \system{MDL Gated} & & & & & & & & & \\

    \system{ADM} & & & & & & & & & \\
    \system{DM}    & & & & & & & & & \\
    \system{WDC}  & & & & & & & & & \\
    \hline
  \end{tabular}
  \caption{Translation performance with variable domain definitions. For the Split experiments, we only report the loss in performance for the \domain{med} test set, (*) denote significant losses when domains are split or merged; (+) denotes a significant improvement over \system{Full FT}; \fyTodo{How about wrong ?}}
  \label{tab:redomains}
\end{table}

Our main findings can be summarized as follows: for the split experiments, we see a general loss with respect to the situation were domains are kept a whole, except in rare cases\fyTodo{To be completed}. As expected, \system{FT} incurs a large loss, especially for the small split. 
Findings so far:
\begin{itemize}
\item Split UFAL half / half: domain-tag and Gated-res are OK, all systems loose wrt baseline; 
\item Split UFAL 0,75 / 0.25: same thing, and we even see improvements for domain tags - they are small though
\item Split Law : only domain feature works ? all the rest have losses, from large to small
\item Merge domain: bad for many, OK for TTM (as the domain is learnt this is not so bad ?); bad for domain tag, domain feature is again OK ?
\end{itemize}

\fyTodo{We do need significance testing (a) each system against itself (non-split); (b) each system agains FT in split  condition}

Another series of experiments evaluate the ability to handle additional domains (requirement [P6-DYN]) as follows. Starting with the existing MDMT systems of Section~\ref{ssec: ssec:rawperformance}, we introduce an additional domain (\domain{News}) and resume training with a mixture of data\footnote{The design of a proper mixture is again of utmost importance for achieving optimal performance: since our goal is to evaluate all systems in the same conditions, we consider a basic mixing policy where half the batches are from the new domain, and half of the batches reproduce the initial training distribution. This is of course detrimental to the small domains, for which the ``negative transfer'' effect is stronger than for larger domains.} for an XXX epochs.\fyTodo{Training regime of continuation}. We contrast this approach with retraining all the systems from scratch, and report the difference in performance in Table~\ref{tab:XXX}. We expect that a good MDMT system should not be significantly affected by the addition of new data, and reach the same performance as if the training included this domain from scratch. It should be noted that dynamically integrating new domains is straightforward for \system{DC-Tag}, \system{TTC} or \system{DC-Feat} where adding a new domain simply adds a new tag. It is less so for \system{LDC} \fyTodo{Complete effects of new domain}, for which the number of possible domains is built in the architecture and has to be anticipated from the begining. This makes a first difference between domain-bounded MT-systems, for which the number of possible domains is upper bounded; and the open-domain systems, which seamlessly integrate additional domains.

\begin{table*}
  \begin{tabular}{|p{3cm}|*{9}{r|}} \hline
    Model / Domain & \multicolumn{1}{c|}{\domain{ med}} & \multicolumn{1}{c|}{\domain{ law}} & \multicolumn{1}{c|}{\domain{bank}} & \multicolumn{1}{c|}{\domain{talk}} & \multicolumn{1}{c|}{\domain{ it }} & \multicolumn{1}{c|}{\domain{ rel}} & \multicolumn{1}{c|}{\domain{news}} & \multicolumn{1}{c|}{w\domain{avg}} & \multicolumn{1}{c||}{\domain{avg}} \\ \hline
    
    \system{Mixed-Nat}  & 37.3  & 54.6   & 50.1   & 33.5  & 43.2  & 77.5  & & 41.1 & 49.4 \\
    \system{FT-Full}       & 37.7  & 59.2   & 54.5   & 34.0  & 46.8  & 90.8  & & 42.8 & 53.8 \\ \hline\hline
     \system{DC-Feat}     & 37.4 & 54.9   & 50.0    & 34.7  &  43.9  & 79.6 & 28.9 \\
                                    & 36.9 & 53.7   & 49.1    & 33.7  &  42.8  & 74.1 & 28.9 &\\
    \system{DC-Tag}      &  37.7 & 54.5   & 49.9    &  34.8 &  43.9  & 78.8 & 29.5 \\
                                   & 38.0  & 54.0   & 48.8    & 33.5  &  42.7  &73.2  & 28.3& \\
    \system{TTM}           &  37.3 & 54.4   & 49.6    & 33.8  &  42.9  &78.1  & 29.1 \\
                                   & 36.8  & 53.4   & 47.9    & 32.4  &  42.5  &72.8  & 28.4& \\
    \system{LDC-Tag}    & 37.0   & 54.6  & 49.6    & 34.3  &  43.0  &77.0  & 28.7 \\
    continual                 &&&&&& \\
    \system{DM}            &36.0 &51.3&46.8&31.8&39.8&65.7&27.0 \\
                                  &36.5&51.6&47.3&31.7&40.3&65.7&25.8& \\
    \system{ADM}          &36.6&54.2&49.1&32.9&42.1&75.7&28.7 \\
                                  &  36.9 &53.5&48.3&32.7&41.7&70.7&26.8& \\
    \system{WDCMT}     &34.8  &51.8&47.8&30.7&37.3&49.3&25.9 \\
    continual                 &&&&&&&& \\
    \system{MDL-Res}    &37.7   & 55.6   & 51.1   & 34.4  & 44.5  & 87.5  & 29.1 & & \\
     continual                &&&&&&&& \\
     \hline
  \end{tabular}
  \caption{Ability to handle new a domain: BLEU performance in ``resume training'' mode compared to a complete training session.}
  \label{tab:warmrestart}
\end{table*}
\fyTodo{Insert table, significancy testing against full learning condition, for each domain}

Considerations, \fyTodo{To be completed}: 

\begin{itemize}
\item even when trained from entirely scratch, very few systems are able to take advantage of the additional data for the 6 initial domains - this is observed on the averaged scores in Table~\ref{tab:warmrestart} with hardly surpass those in Table~\ref{tab:performance}
\item restart is computationally cheaper (\fyTodo{how much cheaper here ?}) but underperforms cold start for all systems, sometimes by a wide margin.
\item the loss is largest for the small domains - how about distance to \domain{news} ?
\end{itemize}
\fyTodo{Very close to fine tuning towards News, no ?}:

\subsection{Automatic domains \label{ssec:autodomains}}
In this section, we experiment with automatic domains, obtained by clustering sentences into $k=30$ classes using the k-means algorithm based on generic sentence representations obtained via mean pooling as in section~\ref{ssec:corpora}. This allows us to evaluate requirement [P7-scale], training and testing our systems as if these domains were genuine. We also note that many of these classes are mere split of the large \domain{med}, while a few number of classes being mix of two existing domains. We are thus in a position to reiterate in more realistic conditions the measurements of section~\label{ssec:}, and to test whether multi-domain systems can effectively take advantage from the cross-domain similarities, and to eventually perform better that fine-tuning.

Results are in table~\ref{tab:subdomains}.\fyTodo{Fill the tables with automatic and existing domains - we will have significancy results for the existing domains, for the other may be for larger classes we will have enough test instances?}

\section{Related work \label{sec:related}}

The multi-domain training regime is more the norm than the exception for NLP \cite{Dredze08online}. We focus here mostly on multi-domain learning in machine translation, keeping in mind that similar problems and solutions (parameter sharing, instance selection and weighting, adversarial training, etc) have also been proposed for other language processing tasks.
%; scores are reported separately per domain, suggesting that all domains are equally important at test time.
% Adversarial adpatation is used in many domains
% for QA @misc{lee2019domainagnostic,
%    title={Domain-agnostic Question-Answering with Adversarial Training},
%    author={Seanie Lee and Donggyu Kim and Jangwon Park},
%    year={2019},
%    eprint={1910.09342},
%    archivePrefix={arXiv},
%    primaryClass={cs.CL}
%}, NLU and many others

The authors of \cite{Kobus17domaincontrol} propose to inject additional domain feature in the model, either in the form of an extra (initial) domain-token, or in the form of additional domain-feature associated to each word. This technique has also been used to control the style of MT outputs in \cite{Sennrich16politeness}, and to the source or target languages in multilingual MT \cite{Firat16multiway,Johnson17google}. This approach is extended in \cite{Pham19generic} where source word embeddings are likely to vary across domains: this is achieved by spliting the embedding vector into a generic and a domain-specific part (one for each possible domain). Such feature-based approaches strongly rely on earlier proposals from the era of statistical MT \cite{Daume07frustratingly,Clark12onesystem}. Domain control can also be included in the target side, as in \cite{Chen16guided}, where a vector of topic describing the current document is included as an extra context in the softmax layer of the decoder.  

In \cite{Britz17mixing}, several methods are considered which aim to ensure that domain information is actually used in a mix-domain system. Three alternatives are considered, using either domain classification (or domain normalization, via adversarial training) on source or target representations. There is no clear winner in either of their three language pairs considered, which only distinguish a limited number of domains. The same basic techniques are simultaneously at play in \cite{Zeng18multidomain,Su19exploring}: in this approach, the upper layers of the MT aimed to sort out domain specific information on the one hand, and domain-agnostic information on the other hand, using auxiliary classification tasks, hoping that the domain-specific part will capture specific peculiarities of a domain, while the domain-agnostic part will be useful across the board. These techniques are implemented in an attentional sequence-to -sequence architecture in \cite{Zeng18multidomain}, and in Transformers in \cite{Su19exploring}.

A slightly more complex parameter-sharing scheme is presented in \cite{Jiang19multidomain}, which augments a transformer model with domain specific heads, whose individual contributions are regulated at the word (position-level): some words have ``generic'' use and rely on mixed-domain heads, while for some other words it is preferable to use domain specific heads. The results for three language pairs outperform several standard baselines for a 2-domain systems (in fr:en and ge:en) and a 4-domain system (zh:en).% \footnote{The reported results for WDC+WL are very bad, worst than anything \fyTodo{This is a mystery to me}.}   

The work of \cite{Tars18multidomain} makes a good case against fine-tuning, which is often found to be too brittle, a defect that MDMT can fix, delivering superior generalization accuracy. They perform experiment with reference domain tags, using mostly the domain-control approach. They also report results obtained with automatically induced domain tags, which show that working with automatically induced tags (at the sentence level) is a viable alternative to using corpus labels as domain tags, a result that we have confirmed in section~\ref{}.

\newcite{Farajian17multidomain} also used automatic domains, performing on-line adaptation at the sentence-level. In their approach, the processing of a test sentence triggers the selection of a dedicated (small) set of fine-tuning instances assumed to be similar to the input text; using these, the generic NMT system is then tuned for some epochs, before delivering its translation. Different of most approaches studied here, this approach entirely dispenses with the notion of a domain, relying primarily on data selection/weighting techniques to handle data heterogeneity. 

\subsection{Formalizing multi-domain adaptation (MDA)}

In MDA, the typical training situation (for a classifier) considers $k$ domains with $\mathcal{D}_k(x,y)$ the joint distribution of features $x \in \mathcal{X} and$ labels $y \in \mathcal{Y}$ for domain $k$. In \cite{Mansour09domainadaptation}, the labeling function $f$ is deterministic and the joint distribution is entirely defined by the marginal $\mathcal{D}_k(x)$ and $f()$.This work further assumes $k$ hypotheses $h_k(x): \mathcal{X} \rightarrow{} \mathcal{Y}$ with an error smaller than $\epsilon$ when $x\sim \mathcal{D}_k$: $\sum_{x} L(h_k(x), f(y)) \mathcal{D}_k(x) < \epsilon_k$ and that the target distribution is a convex combination of $\{\mathcal{D}_k\}$: $\mathcal{D}_T(x) = \sum_k \lambda_k \mathcal{D}_k(x)$. \cite{Mansour09domainadaptation} focuses on \textsl{combined hypotheses}, ie.\ hypotheses for which each prediction $h(x)$ is a linear combination of domain specific hypotheses $h(x_k)$, and derives generalization bounds for the case where $\mathcal{D}_T(x)$ is fixed in advance, or worst case bounds valid for any mixture target distribution. The optimal solution has the form $h_{z,\eta}(x) = \sum_k \frac{z_kD_k(x) + U(x)/\gamma}{\sum_{k'} z_kD_k(x) + \eta U(x)/l}$ and is termed the \emph{distribution weighted combined rule}. Further work in \cite{Mansour09multiple} generalizes these bounds to the case where $\mathcal{D}_T$ is not a mixture of existing domains (in that case we will find a mixture that is close to $\mathcal{}$in the Renyì-divergence sense ); where $\mathcal{D}_k$ are unknown and estimated from the data; or when the labelling function is not the same across domains. More recent work study the case where the labelling function is stochastic, meaning that domains are defined by joint ($\mathcal{D}_k(x,y)$) rather than marginal distributions \cite{Hoffman18algorithms}, and provides an algorithm to compute the optimal weights for the combined rule based on estimates of the joint distributions. A guiding principle is that this rule should tend towards uniform performance across domains, to ensure robustness in the worst case scenario \cite{Oren19distributionally}.

A related problem is \emph{Multiple source single domain adaptation} which is a variation of conventional DA from a mixture of source domains. In this setting it is advantageous to take advantage of the similarities between each source and the fixed target domain (see eg.\ \cite{Duan12domain,Cortes19AdaptationBO,Wen19domain} for studies of the supervised case, \cite{Zhao18adversarial} for the unsupervised case).

\section{Conclusion and outlook \label{sec:conclusion}}
\fyTodo{Write conclusions}

% --------------
% \section*{Acknowledgements}
% This work was granted access to the HPC resources of [TGCC/CINES/IDRIS] under the allocation 20XX- [numéro de dossier] made by GENCI (Grand Equipement National de Calcul Intensif)
% The acknowledgements should go immediately before the references.  Do
% not number the acknowledgements section. Do not include this section
% when submitting your paper for review.

% include your own bib file like this:
\bibliographystyle{coling}
\bibliography{multidomain}

\appendix{}

\begin{table}
  \centering
  \begin{tabular}{|p{3.2cm}|*{30}{r|}} \hline
    Model / Set-up &d1&d2&d3&d4&d5&d6&d7&d8&d9&d10&d11&d12&d13&d14&d15
    &d16&d17&d18&d19&d20&d21&d22&d23&d24&d25&d26&d27&d28&d29&d30 \\\hline
    %
    & \multicolumn{30}{|p{20cm}|}{%
      }% \hline
    \\
    %
    domain size ($\times 10^3$) &&&&&&&&&&&&&&&&&&&&&&&&&&&&&& \\ \hline\hline
    %
   \system{FT-Full}      &&&&&&&&&&&&&&&&&&&&&&&&&&&&&& \\  
    \system{DC-Tag}    &&&&&&&&&&&&&&&&&&&&&&&&&&&&&& \\     
    \system{DC-Feat}   &&&&&&&&&&&&&&&&&&&&&&&&&&&&&& \\ 
    \system{TTM}         &&&&&&&&&&&&&&&&&&&&&&&&&&&&&& \\ 
    \system{LDC-Tag}  &&&&&&&&&&&&&&&&&&&&&&&&&&&&&& \\ 

    \system{MDL Res} 
%    &&&&&&&&&&&&&&&&&&&&&&&&&&&&&& \hline
  \end{tabular}
  \caption{Detailed results of the experiments with automatic domains}
  \label{tab:automatic_domains}
\end{table}

\todos{}

\end{document}