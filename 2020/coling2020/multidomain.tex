%
% File coling2020.tex
%
% Contact: feiliu@cs.ucf.edu & liang.huang.sh@gmail.com
%% Based on the style files for COLING-2018, which were, in turn,
%% Based on the style files for COLING-2016, which were, in turn,
%% Based on the style files for COLING-2014, which were, in turn,
%% Based on the style files for ACL-2014, which were, in turn,
%% Based on the style files for ACL-2013, which were, in turn,
%% Based on the style files for ACL-2012, which were, in turn,
%% based on the style files for ACL-2011, which were, in turn, 
%% based on the style files for ACL-2010, which were, in turn, 
%% based on the style files for ACL-IJCNLP-2009, which were, in turn,
%% based on the style files for EACL-2009 and IJCNLP-2008...

%% Based on the style files for EACL 2006 by 
%%e.agirre@ehu.es or Sergi.Balari@uab.es
%% and that of ACL 08 by Joakim Nivre and Noah Smith

\documentclass[11pt]{article}
\usepackage{coling2020}
\usepackage{times}
\usepackage{url}
\usepackage{latexsym}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage[draft]{todo}
%\setlength\titlebox{5cm}
%\colingfinalcopy % Uncomment this line for the final submission

% You can expand the titlebox if you need extra space
% to show all the authors. Please do not make the titlebox
% smaller than 5cm (the original size); we will check this
% in the camera-ready version and ask you to change it back.
\newcommand{\fyTodo}[1]{\Todo[FY:]{\textcolor{orange}{#1}}}
\newcommand{\fyTodostar}[1]{\Todo*[FY:]{\textcolor{orange}{#1}}}
\newcommand{\fyDone}[1]{\done[FY]\Todo[FY:]{\textcolor{orange}{#1}}}
\newcommand{\fyDonestar}[1]{\done[FY]\Todo[FY:]{\textcolor{orange}{#1}}}
\newcommand{\jcTodo}[1]{\Todo[JC:]{\textcolor{orange}{#1}}}

\newcommand{\src}{\ensuremath{\mathbf{f}}} % source sentence
\newcommand{\trg}{\ensuremath{\mathbf{e}}} % target sentence
\newcommand{\domain}[1]{\texttt{\textsc{#1}}}
\newcommand{\system}[1]{\texttt{\textbf{#1}}}

\title{Revisiting Multi-Domain Machine Translation}

\author{First Author \\
  Affiliation / Address line 1 \\
  Affiliation / Address line 2 \\
  Affiliation / Address line 3 \\
  {\tt email@domain} \\\And
  Second Author \\
  Affiliation / Address line 1 \\
  Affiliation / Address line 2 \\
  Affiliation / Address line 3 \\
  {\tt email@domain} \\}

\date{}

\begin{document}
\maketitle
\begin{abstract}
Abstract goes here
\end{abstract}

\section{Introduction}
\label{intro}

%
% The following footnote without marker is needed for the camera-ready
% version of the paper.
% Comment out the instructions (first text) and uncomment the 8 lines
% under "final paper" for your variant of English.
% 
\blfootnote{
    %
    % for review submission
    %
    \hspace{-0.65cm}  % space normally used by the marker
    Place licence statement here for the camera-ready version. See
    Section~\ref{licence} of the instructions for preparing a
    manuscript.
    %
    % % final paper: en-uk version 
    %
    % \hspace{-0.65cm}  % space normally used by the marker
    % This work is licensed under a Creative Commons 
    % Attribution 4.0 International Licence.
    % Licence details:
    % \url{http://creativecommons.org/licenses/by/4.0/}.
    % 
    % % final paper: en-us version 
    %
    % \hspace{-0.65cm}  % space normally used by the marker
    % This work is licensed under a Creative Commons 
    % Attribution 4.0 International License.
    % License details:
    % \url{http://creativecommons.org/licenses/by/4.0/}.
}

Data-based Machine Translation (MT), whether statistical or neural, rests on well understood machine learning principles. Given a training sample of paired source-target sentences $(\src,trg)$ .\jcTodo{shouldnt trg be e as below ?} drawn from an underlying distribution $\mathcal{D}$, a model parameterized by $\theta$ (here a translation function $f_{\theta}$) is trained by minimizing the empirical expectation of a loss function $\ell(f_\theta(\src, \trg))$. This approach ensures that the expectation of the translation loss will remain low when translating new (test) sentences \emph{drawn from the same distribution}.

Owing to the great variability of language data, this ideal situation is rarely met in practice, warranting the study of an alternative scenario, where the test distribution $\mathcal{D'}$ differs from $\mathcal{D}$. In this setting, \emph{domain adaptation} (DA) methods are required. DA has a long history in Machine Learning in general \cite{Daume06domain} and in NLP in particular \cite{Blitzer07domain}, and various techniques exist to handle both the situations where a (small) training sample drawn from $\mathcal{D'}$ is available in training, or where only samples of source-side (or target-side) sentences are available (see \cite{Chu18asurvey} for a recent survey of DA for Neural MT).
% In a nutshell, DA attempts to compensate for the distribution mismatch either by biasing the training sample to match ${\mathcal{D'}$ (eg.\ using resampling or instance-weighting strategies), or by adapting the 

A seemingly related problem is multi-domain (MD) machine translation \cite{Hassan17neural,Farajian17multidomain,Kobus17domaincontrol,Zeng18multidomain,Pham19generic} when \emph{a single system} has to be trained  and tested with data from multiple domains. As pointed out eg.\ by \newcite{Dredze08online}, this setting borrows much from DA, but also from multi-task learning \cite{Caruana97multitask}.  If the intuitions behind MD may seem simple, the exact specifications of MDMTs system are rarely spelled out, which makes their evaluation difficult. For instance, should a simple score (eg.\ BLEU) averaged across domain be reported, as in \cite{Farajian17multidomain}, or a domain-weighted average as in \cite{}? How about domains not seen in training or domain labelling errors? How should they be taken into account in the evaluation?

A related question: Can a MD learning improve over DA ? 
 
In this paper, we try to answer these questions and to spell out in a more precise fashion the requirements that a MDMT system should meet. Our main contribution is of methodological nature and is list of expected properties of MDMT system, and associated measurements to evaluate them. In addition, we provide a thorough reanalysis of most recent multi-domain approaches from the literature, and also propose two variants of an original model.
 
% From Kobus et al
% Our goal is toallow a model built from a diverse set of trainingdata  to  produce  in-domain  translations.  This is, to extend the coverage of generic NMT models to specific domains, with their specialized terminology and style, without lowering translation quality on more generic data.

\section{Requirements of multi-domain MT \label{sec:requirements}}
In this section, we try to clearly spell out the reasons for considering a multi-domain scenario, and discuss the implications in terms of performance evaluation.

\subsection{MDMT: a tentative definition}
Informally, a multi-domain system should be able to accurately translate heterogeneous sources of data. This loose definition correspond to several practical situations:
\begin{itemize}
\item the more common situation is when both training and testing documents (or sentences) are domain-tagged, where each domain corresponds to a specific source of data, and differ from the other documents in terms of textual genre, thematic content \cite{Zhang16topicinformed}, register, style \cite{Niu18multitask}, etc. It is usally assumed that all the set of test domains is the same as the set of training domains -- when this is not the case, we can have zero-shot domain adaptation, similar to what is studied in multilingual MT \cite{Firat16multiway,Ha16towards,Johnson17google,Platanios18contextual}. This is the more general setting, already proposed in \cite{Dredze09multidomain}.\fyTodo{do we need a name: such as supervised MDMT ? and mostly supervised MDMT for the second case ?}

\item It can also be the case that domains are only known in training, while test inputs may or may not be associated with explicit domains. This is for instance the setting adopted \cite{Zeng18multidomain}, where automatic domain identification is performed for test sentences. Note that automatic domain identification can also be effectively performed in the first setting, especially in the presence of related domains in training.
  
\item A last setting, that has not received much attention for MT (see however \cite{Sennrich13multidomain}), is where both train and test domains are automatically infered from data, using clustering techniques. Even though domain labels are typically available in most training scenario, these approaches assumes that might be preferable, owing to the multiple sources of intra-corpus and inter-corpus variabilities, to resort to automatic domain assignments. The online approach of \cite{Farajian17multidomain}, where surface similarities cues, rather that categorical tags, are used for fine-tuning, is a variant of this strategy.
\end{itemize}

In the rest of the paper, we mostly adhere to the first definition, and assume that multidomain MT is mostly about effectively using domain tags in training and testing. We also experiment in Section~\ref{sec:results} with the other two alternative scenarios, as they also provide useful contexts to evaluate the actual robustness of MDMT systems.

\jcTodo{...a very common scenario in MT industry where clients dispose of mulitiple data corresponding to different domains (differing in genre, thematic, register, style, etc.). }

\subsection{Reasons for using MDMT \label{ssec:whymdmt}}
The motivations for considering MDMT are many. Looking at the literature, the main arguments in support of MDMT seem to be the following:
\begin{enumerate}
\item[M1] computational / practical arguments. A first motivation for MDMT is practical: when faced with inputs that are potentially from multiple domains, it is easier and computationally cheaper to develop only one system instead of having to optimize (and maintain) multiple MT engines \cite{Sennrich13multidomain;Farajian17neural}. Such arguments are all the more convincing that the number of domains considered is large, which is not always the case: for instance \cite{Britz17mixing,Zeng18multidomain,Jiang19multidomain} only consider 2-4 domains in their experiments.
  
\item[M2] statistical arguments. The training data available for each domain is very unevenly distributed, and domain trained or adapted on small datasets will yield large estimator variance. Building mix-domain systems is likely to reduce this variance, at the expense of a larger statistical bias \cite{Clark12onesystem}. Under this view, MDMT would be especially useful for domains with little training data: this is for instance what we observe for multilingual MT from English: an increase for under-resource languages due to positive transfer, at the cost of a decrease for well-resourced languages \cite{Arivazhagan19massively}. A slightly distinct line of reasoning is that mixing domains has a positive regularization effect for all domains. By introducing variability in training, it prevents domain adaptation from overfitting the available adapation data, and could help improve generalization even for well-resourced domains.
  
\item[M3] linguistic arguments. A related line of reasoning contends that domain specificities are mostly expressed lexically and will primarily affect content words or multi-word expressions; function words on the other hand are domain agnostic and remain semantically stable accross domain. A MDMT system should simultaneously learn lexical domain peculiarities, and leverage cross-domain to improve the representation of generic contexts and words \cite{Zeng18multidomain,Pham19generic}. It is expected that the MDMT scenario should be more profitable when the domain mix includes domains that are closely related, and which can share more information. \fyTodo{polysemy}
\end{enumerate}

These various reasons for adopting MDMT are not mutually exclusive. Each however yields variable expectations with respect to the performance of this approach: if the motivation is primarily computational, then a drop in MT quality with respect to fine-tuning each domain might be acceptable; if it is to improve statistical estimation, then the hope will be that MDMT will improve, at least for some domains, if not all, over individually trained systems.

\subsection{Evaluation issues \label{ssec:evaluation}}

In theory, evaluation procedures should report realistic estimates of the test loss, computed on a sample deemed representative of the anticipated test conditions. Three scenarios need be distinguished:
\begin{enumerate}
\item test data are mixed in the same proportion as training data; this warrants the design of an unbalanced mix-domain test sets in the same proportions as in training; alternatively, if per domain scores are ever reported, their average should take the mixing proportions into account;
\item all domains are equally likely and important in testing (the prior over test domains is uniform), which would warrant the computation of per domain scores and of unweighted averages, as reported in \cite{Farajian17multidomain,Su19exploring}. Note that in this situation, the optimal training regime would be to balance the training set in the same way -- which is hardly ever done in practice, as the baseline scores always consider mixing unevenly all available data.
\item the test distribution is neither equal to the training set, nor uniform; again if this distribution is known in advance an optimal domain mix should be used in training to compute the baseline scores, rather than using the empirical distribution of all the training data. A more challenging case is when the test includes data from domains unseen in training, that is test instance that have a null probability in training. The likeliness of such events should be clearly formulated when describing the evaluation protocol.
\end{enumerate}

The papers in the literature are seldom very explicit regarding their assumption regarding the test distribution. In most cases, per-domain performances are reported, and even sometimes averaged, suggesting that the scenario~2 most faithfully reflects the expected testing condition. 

Note that when computational cost is the main motivation for considering MDMT, evaluation should report the cost of training one single versus multiple systems, which is hardly even done in practice. \fyTodo{Keep this ?}

\section{Challenging multi-domain translation systems}
In this section, we discuss the operational requirements that can be expected from a true multi-domain system, as well as ways to evaluate how much these requirements are met in practice.

\subsection{Multi-domain systems should be effective \label{ssec:effective}}
% \subsubsection{Performance}
The first requirement of a MDMT is that it should perform well in the face of mix-domain data. These yields the following desired properties.

\paragraph{[P1-LAB]}\fyTodo{Decide naming scheme} The first, and most obvious requirement of a MDMT system, is that it should perform well, at least better that the obvious baseline which would simply disregard domain labels. Testing that MDMT actually benefit from domain tags is simple, notwithstanding the previous remarks above regarding evaluation procedures.

\paragraph{[P2-TUN]} Additionally, following motivation~(M2) above, one would expect that MDMT improves over fine tuning, at least in domains where data is scarce. To evaluate this ability, we perform two measurements, using a real as well as an artificial scenario. In the real scenario, we simply compare the performance of MDMT and fine-tuning for domains of varying sizes, expecting a larger gain for smaller domains [P2.1] \jcTodo{what is P2.1} ; in the artificial scenario, we unevenly split a single training domain in two parts which are considered as distinct in training. The prediction here is that a MDMT should yield a clear gain for the both pseudo sub-domains as compared to fine-tuning, as both systems should benefit from the supplementary amount of relevant training.  \jcTodo{I dont understand the prediction... why running MDMT on 2 artificial subdomains should show gains over fine-tunning?}

\subsection{Robustness with respect to domain definition \label{ssec:robusness}}
A second set of requirements is related to the definition of domains. As repeatedly pointed out in the literature, training corpora for MT are often collected opportunistically, and the view that each corpora constitues a single domain is a gross approximation.\footnote{Two of our own domains are composed of several subcorpora (IT and MED), see details in section~\ref{ssec:corpora}.} MDMT should aim to make the best of the available data, and be robust to domain assignments. To challenge these requirements we propose to perform the following tests. 

\paragraph{[P3-HET]}
The notion of a domain is often a fragile one and an effective MDMT system should be able to discover not only when cross-domain sharing is most useful [P2.2] \jcTodo{what is P2.2?}, but also when intra-domain heterogeneity is hurting. Requirement [P3] is tested by artificially conjoining two separate domains in one during training. The loss in performance with respect to the baseline (correct domain tags) should then be as small as possible.

\paragraph{[P4-ERR]}
A MDMT should perform best when the true domain tag is known, but deteriorate gracefully in the presence of domain tag errors. This is in sharp contrast with what is commonly observed with fine-tuning, where giving the wrong domain tag can yield ``catastrophic'' drop in performance. This requirement is measured by decoding the test set for domain A with all other domain tags, and report average drop in performance.

\paragraph{[P5-UNK]}
A related situation occurs when the domain of a test document is unknown. Several alternatives are in order: (a) automatically assigning one of the domain tag for decoding should not be much worse than using the true domain tag, assuming this domain is seen in training. For test documents from other domains, the same procedure can apply, and should outperform the use of a single mix-domain system. \fyTodo{Systems react to unknown domains}These requirements are respectively labelled [P5.1] and [P5.2]\fyTodo{Not so sure of these ones}.

\paragraph{[P6-DYN]}
Another requirement, more of an operational nature, is that a MDMT system should smoothly evolve to accomodate a larger number of domains, without having to retrain the full system each time new data is available. This is a requirement [P6] that we challenge by dynamically changing the number of training and test domains.

\subsection{Effectiveness in handling a large number of domains \label{ssec:scaling}}

\paragraph{[P7-NUM]} As mentioned above, the computational argument (M1) is all the more sensible as the number of domains increases, making the optimization of many individual systems both ineffective and undesirable. For lack of having access to corpora containing a large (eg.\ in the order of 100-1000 domains), we experiment with automatically learned domains, considering a varying number of clusters.

\section{Experimental settings \label{sec:experiments}}

\subsection{Data and metrics \label{ssec:corpora}}

We experiment with translation from English into French and use data originating from 7~domains, corresponding to texts from the following data sources: the UFAL Medical corpus V1.0 (\domain{MED})\footnote{\url{https://ufal.mff.cuni.cz/ufal_medical_corpus}}, the European Central Bank corpus (\domain{Bank}) \cite{Tiedemann12parallel}; The JRC-Acquis Communautaire corpus (LAW) \cite{Steinberger06acquis}, documentations for KDE, Ubuntu, GNOME and PHP from Opus collection \cite{Tiedemann09news}, collectively merged in a \domain{IT}-Domain, Ted Talks (\domain{Talks}) \cite{Cettolo12wit}, and the Koran (\domain{Rel}). Most corpora are variable from the Opus web site.\footnote{\url{http://opus.nlpl.eu}} These corpora were deduplicated and tokenised with in-house tools. Complementary experiments also use v12 of the News Commentary corpus (\domain{News}). Corpus statistics in Table~\ref{tab:Corpora}). %
We randomly select in each corpus a development and a test set of 1000 lines, and keep the rest for training.  Validation sets are used to chose the best model according to the average BLEU score. Significance tests are performed as follows... \fyTodo{Fix this}.

%for contrast experiments, we also use supplementary test sets from three other domains: the official Khresmoi testset \cite{Khresmoi17test}, which is close to EMEA, News test 2014 \cite{Bojar14findings}, and IWSLT 2010 (Talk track) \cite{Paul10overview}. This enables us to evaluate the loss in performance when the test set is from a domain not seen in training.
% The model is also required to achieve comparable performance to generic model. To do so, we use newstest 2009 and IWSLT 2010 whose contain does not particularly belong to any domain.

\begin{table}[h]
  \centering
  \begin{tabular}{ |lllllll|} %*{4}{|r|}}
    \hline
    %\multicolumn{4}{|l|}{Vocab size - En: 30,165, Fr: 30,398}\\
    \domain{Med} & \domain{Law} & \domain{Bank} & \domain{IT} & \domain{Talk} & \domain{Rel} & \domain{News} \\
    \hline
    2609 (0.68) & 190 (0.05)  & 501 (0.13) & 270 (0.07) & 160 (0.04) & 130 (0.03) & 260  \\
    \hline
  \end{tabular}
\caption{Corpora statistics: number of parallel lines ($\times 10^3$) and proportion in the basic domain mix.}
\label{tab:Corpora}
\end{table}

To reduce the number of types and build open-vocabulary systems, we use Byte-Pair Encodings \cite{Sennrich16BPE} with 30,000 merge operations on a corpus containing all sentences in both languages.

\subsection{Baselines \label{ssec:baselines}}

Baselines correspond to standard practices for multi-domain systems.\footnote{We however omit domain-specific systems trained with a subset of the data, which are always inferior to the mix-domain strategy eg.\ \cite{Britz17mixing}.} Using the Transformer architecture of \shortcite{Vaswani17attention} implemented in OpenNMT-tf\footnote{\url{https://github.com/OpenNMT/OpenNMT-tf}} \cite{Klein17opennmt}, we build the following systems:

\begin{itemize}
\item a generic model trained on a concatenation of all corpora (\texttt{Mixed}). We develop two versions of this system, one where domains are unbalanced (\system{Mixed-Nat}) and one where all domains are equally represented in training (\system{Mixed-Bal}) (cf.\ section~\ref{sec:evaluation}); \jcTodo{All systems cand be learned following both Bal and Nat. does Nat corresponds to wAVG in test? is wAVG defined somewhere?}
\item models fine-tuned \cite{Freitag16fast} \jcTodo{i would use Luong and Maning 2015} separately on each domain for at most 20~000 iterations, with early stopping when the dev BLEU stops increasing. In our setting, we again contrast two versions: full fine-tuning (\system{FT-Full}), which may update all the parameters of the initial generic model; and the variant of \cite{Bapna19simple}, where fine-tuning only updates a simple adaptation module that is added (with residual connexions) on top of every Transformer layer (\system{FT-Block}).
\end{itemize}

For all models, we set the embeddings size and the size of hidden layers to~512. Transformers use multi-head attention with 8 heads in each of the 6 layers; the inner feedforward layer contains 2048 cells.
% The domain control systems are exactly as their baseline counterparts (RNN and Transformer), with an additional 2 cells encoding the domain on the input layer.
 %
Training uses Adam with parameters $\beta_1=0.9$, $\beta_2= 0.98$ and Noam decay ($warmup\_steps=4000$). In all cases, we use a batch size of~128 and a dropout rate of 0.1 for all layers. % All our systems are implemented in OpenNMT-tf\footnote{\url{https://github.com/OpenNMT/OpenNMT-tf}} \cite{Klein17opennmt}.
\fyTodo{Describe the block adaptation layer - voir slides} 

\subsection{Multi-domain systems \label{ssec:systems}}
Our comparison of multidomain systems includes the following reimplementations of recent proposals from the literature:
\begin{itemize}
\item a system using domain control as in \cite{Kobus17domaincontrol}: domain information is introduced either as an additional token for each source sentence (\system{DC-Tag}), or introduced in the form of a supplementary feature for each word (\system{DC-Feat}).
\item the three proposals of \cite{Britz17mixing}: \system{TTM}, another tag-based approach where the domain tag is introduced as an extra word \textsl{on the target side}. Training is performed with reference tags and inference is performed with predicted tags, just like for any other target words; \system{DM} is a multi-task learner where a domain classifier is trained on top the MT encoder, so as to make it aware of domain differences; \system{ADM} is the adversarial version of \system{DM}, pushing the encoder towards learning domain-independent source representations;
\item a system using lexicalized domain representations \cite{Pham19generic}: word embeddings are composed of a generic and a domain specific part (\system{LDC-Feat});
\item the multi-domain model of \cite{Zeng18multidomain} (\system{WDC}): in this approach two representations, a domain-agnostic and a domain-specialized representation of the input are simultaneously processed; supervised classification models and adversarial training are used to compute these representations.\footnote{For this system, we use the available RNN-based system from the authors (\fyTodo{URLs}); the improved version of \cite{Su19exploring} seems to produce comparable, albeit slighly improved version.}
\item A multidomain version of the approach by \newcite{Bapna19simple} denoted \system{MDL Res}, where we add a dedicated adaptation module for each domain, which is only activated for the appropriate set of training data; residual connexions make it possible to by-pass these modules. In a variant (\system{MDL Gated}), we use a gating mechanism to merge computations using the the adapter modules and with those that don not. Note that contrarily to \cite{Bapna19simple} we do not start with a trained generic system, but train the multi-domain from scratch. %\fyTodo{Check this.}
\end{itemize}

Note that we did not include here the system of \cite{Farajian17multidomain} which performs on-the-fly tuning for each test sentence, and slightly diverges from our notion of MDMT, which should be able to handle domain information in training and testing. This system has shown to meet many of our requirements, and would also have no problem scaling to an large number of domains.\fyTodo{TBC}

\section{Results and discussion \label{sec:results}}
\subsection{Performance of MDMT systems \label{ssec:rawperformance}}
In this section, we discuss the raw performance of MDMT systems trained and tested on 6~domains. Results are in Table~\ref{tab:performance}. As expected, balancing data in the generic setting makes a great difference (the unweighted average is 2 BP \jcTodo{BP?} better, notably owing to the much better score for \domain{religion}). As explained above, this setting should be the baseline when the test distribution is assumed to be balanced across domains. Fine-tuning each domain separately provides us with better baselines, significantly outperforming \system{generic} for all domains. A complete fine-tuning, which updates all parameters, proves more effective than only adapting a subpart of the network - a result that could be due to lack of tuning the capacity of the adapter module. \fyDone{Add 2 averages ?}
A  first group of MDMTs use an extra domain feature, at the sentence or at the word level: among these, having the domain tag on the source is better than on the target, and very comparable to repeating the tag for every source word. All improve over the generic baseline (for most domains), but yield weaker results than fine tuning.
\fyTodo{More comments when we have all the results}
The use of an decidated adaptation component in each of the transformer layer proves an even better strategy and closes the gap with fine-tuning for half of the domains: the use of a gating mechanism yielding a clear improvement over the board. The effect of the adaptation layer is especially significant for the smaller domains (\domain{bank}, \domain{IT} and \domain{rel}) for which unplugging this layer causes dramatic performance drops. 

\begin{table}
  \centering
  \begin{tabular}{|p{4cm}|*{8}{r|}} \hline
%     &&&&&& \\
    Model / Domain & \multicolumn{1}{c|}{\domain{med}} & \multicolumn{1}{c|}{\domain{law}} & \multicolumn{1}{c|}{\domain{bank}} & \multicolumn{1}{c|}{\domain{talks}} & \multicolumn{1}{c|}{\domain{IT}} & \multicolumn{1}{c|}{\domain{rel}} & \domain{wAVG} & \domain{AVG}\\ \hline
    \system{Mixed-Nat}  & 37.3 & 54.6 & 50.1 & 33.5 & 43.2 & 77.5  & 41.1&49.4\\
    \system{Mixed-Bal}   &  35.3 & 54.1 & 52.5 & 31.9 & 44.9 & 89.5 & 40.3&51.4\\
    \system{FT-Full}       & 37.7 & 59.2 & 54.5 & 34.0 & 46.8 & 90.8   & 42.8&53.8\\
   \system{FT-Block}     & 37.3 & 57.9 & 53.9 & 33.8 & 46.7 & 90.2    & 42.3&53.3\\

 %   Full-finetuned on extended in-domain corpora (news) & && 33.96&&& & &\\nn

    \system{DC-Tag}   & 38.06 & 55.27 & 49.9   & 33.24 & 43.54 & 80.45 &41.7 & 50.1\\
    \system{DC-Feat}  & 37.3   & 54.87 & 49.5   & 32.94 & 43.56 & 79.91 &41.4 & 49.9\\
    \system{TTM}        & 37.71 & 54.76 & 49.72 & 33.43 & 43.81 & 79.98 &41.1 & 49.7\\
    \system{LDC-Tag} &&&&& \\
    \system{DM}  & 36.5& 52.4& 47.7& 32.1 & 41.0 & 70.4 & 39.7 & 46.7 \\
    \system{ADM}   37.1& 53.8&	48.9&32,8 & 42.3 & 78.5 & 40,7 & 48.9 \\
    \system{MDL Res}        & 37.86 & 56.01 & 51.2   & 33.5   &  44.4  & 88.34 & 42.1 & 51.9\\
    MDL no adapt       & 37.72 & 51.02 & 33.98 & 30.35 & 34.17 & 15.17 & 36.4 & 33.7\\
    \system{MDL Gated} & 37.68 & 56.49 & 53.17 & 34.05 & 44.57 & 90.73 & 42.3 & 53.3\\
     \hline
  \end{tabular}
  \caption{Translation performance of various MDMT systems. We report the BLEU score for each domain, as well as weighted and unweighted averages.}
  \label{tab:performance}
\end{table}

\subsection{Redefining domains \label{ssec:redomains}}
Table~\ref{fig:redomains} summaries the results of \fyTodo{XX} experiments where we artificially redefine the limits of domains. In the three \textsl{split} experiments, we randomly split a domain corpus in two part: a good MDMT system should detect that these two ``domains'' should be mutually beneficial, and should be hardly affected by this change wrt.\ the baseline scenario. This is the situation where we should see clear improvements even over finetuning, which only exploits part of the available relevant data. In the \textsl{merge} experiment, we do the reverse and merge two corpora in training, in order to assess the robustness with respect to heterogenous domains. In the last column, we finally report the averaged (across domains) drop in performance when the domain information is erroneous.

\begin{table}
  \centering
  \begin{tabular}{|p{2.5cm}|*{9}{r|}} \hline
    % &&&&&& \\
    Model / Set-up & \multicolumn{2}{c|}{Split} &  \multicolumn{2}{c|}{Split} & \multicolumn{2}{c|}{Split} & \multicolumn{2}{c|}{Merge} & Wrong \\ % \hline
     Model / Set-up & \multicolumn{2}{c|}{\domain{med} \footnotesize{(0.5 / 0.5)}} &  \multicolumn{2}{c|}{\domain{med} {\footnotesize (0.75 / 0.25)}} & \multicolumn{2}{c|}{\domain{bank} {\footnotesize (0.5 / 0.5)}} & \multicolumn{2}{c|}{\domain{bank}+\domain{law}} & domain \\ \hline
    & \domain{med-1} & \domain{med-2} & \domain{med-1} & \domain{med-2} &  \domain{bank-1} & \domain{bank-2} & \domain{bank} & \domain{law} & \\
    \system{FT-Full}       & & & & & & & & & \\
    \system{DC-Tag}      & & & & & & & & & \\
    \system{DC-Feat}     & & & & & & & & & \\
    \system{TTM}           & & & & & & & & &  \\
    \system{LDC-Tag}    & & & & & & & & & \\ 

    \system{MDL Res}    & & & & & & & & & \\
    \system{MDL Gated} & & & & & & & & & \\

    \system{ADM} & & & & & & & & & \\
    \system{DM}    & & & & & & & & & \\
    \system{WDC}  & & & & & & & & & \\
    \hline
  \end{tabular}
  \caption{Translation performance with variable domain definitions. For the Split experiments, we only report the loss in performance for the \domain{med} test set, (*) denote significant differences, (+) denotes a significant improvement over \system{Full FT}; for the merge experiments for domains \domain{bank} and \domain{Law}.\fyTodo{How about wrong ?}}
  \label{tab:performance}
\end{table}

Findings so far:
\begin{itemize}
\item Split UFAL half / half: domain-tag and Gated-res are OK, all systems loose wrt baseline; 
\item Split UFAL 0,75 / 0.25: same thing, and we even see improvements for domain tags - they are small though
\item Split Law : only domain feature works ? all the rest have losses, from large to small
\item Merge domain: bad for many, OK for TTM (as the domain is learnt this is not so bad ?); bad for domain tag, domain feature is again OK ?
\end{itemize}

\fyTodo{We need significance testing}

Another series of experiments aims to evaluate requirement [P6-DYN] as follows: starting with the existing MDMT systems of Section~\ref{ssec: ssec:rawperformance}, we introduce an additional domain (\domain{News}) and resume training with a mixture of data for an additional XXX epochs. We contrast this approach with retraining all the systems from scratch, which would be the ideal situation, and report the difference in performance in Table~\ref{tab:XXX}. We expect that a good MDMT system should not be significantly affected by the addition of new data.

\subsection{Automatic domains \label{ssec:autodomains}}
\begin{todo}
  Distance entre domaines: KL -->
\end{todo}

\section{Related work \label{sec:related}}

The authors of \cite{Kobus17domaincontrol} propose to inject additional domain feature in the model, either in the form of an extra (initial) domain-token, or in the form of additional domain-feature associated to each word. This approach is extended in \cite{Pham19generic} where source word embeddings are likely to vary across domains: this is achieved by spliting the embedding vector into a generic and a domain-specific part (one for each possible domain). Such feature-based approaches strongly relies on earlier proposals from the era of statistical MT \cite{Daume07frustratingly,Clark12onesystem}.

In \cite{Britz17mixing}, several methods are considered which aim to ensure that domain information is actually used in a mix-domain system. Three alternatives are considered, using either domain classification (or domain normalization, via adversarial training) on source or target representations. There is no clear winner in either of their three language pairs considered, which only distinguish a limited number of domains. The same kinds of techniques are simultaneously at play in \cite{Zeng18multidomain,Su19exploring}: in this approach, the upper layers of the MT aimed to sort out domain specific information on the one hand, and domain-agnostic information on the other hand, using auxiliary tasks, hoping that the domain-agnostic part will be useful across the board. These two sets of representations are then input to a conventional attentional sequence-to -sequence models in \cite{Zeng18multidomain}, and with Transformers in \cite{Su19exploring}; scores are reported separately per domain, suggesting that all domains are equally important at test time.

The motivations of \cite{Tars18multidomain} conjoins both (a) they argue that fine-tuning is brittle, owing to the reduced size of the tuning data, and can impair generalization; (b) they also argue that having one domain-agnostic system is a practical solution in many contexts
\begin{quote}
  In-domain fine-tuning has two main shortcomings: it depends on the availability of sufficientamounts of in-domain data in order to avoid over-fitting and it results in degraded performance for all other domains. The latter means that for translating multiple domains one has to run an individual NMT system for each domain.
\end{quote}
In their experiments, they consider that all domains are equally important and report individual scores for each known domain; further experiments are performed with automatically induced domain tags, which show that working with automatically induced tags (at the sentence level) is a viable alternative to using corpus labels as domain tags.

\cite{Farajian17multidomain} performs on-line adaptation at the sentence-level - processing a test sentence triggers the selection of a dedicated (small) set of fine-tuning instances assumed to be similar to the input text; using these, the generic NMT system is then tuned for some epochs, before delivering its translation. No clear notion of a domain, so it would pass all our tests, presumably.  Also improves upon systems fine-tuned individually, sometimes by a wide margin. Test distribution is different from the training distribution (for domains). 

A slightly more complex parameter-scheme is presented in \cite{Jiang19multidomain}, which augments a transformer model with domain specific heads, whose individual contributions are regulated at the word (position-level): some words have ``generic'' use and rely on mixed-domain heads, while for some other words it is preferable to use domain specific heads. The results for three language pair outperform the standard baselines for a 2-domain systems (in fr:en and ge:en) and a 4-domain system (zh:en)\footnote{The reported results for WDC+WL are very bad, worst than anything \fyTodo{This is a mystery to me}.}   

\section{Conclusion and outlook \label{sec:conclusion}}

% --------------
% \section*{Acknowledgements}

% The acknowledgements should go immediately before the references.  Do
% not number the acknowledgements section. Do not include this section
% when submitting your paper for review.

% include your own bib file like this:
\bibliographystyle{coling}
\bibliography{multidomain}

\end{document}
