\documentclass[12pt,times,a4paper,twoside]{article}

\usepackage[utf8]{inputenc}	% Para caracteres en espa√±ol
\usepackage[left=3cm,right=3cm,top=2cm,bottom=3cm]{geometry}
\usepackage{pagenote}

\usepackage{algorithm}
\usepackage{algorithmic}

\usepackage{amsmath,amsthm,amsfonts,amssymb,amscd}
\usepackage{mathtools,xparse}
\usepackage{mathrsfs}

\usepackage{multirow,booktabs}
\usepackage[table]{xcolor}
\usepackage{fullpage}
\usepackage{lastpage}
\usepackage{enumitem}
\usepackage{fancyhdr}

\usepackage{graphicx}
\usepackage{wrapfig}
\usepackage{setspace}
\usepackage{calc}
% \usepackage{color,soul}
\usepackage{multicol}
\usepackage{cancel}
\usepackage[retainorgcmds]{IEEEtrantools}
% \usepackage{xcolor} -- implied by xcolor

\usepackage{listings}
\usepackage{spverbatim}
\usepackage{fancyvrb}
\usepackage{hyperref}
\usepackage{float} 
\usepackage{natbib}

\usepackage{tikz}
\usepackage{pgfplots}
\usepackage{pgfplotstable}

\usepackage{soulutf8}
\usepackage{tabularx}

\usepackage[draft]{todo}
\newcommand{\fyTodo}[1]{\Todo[FY:]{\textcolor{orange}{#1}}}
\newcommand{\fyTodostar}[1]{\Todo*[FY:]{\textcolor{orange}{#1}}}
\newcommand{\fyDone}[1]{\done[FY]\Todo[FY:]{\textcolor{orange}{#1}}}
\newcommand{\fyDonestar}[1]{\done[FY]\Todo[FY:]{\textcolor{orange}{#1}}}

\colorlet{shadecolor}{orange!15}
\parindent 0in
\parskip 12pt
\geometry{margin=1in, headsep=0.25in}
\setlength{\belowdisplayskip}{8pt} \setlength{\belowdisplayshortskip}{8pt}
\setlength{\abovedisplayskip}{8pt} \setlength{\abovedisplayshortskip}{8pt}
\setlist{nosep}
\setlength{\parskip}{0.1cm}
\setlength{\parindent}{1em}

\theoremstyle{definition}
\newtheorem{defn}{Definition}
\newtheorem{lemma}{Lemma}[section]
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}{Proposition}[section]
\newtheorem{definition}{Definition}[section]
\DeclareMathOperator*{\argmax}{argmax}
\newcommand{\R}{\ensuremath{\mathbb{R}}}

\pgfkeys{
    /tr/rowfilter/.style 2 args={
        /pgfplots/x filter/.append code={
            \edef\arga{\thisrow{#1}}
            \edef\argb{#2}
            \ifx\arga\argb
            \else
                \def\pgfmathresult{}
            \fi
        }
    }
}

\DeclarePairedDelimiter{\abs}{\lvert}{\rvert}
\DeclarePairedDelimiter{\norm}{\lVert}{\rVert}
\NewDocumentCommand{\normL}{ s O{} m }{%
  \IfBooleanTF{#1}{\norm*{#3}}{\norm[#2]{#3}}_{L_1}%
}

\title{Rebuttal: Revisiting Multi-Domain Machine Translation}
\author{}
\date{}

\begin{document}
\maketitle
\section*{General comments}
As TACL action editor for submission 2327, "Revisiting Multi-Domain Machine Translation", I am happy to tell you that I am accepting your paper subject (conditional) to your making specific revisions within two months.

Please address all comments by the reviewers, paying special attention to the following:
\begin{itemize}
	\item Address the concern by reviewer A about the difference between MDL Res and FT-Block, and include FT-block (and ideally FT-Full also) in the later experiments (5.2.2 and 5.3).
	\item Carefully specify the number of parameters for all models, including the proportions that are domain-specific and domain-independent.
	\item Provide all the experimental details requested, and in particular give enough information to allow the corpora to be reproduced, or (ideally) make them publicly available.
\end{itemize}

\section*{Reviewer A}
Reviewer A:
\\
Overall, I'm a bit ambivalent about the paper. It is a well-written paper with a carefully-executed set of experiments. An important contribution of the paper is to formulate a set of expected properties of multi-domain MT (MDMT) (against which existing approaches are evaluated), and I generally like the choice of properties and that the paper draws attention to them, for example systematically testing how multi-domain approaches behave when gold domain labels are suboptimal. However, it is ambitious to systematically compare domain adaptation or multi-domain approaches which were developed with various conditions in mind. In several cases, I get the feeling that in an attempt to be broad and systematic, the authors have drawn boundaries and distinctions where none need exist, or drew conclusions that are too general. Details follow below.
\\
The authors have chosen a setting where domain labels are generally
available at training and test time, and in the the empirical comparison, focus on approaches that use domain labels at training and test time (533-537), but this is only partially true: their comparison does include systems which only use domain labels at training time (TTM/DM/ADM/WDCMT), and one set of experiments operates with automatically induced domain labels based on k-means clustering of sentence representations. Systems which were designed to work without domain labels, such as that by Farajian et al.(2017) would be a very relevant comparison at least in the latter setting without gold domain labels.
\\
The authors consider fine-tuning separate from MDMT, and declare failure in that no MDMT system meets the expectation that it should outperform fine-tuning. I'm confused why fine-tuning is specifically excluded, since it meets many of the reasons for building MDMT systems. Authors do mention that it would be cheaper to develop a single system instead of having to maintain multiple systems, but they include "MDL Res", which has domain-specific adaptation modules and is very similar to "FT-Block" (a baseline that also combines a general model with domain-specific adaptation modules). The line
between a single system and multiple systems gets fuzzy here, and I see no clear motivation to draw this clear distinction and only consider "MDL Res" a multi-domain system.
\\
The large number of approaches considered is admirable. However, the depth of the discussion suffers as a consequence, and the paper risks making over-general statements about the weaknesses of different approaches, despite related work that has already addressed them. The authors conclude that fine-tuning "incurs large losses [when applied on] the small data condition", and fails to deteriorate gracefully in the presence of tag errors, issues that have been addressed by Miceli Barone et al. (2017) and Thompson et al. (2019), respectively. Similarly, the negative impact of adding data from a "new" domain seems slightly overblown. Firstly, this is probably highly dependent on data mixtures, length of training and regularization strategies, so I'd be wary of drawing general conclusions from a single set of experiments. Secondly, when looking at the models where such a performance deterioration is said to happen (DC-Tag, TTM, ADM; lines
770-771), this deterioration is actually small or non-existent when
comparing table 3 and table 5 averages (-0.2 BLEU for DC-Tag; -0.3 BLEU for TTM; +0.9 BLEU for ADM).
\\
Question to authors:
\\
- can you describe in more detail the differences between FT-Block and MDL Res, apart from the fact that the former is fine-tuned from a generic model, the latter trained from scratch?
\\
\textcolor{red}{The two methods use the same architecture composing of one standard Transformer model and domain adapters presented in the paper of \cite{Bapna19simple}. The only difference between FT-block and MDL Res is their training process.
\\
To train FT-block, we plug residual adapters to the output of each self-attention layer of the model Mixed-NAT, then resume the training with the learning rate captured in the end of the training of the model Mixed-NAT while freezing all of the parameters of the model Mixed-NAT. We use only the in-domain data as the training data of the finetuning procedure. 
\\
To train MDL res, we optimize the parameters of the Transformer model and the parameters of the residual adapters from scratch. Precisely, to create a training batch, we randomly pick a domain with a probability equal to the proportion of the domain in training data, then we sample a batch from its corpora. Given a batch from domain i, we optimize the parameters of Transformer model and the parameters of the residual adapters corresponding to the domain i by plugging the residual adapters of domain i to the Transformer model and calculating gradients of the activated parameters.}
\\
minor issues:
\\
- I disagree that generalization to unseen domains is similar to zero-shot multilingual MT. In zero-shot multilingual MT, both the source and target distribution have been observed at training time, just paired with other languages. In the generalization to unseen domains, there may be characteristics in the data that were never seen in the training data, including new vocabulary (in the source text, target text, or both).
\\
- 567-568: "having the domain tag on the source is better than on the
target". I assume this refers to the comparison of "DC-Tag" and "TTM". However, TTM actually uses the predicted domain label on the target side, and could perform better with forced decoding of the gold domain label. More precisely, the statement should be that "having the gold tag is better than a predicted tag", but this is hardly surprising.
\\
\textcolor{red}{We agree that the conclusion is not really fair for "TTM" because "TTM" intend to predict domain label rather than to use gold domain label and TTM could definitively translate with gold domain label as prefix of each hypothesis. We will report the performance of "TTM" when translating using gold domain label as prefix in target side.}
\section*{Reviewer B}
The paper presents a thorough review of Multi-Domain Machine Translation (MDMT) approaches, and the authors provide comprehensive experiments to study different aspects of MDMT. The innovation of this work can be seen in two main categories:
\begin{itemize}
	\item suggesting a methodology to evaluate MDMT models based on seven properties that they defined in the paper.
	\item empirical studies on eight recent MTMD approaches.
\end{itemize}
In general, the paper has written well, and it is a good read for those who are researching in the domain adaptation field. 
\\
I have a few comments:
\\
* You provide some useful experimental results on various MDMT approaches. To benefit from your results, we need access to the same segmentation of train/dev/test sets that I hope you will provide to the community.
\fyTodo{to share data}
\\
* On your discussion on the number of parameters (L. 591), could you share the number of parameters per each MDMT approach? It will be helpful to see  the impact of the number of parameters on the MT 
results.
\\ 
\textcolor{red}{The description of model in each MDMT approach is as follows:
\begin{enumerate}
	\item FT-full, Mixed-NAT, TTM, DC-Tag use medium Transformer model.
	\item FT-block and MDL res use medium Transformer model, and residual layers with the bottleneck dimension of size 1024. \cite{Bapna19simple} showed that if the bottleneck dimension is large enough, the gap between FT-block and FT-full will be small.
	\item ADM, DM use medium Tranformer model and a domain classifier composing of 3 dense layers of size $512 \times 2048$, $2048 \times 2048$ and $2048 \times domain\_num$. The two first layers of the classifier use relu as activation function, the last layer uses tanh as activation function.
	\item DC-Feat uses medium Transformer model, and domain embeddings of size 4. Given a sentence of domain i in training batch, the embedding of domain i is concatenated to the embedding of each token in the sentence.
	\item LDR uses medium Transformer model and for each token we introduce a LDR feature of size $4 \times domain\_num$. Given a sentence of domain i $i\in[1,..,K]$ in training batch, for each token of the sentence, the LDR units of the indexes outside of the range $[4(i-1),..,4i-1]$ are masked to 0, the masked LDR feature will be concatenated to the embedding of the token.
	\item WDCNMT uses one layer GRU bidirectional as encoder and one layer GRU-conditional as decoder. The size of each hidden layer is 1024, the size of word embedding is 512.
	\item Mixed-NAT-RNN uses one layer LSTM bidirectional as encoder and one layer LSTM as decoder. The size of each hidden layer is 1024, the size of word embedding is 512. 
\end{enumerate}
}
\fyTodo{system's description}
\\
* Thinking about your seven operational requirements, I wonder if you could also define a domain or the minimum number of samples to specify a domain. I like your selections of domains that you include multiple domains with varying sizes, but what we expect in extreme cases. Having your Split experiments on REL (Table 4), or providing further details on your last analysis in Section 5.3, if you have tiny clusters could help.
\\
* Please define Lambda coefficients in lines 142 and 146.
\\
\textcolor{red}{We formalize the data distribution as a mixture of several in-domain distribution. $\lambda$ is the prior distribution on the set of domains. We hypothesize that training distribution is different from testing distribution, so we have 2 different mixtures.}
\\
* On dynamically changing the number of training and test domains, there is a new related work published recently that might worth citing it: Sharaf A, Hassan H, Daum√© III H. Meta-Learning for Few-Shot NMT Adaptation.
\\
\textcolor{red}{This is a very interesting work that we would like to cite in our final version of the paper.}
\\
* I‚Äôd prefer to see the number of running words in Table 1, especially when the batch size is given as the number of tokens (L. 485).
\\
\fyTodo{to report number of tokens of the datasets}
\\
* L. 503: It seems you refer to this method as LDR in the rest of the paper. Please make them consistent.
\\
\textcolor{red}{We have noted this remark. We agree to change the notation correctly.}
\\
* L. 576: Although you describe in footnote 11, it is better to mention that you compare WDCMT results with Mixed-NAT-RNN explicitly.
\\
\fyTodo{to explicitly compare WDCNMT with Mixed-NAT-RNN}
\\
* L. 681-683: I don‚Äôt see it as a failure, and I think it is a property (requirement) as you describe in the definition of [P4-ERR], we expect deterioration in the presence of tag error. Am I right?
\\
* L. 687: Please describe how you predict the domain tag, and please share the tag prediction error rate on each domain test set. It is helpful to see the amount and the impact of the error on MDMT results.
\\
\textcolor{red}{We train a language model in source language for each domain using respective source training corpus. To predict domain tag of a sentence, we compute probability of the sentence according to each pretrained language model and choose the domain corresponding to the highest probability as domain tag of sentence.} \fyTodo{to compute error rate of domain prediction over each test set}
\\
I spotted a few typos in the text:
\\
* L 100: section 2 $\rightarrow$ Section 2
\\
* L 104: section 3 $\rightarrow$ Section 3
\\
* L 256: could ben $\rightarrow$ could be (?)
\\
* L 702, Table 4, Col. 5: MED1 $\rightarrow$ MED2
\\
* L 790: that $\rightarrow$ than
\\
* L 802-805: It‚Äôs better to bring DC-Feat results after DC-Tag to keep the consistency with other tables 
\\
* L 928: ge:en, do you mean de:en (German-English)? 
\\
* L 973: domains:this $\rightarrow$ domains this
\\
* L 979: drawed $\rightarrow$ drawn 
\section*{Reviewer D}
The authors deliver on their promise given in the title and abstract of the paper by doing exactly what I was hoping for: They provide a dense but comprehensive review of approaches to multi-domain MT. In a commendably straight and structured manner, they identify requirements and challenges of multi-domain MT. And they then proceed with empirically evaluating lots of multi-domain MT approaches.
\\
The empirical evaluation replicates more than ten multi-domain translation approaches from the literature and directly compares them against each other on the English-French language pair. The experiments are conducted using a coherent scenario with respect to both the amount of training data and the amount and type of distinct domains that the authors test on.
\\
The authors come up with a series of experiments that carefully considers the initially identified requirements and challenges. The (seven) identified challenges fall under the three categories "effectiveness", "robustness", and "scalability". There can be competing objectives when building multi-domain translation engines, and the results and discussion in the paper reveal the strengths and weaknesses of the various approaches in the considered scenario.
\\
Even though I cannot easily name the one striking contribution of this work, it's quite obvious to me that many fellow researchers will benefit from the clear presentation of multi-domain MT as well as from the extensive empirical results provided in this paper. I'd like to see it published.
\\
Let me nevertheless formulate a few questions or points of criticism:
\\
- I'd like to see some analysis of vocabulary coverages (such as
cross-domain OOV rates), and of "seen errors" and "new sense errors" as defined by Irvine et al. in "Measuring machine translation errors in new domains" (TACL 2013).
\\
\fyTodo{extra experiments to do.}
\\
- Maybe the (possible) effect of early stopping (and the choice of the validation set) should be discussed? E.g., the validation set for
finetuning can originate from one single domain for each finetuned model. The way you're doing early stopping may even have had an impact on the results in Table 5. Is your validation set always a concatenation of 1000 lines from the six domains, without NEWS? Are you actually always training until the early stopping criterion is reached, and are you then using the model from the best checkpoint? (There are contradictory statements in the paper about "50,000  additional iterations" (?) for the experiments in Table 5.)
\\
\textcolor{red}{For each domain, we create train/dev/test sets by randomly splitting the respective corpora. We maintain the size of validation sets and of test sets equal to 1000 lines for every domain. In the table 3 and 4, to train MDMT models except FT-full and FT-block we use a validation set which is a concatenation of 6 validation sets of 6 domains. In the table 5, to train MDMT models except FT-full and FT-block we concatenate the validation set of NEWS to the mixture of 6 validation sets. To train FT-full model and FT-block model we only use the in-domain validation set. We stop training if either the training iteration reach the maximal number of iterations or the score on the validation set does not increase in 3 consecutive evaluations. We average 5 checkpoints up to the best checkpoint to get the final model.}
\\
- Report the learning rate. Is it decreased for finetuning?
\\
\textcolor{red}{The learning rate is determined by this formula $lr = d_{model}^{-0.5}*\min(step\_num^{-0.5},step\_num * warmup\_steps^{-1.5})$ where $d_{model}=512$,$warmup\_steps=4000$. In the finetuning procedure, we continue the training using the same learning rate schedule and the same $step\_num$}
\\
- To some extent, I dislike Table 5. When not reading carefully, the
difference values could be interpreted as differences wrt. scores in Table 3. (Except that the numbers would be wrong...) Differences wrt. to Table 3 are important and it took me a while to obtain an overview for myself. Furthermore, it should be immediately obvious which number is "resumed training" and which is training from scratch. (It isn't from the table caption alone.)
\\
\textcolor{red}{In the table 5, we report the difference of the model's performance in 2 situations including resuming training and training from beginning with 7 domains. We would like to report the difference wrt to table 3 in the later version of the paper. We agree that the comparison to table 3 is also important, because it could assess the impact of new domain to the previously learned domains.}
\\
- How is "Mixed-Bal" achieved? By means of upsampling of the corpora from the less highly represented domains?
\\
\textcolor{red}{In all of our experiments, to get a training batch, we first sample a domain from a set of available domains in training procedure with a fixed probability, the we sample a from the corpora corresponding to the previously picked domain. Therefore, between our notations, "Mixed-NAT" uses a domain probability equal to proportion of domain in training data, and "Mixed-Bal" uses a domain probability equal to uniform probability.}
\\
- For "DC-Feat", is the feature added for each word or for each subword token? Is it attached to the word (increasing the vocabulary size a lot) or does it remain a separate token (increasing the input length a lot)?
\\
\textcolor{red}{DC-Feat is a small embedding representing the domain. It is concatenated to every word embedding whenever its corresponding domain is detected. The use of DC-Feat increase the size of the input embedding, but does not increase the vocabulary size nor the input length.}
\\
- How is the domain-weighted average (WAVG) calculated? Is the weight determined by proportion of the respective domain in the training data?
\\
\textcolor{red}{The domain-weighted average is calculated by weighting the performance of the model in each domain test with the proportion of the respective domain in the training data.}
\\
- Wasn't it possible to use standard test sets? The "REL" Bleu scores are suspiciously high and there might be considerable redundancy in the data, or rather: training data contaminated with test data. (Since duplicates are possible, have you verified that the test sentences don't still appear in the remaining training data?)
\\
\textcolor{red}{There are standard test sets for the domains including medical, IT, news and talk, but we could not find any standard test sets for the other domains including religion, law and bank. We carefully eliminated every duplicate in all of the experimental corpora. In our opinion, the reason why the models achieve extremely high scores in the domain religion is because the text in this domain is much less variable.}
\\
Other (minor) remarks:
- From the UFAL medical corpus, please say that you're using the indomain sections only (which is mostly PatTR Medical).
\\
\textcolor{red}{From the UFAL medical corpus of the language pair En-Fr, we select only sentences in categories including EMEA, ParTR, CESTA, ECDC.}
\\
- The "DC-Feat" and "DC-Tag" lines in Table 5 should be swapped in order to be in line with Tables 3 and 4.
\\
\textcolor{red}{We agree to change the order of "DC-Feat" and "DC-Tag" to be consistent with tables 3 and 4.}
\\
- l. 503: "LDC-Feat" $\rightarrow$ "LDR"
\\
- l. 108: "automating" $\rightarrow$ "automatic"
\\
- l. 183: "accross" $\rightarrow$ "across"
\\
- l. 208: "under-resource" $\rightarrow$ "under-resourced"
\\
- l. 256: "ben" $\rightarrow$ "be"
\\
- l. 310: "used MT" $\rightarrow$ "used in MT"
\\
- l. 379: "Corpus statistics in Table 1." $\rightarrow$ "Corpus statistics are given in
Table 1."
\\
- l. 380: "variable" $\rightarrow$ "available"
\\
- l. 426: "the all others" $\rightarrow$ "all others"
\\
- l. 696: "suplementary" $\rightarrow$ "supplementary"
\\
- l. 979: "drawed" $\rightarrow$ "drawn"
\\
- l. 991: "may be" $\rightarrow$ "maybe"

\bibliographystyle{acl_natbib}
\bibliography{multidomain}
\end{document}


