\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{lample2018word,artetxe2018iclr}
\citation{Zhao:2002:APS:844380.844785,W04-3208,J05-4003,W17-2509,P17-3003}
\citation{P16-1009,W17-4714}
\citation{tiedemann2011bitext,XU16.310}
\citation{chen2016adaptation}
\citation{LisonTiedemann2016}
\citation{C14-1055}
\citation{ZWEIGENBAUM18.12}
\providecommand \oddpage@label [2]{}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{tab:examples}{{1}{1}{Table caption text}{table.caption.1}{}}
\citation{J94-4004}
\citation{goute2012}
\citation{chen2016adaptation}
\citation{Hassan2018AchievingHP}
\citation{conf/semeval/AgirreBCDGMRW16}
\citation{Mueller:2016:SRA:3016100.3016291}
\citation{N16-1108}
\citation{W17-3209}
\citation{DBLP:journals/corr/abs-1803-11112}
\citation{N16-1108}
\citation{J05-4003}
\citation{E09-1003}
\citation{W17-2508}
\citation{W15-1521}
\citation{W17-2509}
\newlabel{related}{{2}{2}{Related Work}{section.2}{}}
\citation{W16-2207}
\citation{W16-2207}
\citation{W17-2509}
\newlabel{sec:similarity}{{3}{3}{Neural Similarity Classifier}{section.3}{}}
\newlabel{cosine}{{1}{3}{Neural Similarity Classifier}{equation.3.1}{}}
\newlabel{aggregation}{{2}{3}{Neural Similarity Classifier}{equation.3.2}{}}
\newlabel{loss_wemb}{{3}{3}{Neural Similarity Classifier}{equation.3.3}{}}
\citation{W17-2509}
\citation{W17-2509}
\newlabel{loss_semb}{{5}{4}{Neural Similarity Classifier}{equation.3.5}{}}
\newlabel{training}{{3.1}{4}{Training with Negative Examples}{subsection.3.1}{}}
\newlabel{correction}{{3.2}{4}{Fixing Translation Divergencies}{subsection.3.2}{}}
\citation{LisonTiedemann2016}
\citation{mslt-corpus-iwslt-2016-release}
\citation{W17-4717}
\citation{ZWEIGENBAUM18.12}
\citation{lample2018word}
\citation{Pascanu:2013:DTR:3042817.3043083}
\citation{W17-2509}
\citation{Sennrich2016}
\newlabel{experiments}{{4}{5}{Experiments}{section.4}{}}
\newlabel{corpora}{{4.1}{5}{Corpora}{subsection.4.1}{}}
\newlabel{divergence}{{4.2.1}{5}{Neural Similarity Classifier}{subsubsection.4.2.1}{}}
\citation{P02-1040}
\newlabel{translation}{{4.2.2}{6}{Neural MT}{subsubsection.4.2.2}{}}
\newlabel{sec:results}{{5}{6}{Results}{section.5}{}}
\newlabel{results_puri}{{2}{6}{Word divergence accuracies for networks trained with different combinations of negative examples. P, U, R and I stand respectively for pair, unpair, replace and insert.\relax }{table.caption.7}{}}
\newlabel{results_wemb}{{3}{6}{BLEU scores obtained by a neural MT network when trained over the OpenSubtitles corpus filtered using different similarity thresholds.\relax }{table.caption.9}{}}
\bibdata{biblio}
\bibstyle{acl_natbib_nourl}
\newlabel{results_nmt}{{4}{7}{.\relax }{table.caption.10}{}}
\newlabel{results_embeddings}{{5}{7}{Number of parallel sentences found on the $N$-best cosine similarity filter (FAISS) according to the sentence embeddings produced by different networks.\relax }{table.caption.11}{}}
\newlabel{results_similarity}{{6}{7}{Precission, Recall and F-measure for the three evaluated models on the BUCC task.\relax }{table.caption.12}{}}
\newlabel{conclusions}{{6}{7}{Conclusions}{section.6}{}}
\newlabel{further}{{7}{7}{Further Work}{section.7}{}}
\newlabel{network}{{1}{8}{Illustration of the model. The network is composed of source and target word embedding lookup tables ($LT_s$ and $LT_t$) and two identical subnetworks ($net_s$ and $net_t$) that compute in context representations of source ($s_i$) and target words ($t_j$).\relax }{figure.caption.2}{}}
\newlabel{net_sentence}{{2}{8}{Illustration of the model presented in~\cite {W17-2509}. Sentence embeddings (vectors in dark grey) are computed by the same Bi-LSTM layers shown in Figure~\ref {network}.\relax }{figure.caption.3}{}}
\newlabel{matrix}{{3}{8}{Example of neural alignment matrices and divergence scores obtained with our model when trained over pair and unpair examples (right), and over all types of examples (left) as detailed in Section~\ref {training}. Aggregation scores of Equation~\ref {aggregation} are shown next to source and target words. Matrices contain word alignment scores. Sentence similarity scores computed by Equation~\ref {cosine} are shown below each matrix.\relax }{figure.caption.8}{}}
